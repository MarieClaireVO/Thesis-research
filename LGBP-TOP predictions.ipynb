{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBP-TOP predictions\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries to load the data\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = # Please provide own path\n",
    "#os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_annot_training = open(path+\"\\\\\"+\"annotation_training.pkl\", \"rb\")\n",
    "annotation_training = pickle.load(infile_annot_training, encoding = \"latin1\")\n",
    "\n",
    "infile_annot_validation = open(path+\"\\\\\"+\"annotation_validation.pkl\", \"rb\")\n",
    "annotation_validation = pickle.load(infile_annot_validation, encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the ethnicity and gender data\n",
    "# and preparing it to merge with other data frames\n",
    "excel_data = pd.read_excel(r\"C:\\Users\\Marie-Claire\\Downloads\\eth_gender_anno_all.xlsx\")\n",
    "excel_to_merge = excel_data.drop(['YouTubeID'], axis = 1)\n",
    "excel_to_merge.columns = excel_to_merge.columns.str.lower()\n",
    "excel_to_merge.rename(columns = {\"videoname\" : \"filenames\", \n",
    "                         }, inplace = True)\n",
    "#excel_to_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBP-TOP predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the lgbptop features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = # Please provide own path\n",
    "\n",
    "lgbptop_file_path = os.path.join(data_dir, 'lgbptop.mat')\n",
    "feats = loadmat(lgbptop_file_path)\n",
    "\n",
    "mdtype = feats['lgbptop'].dtype\n",
    "\n",
    "lgbptop_ndata = {n: feats['lgbptop'][n][0, 0] for n in mdtype.names}\n",
    "\n",
    "filenames = [filename[0] for filename in lgbptop_ndata['filename'].squeeze()]\n",
    "\n",
    "data = {'filenames': filenames, 'features': lgbptop_ndata['data']}\n",
    "\n",
    "#pickle.dump(data, open(os.path.join(data_dir, 'lgbptop.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 50112)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbptop_ndata['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training data by converting the dictionaries to dataframes\n",
    "# This way the data can be merged later on\n",
    "\n",
    "# Creating a copy just in case\n",
    "data_1 = data.copy()\n",
    "\n",
    "# Creating a data frame of the features data\n",
    "                         #test_array = np.array(data_1['features'])\n",
    "features_df = pd.DataFrame(data_1['features'])\n",
    "#features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy just in case\n",
    "data_2 = data.copy()\n",
    "\n",
    "# Creating a data frame of the filenames data\n",
    "filenames_df = pd.DataFrame(data_2['filenames'])       # np.array() weggehaald\n",
    "filenames_df = filenames_df.rename(columns= {0 : \"filenames\"})\n",
    "#filenames_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_containing_lgbptop = pd.concat([filenames_df, features_df], axis = 1)\n",
    "#df_containing_lgbptop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Ymqszjv54.001.mp4\n",
      "[0.16947712 0.14913054 0.13073076 ... 0.3226819  0.32539284 0.29858854]\n",
      "--Ymqszjv54.003.mp4\n",
      "[0.15545914 0.14307977 0.1280805  ... 0.29946554 0.30811465 0.2924719 ]\n"
     ]
    }
   ],
   "source": [
    "# Checking if the sequence is in correct order\n",
    "print(data['filenames'][0])\n",
    "print(data['features'][0])\n",
    "\n",
    "print(data['filenames'][1])\n",
    "print(data['features'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_interview = pd.DataFrame(annotation_training[\"interview\"],  index=[0]).T\n",
    "df_interview.reset_index(inplace = True)\n",
    "df_interview.rename(columns = {\"index\" : \"filenames\", 0 : \"interview\"}, inplace = True)\n",
    "print(df_interview.shape)\n",
    "#df_interview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 50114)\n"
     ]
    }
   ],
   "source": [
    "lgbptop_1_train = pd.merge(df_interview,df_containing_lgbptop, on = [\"filenames\"], how = \"left\")\n",
    "print(lgbptop_1_train.shape)\n",
    "#lgbptop_1_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing data & outliers in the training set\n",
    "for column in lgbptop_1_train.columns:\n",
    "    if lgbptop_1_train[column].isnull().any():\n",
    "        print(column)                           # No output means no missing data in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>50102</th>\n",
       "      <th>50103</th>\n",
       "      <th>50104</th>\n",
       "      <th>50105</th>\n",
       "      <th>50106</th>\n",
       "      <th>50107</th>\n",
       "      <th>50108</th>\n",
       "      <th>50109</th>\n",
       "      <th>50110</th>\n",
       "      <th>50111</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503780</td>\n",
       "      <td>0.131773</td>\n",
       "      <td>0.130794</td>\n",
       "      <td>0.104524</td>\n",
       "      <td>0.103676</td>\n",
       "      <td>0.140296</td>\n",
       "      <td>0.134568</td>\n",
       "      <td>0.156796</td>\n",
       "      <td>0.144616</td>\n",
       "      <td>0.123416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270877</td>\n",
       "      <td>0.284464</td>\n",
       "      <td>0.280694</td>\n",
       "      <td>0.293382</td>\n",
       "      <td>0.293395</td>\n",
       "      <td>0.297859</td>\n",
       "      <td>0.310077</td>\n",
       "      <td>0.308510</td>\n",
       "      <td>0.312179</td>\n",
       "      <td>0.292185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>0.018125</td>\n",
       "      <td>0.021917</td>\n",
       "      <td>0.023105</td>\n",
       "      <td>0.022883</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>0.014479</td>\n",
       "      <td>0.017150</td>\n",
       "      <td>0.023376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024335</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.027614</td>\n",
       "      <td>0.026429</td>\n",
       "      <td>0.026354</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.028314</td>\n",
       "      <td>0.029683</td>\n",
       "      <td>0.030611</td>\n",
       "      <td>0.024684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.401869</td>\n",
       "      <td>0.118181</td>\n",
       "      <td>0.118380</td>\n",
       "      <td>0.087601</td>\n",
       "      <td>0.087156</td>\n",
       "      <td>0.128297</td>\n",
       "      <td>0.127144</td>\n",
       "      <td>0.148242</td>\n",
       "      <td>0.135780</td>\n",
       "      <td>0.107362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253954</td>\n",
       "      <td>0.267186</td>\n",
       "      <td>0.261962</td>\n",
       "      <td>0.275575</td>\n",
       "      <td>0.275043</td>\n",
       "      <td>0.278020</td>\n",
       "      <td>0.290144</td>\n",
       "      <td>0.287040</td>\n",
       "      <td>0.290923</td>\n",
       "      <td>0.275801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.132467</td>\n",
       "      <td>0.130464</td>\n",
       "      <td>0.105628</td>\n",
       "      <td>0.104645</td>\n",
       "      <td>0.142826</td>\n",
       "      <td>0.135303</td>\n",
       "      <td>0.158083</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.122332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269839</td>\n",
       "      <td>0.283934</td>\n",
       "      <td>0.280072</td>\n",
       "      <td>0.292645</td>\n",
       "      <td>0.291822</td>\n",
       "      <td>0.296793</td>\n",
       "      <td>0.309200</td>\n",
       "      <td>0.307809</td>\n",
       "      <td>0.310607</td>\n",
       "      <td>0.291694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.146711</td>\n",
       "      <td>0.143214</td>\n",
       "      <td>0.120693</td>\n",
       "      <td>0.120342</td>\n",
       "      <td>0.155087</td>\n",
       "      <td>0.142602</td>\n",
       "      <td>0.166577</td>\n",
       "      <td>0.156039</td>\n",
       "      <td>0.139146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285992</td>\n",
       "      <td>0.300982</td>\n",
       "      <td>0.297976</td>\n",
       "      <td>0.309925</td>\n",
       "      <td>0.309842</td>\n",
       "      <td>0.316117</td>\n",
       "      <td>0.328546</td>\n",
       "      <td>0.327458</td>\n",
       "      <td>0.331735</td>\n",
       "      <td>0.307382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199617</td>\n",
       "      <td>0.201120</td>\n",
       "      <td>0.190379</td>\n",
       "      <td>0.177261</td>\n",
       "      <td>0.221328</td>\n",
       "      <td>0.177877</td>\n",
       "      <td>0.198703</td>\n",
       "      <td>0.208058</td>\n",
       "      <td>0.215836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.397753</td>\n",
       "      <td>0.402384</td>\n",
       "      <td>0.405589</td>\n",
       "      <td>0.421252</td>\n",
       "      <td>0.432532</td>\n",
       "      <td>0.444969</td>\n",
       "      <td>0.433340</td>\n",
       "      <td>0.422226</td>\n",
       "      <td>0.455183</td>\n",
       "      <td>0.406028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         interview            0            1            2            3  \\\n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000   \n",
       "mean      0.503780     0.131773     0.130794     0.104524     0.103676   \n",
       "std       0.150148     0.021704     0.018125     0.021917     0.023105   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.401869     0.118181     0.118380     0.087601     0.087156   \n",
       "50%       0.514019     0.132467     0.130464     0.105628     0.104645   \n",
       "75%       0.609813     0.146711     0.143214     0.120693     0.120342   \n",
       "max       1.000000     0.199617     0.201120     0.190379     0.177261   \n",
       "\n",
       "                 4            5            6            7            8  ...  \\\n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000  ...   \n",
       "mean      0.140296     0.134568     0.156796     0.144616     0.123416  ...   \n",
       "std       0.022883     0.012633     0.014479     0.017150     0.023376  ...   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  ...   \n",
       "25%       0.128297     0.127144     0.148242     0.135780     0.107362  ...   \n",
       "50%       0.142826     0.135303     0.158083     0.146800     0.122332  ...   \n",
       "75%       0.155087     0.142602     0.166577     0.156039     0.139146  ...   \n",
       "max       0.221328     0.177877     0.198703     0.208058     0.215836  ...   \n",
       "\n",
       "             50102        50103        50104        50105        50106  \\\n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000   \n",
       "mean      0.270877     0.284464     0.280694     0.293382     0.293395   \n",
       "std       0.024335     0.025905     0.027614     0.026429     0.026354   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.253954     0.267186     0.261962     0.275575     0.275043   \n",
       "50%       0.269839     0.283934     0.280072     0.292645     0.291822   \n",
       "75%       0.285992     0.300982     0.297976     0.309925     0.309842   \n",
       "max       0.397753     0.402384     0.405589     0.421252     0.432532   \n",
       "\n",
       "             50107        50108        50109        50110        50111  \n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000  \n",
       "mean      0.297859     0.310077     0.308510     0.312179     0.292185  \n",
       "std       0.027800     0.028314     0.029683     0.030611     0.024684  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.278020     0.290144     0.287040     0.290923     0.275801  \n",
       "50%       0.296793     0.309200     0.307809     0.310607     0.291694  \n",
       "75%       0.316117     0.328546     0.327458     0.331735     0.307382  \n",
       "max       0.444969     0.433340     0.422226     0.455183     0.406028  \n",
       "\n",
       "[8 rows x 50113 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbptop_1_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the test set similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "df_interview_test = pd.DataFrame(annotation_validation[\"interview\"],  index=[0]).T\n",
    "df_interview_test.reset_index(inplace = True)\n",
    "df_interview_test.rename(columns = {\"index\" : \"filenames\", 0 : \"interview\"}, inplace = True)\n",
    "print(df_interview_test.shape)\n",
    "#df_interview_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 50114)\n"
     ]
    }
   ],
   "source": [
    "lgbptop_1_test = pd.merge(df_interview_test, df_containing_lgbptop, on = [\"filenames\"], how = \"left\")\n",
    "print(lgbptop_1_test.shape)\n",
    "#lgbptop_1_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing data & outliers in the test set\n",
    "for column in lgbptop_1_test.columns:\n",
    "    if lgbptop_1_test[column].isnull().any():\n",
    "        print(column)                           # No output means no missing data in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>50102</th>\n",
       "      <th>50103</th>\n",
       "      <th>50104</th>\n",
       "      <th>50105</th>\n",
       "      <th>50106</th>\n",
       "      <th>50107</th>\n",
       "      <th>50108</th>\n",
       "      <th>50109</th>\n",
       "      <th>50110</th>\n",
       "      <th>50111</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.504650</td>\n",
       "      <td>0.131936</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.104702</td>\n",
       "      <td>0.104102</td>\n",
       "      <td>0.140878</td>\n",
       "      <td>0.134368</td>\n",
       "      <td>0.156608</td>\n",
       "      <td>0.144762</td>\n",
       "      <td>0.122347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270992</td>\n",
       "      <td>0.284268</td>\n",
       "      <td>0.279960</td>\n",
       "      <td>0.292701</td>\n",
       "      <td>0.293285</td>\n",
       "      <td>0.298175</td>\n",
       "      <td>0.310036</td>\n",
       "      <td>0.308314</td>\n",
       "      <td>0.312029</td>\n",
       "      <td>0.292279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.145160</td>\n",
       "      <td>0.021385</td>\n",
       "      <td>0.017580</td>\n",
       "      <td>0.021373</td>\n",
       "      <td>0.022947</td>\n",
       "      <td>0.022748</td>\n",
       "      <td>0.012506</td>\n",
       "      <td>0.014496</td>\n",
       "      <td>0.017229</td>\n",
       "      <td>0.023267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023687</td>\n",
       "      <td>0.025359</td>\n",
       "      <td>0.026714</td>\n",
       "      <td>0.025421</td>\n",
       "      <td>0.025818</td>\n",
       "      <td>0.027097</td>\n",
       "      <td>0.027646</td>\n",
       "      <td>0.029096</td>\n",
       "      <td>0.029786</td>\n",
       "      <td>0.023826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045359</td>\n",
       "      <td>0.063862</td>\n",
       "      <td>0.050175</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>0.046517</td>\n",
       "      <td>0.086030</td>\n",
       "      <td>0.106256</td>\n",
       "      <td>0.065572</td>\n",
       "      <td>0.051018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204821</td>\n",
       "      <td>0.214008</td>\n",
       "      <td>0.202515</td>\n",
       "      <td>0.218756</td>\n",
       "      <td>0.215397</td>\n",
       "      <td>0.225334</td>\n",
       "      <td>0.231037</td>\n",
       "      <td>0.231553</td>\n",
       "      <td>0.215236</td>\n",
       "      <td>0.187953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.118335</td>\n",
       "      <td>0.118982</td>\n",
       "      <td>0.088642</td>\n",
       "      <td>0.087917</td>\n",
       "      <td>0.129544</td>\n",
       "      <td>0.127067</td>\n",
       "      <td>0.147840</td>\n",
       "      <td>0.136353</td>\n",
       "      <td>0.106599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.253867</td>\n",
       "      <td>0.266863</td>\n",
       "      <td>0.261872</td>\n",
       "      <td>0.275247</td>\n",
       "      <td>0.274919</td>\n",
       "      <td>0.279009</td>\n",
       "      <td>0.290256</td>\n",
       "      <td>0.288011</td>\n",
       "      <td>0.291608</td>\n",
       "      <td>0.276596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.132221</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>0.105440</td>\n",
       "      <td>0.104743</td>\n",
       "      <td>0.143621</td>\n",
       "      <td>0.135160</td>\n",
       "      <td>0.157726</td>\n",
       "      <td>0.146522</td>\n",
       "      <td>0.121562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.270134</td>\n",
       "      <td>0.282925</td>\n",
       "      <td>0.278809</td>\n",
       "      <td>0.291514</td>\n",
       "      <td>0.291673</td>\n",
       "      <td>0.297360</td>\n",
       "      <td>0.309224</td>\n",
       "      <td>0.307131</td>\n",
       "      <td>0.310641</td>\n",
       "      <td>0.291552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.146833</td>\n",
       "      <td>0.142530</td>\n",
       "      <td>0.120321</td>\n",
       "      <td>0.120804</td>\n",
       "      <td>0.155869</td>\n",
       "      <td>0.142519</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.156474</td>\n",
       "      <td>0.137712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286083</td>\n",
       "      <td>0.301085</td>\n",
       "      <td>0.296979</td>\n",
       "      <td>0.309320</td>\n",
       "      <td>0.310252</td>\n",
       "      <td>0.315926</td>\n",
       "      <td>0.328515</td>\n",
       "      <td>0.327173</td>\n",
       "      <td>0.330728</td>\n",
       "      <td>0.307309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.199747</td>\n",
       "      <td>0.192830</td>\n",
       "      <td>0.174323</td>\n",
       "      <td>0.190292</td>\n",
       "      <td>0.210186</td>\n",
       "      <td>0.176856</td>\n",
       "      <td>0.198523</td>\n",
       "      <td>0.209860</td>\n",
       "      <td>0.206451</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381111</td>\n",
       "      <td>0.373208</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.442470</td>\n",
       "      <td>0.420939</td>\n",
       "      <td>0.427435</td>\n",
       "      <td>0.421934</td>\n",
       "      <td>0.413157</td>\n",
       "      <td>0.392608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50113 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         interview            0            1            2            3  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.504650     0.131936     0.131003     0.104702     0.104102   \n",
       "std       0.145160     0.021385     0.017580     0.021373     0.022947   \n",
       "min       0.000000     0.045359     0.063862     0.050175     0.035888   \n",
       "25%       0.411215     0.118335     0.118982     0.088642     0.087917   \n",
       "50%       0.514019     0.132221     0.131372     0.105440     0.104743   \n",
       "75%       0.607477     0.146833     0.142530     0.120321     0.120804   \n",
       "max       0.915888     0.199747     0.192830     0.174323     0.190292   \n",
       "\n",
       "                 4            5            6            7            8  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean      0.140878     0.134368     0.156608     0.144762     0.122347  ...   \n",
       "std       0.022748     0.012506     0.014496     0.017229     0.023267  ...   \n",
       "min       0.046517     0.086030     0.106256     0.065572     0.051018  ...   \n",
       "25%       0.129544     0.127067     0.147840     0.136353     0.106599  ...   \n",
       "50%       0.143621     0.135160     0.157726     0.146522     0.121562  ...   \n",
       "75%       0.155869     0.142519     0.166700     0.156474     0.137712  ...   \n",
       "max       0.210186     0.176856     0.198523     0.209860     0.206451  ...   \n",
       "\n",
       "             50102        50103        50104        50105        50106  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.270992     0.284268     0.279960     0.292701     0.293285   \n",
       "std       0.023687     0.025359     0.026714     0.025421     0.025818   \n",
       "min       0.204821     0.214008     0.202515     0.218756     0.215397   \n",
       "25%       0.253867     0.266863     0.261872     0.275247     0.274919   \n",
       "50%       0.270134     0.282925     0.278809     0.291514     0.291673   \n",
       "75%       0.286083     0.301085     0.296979     0.309320     0.310252   \n",
       "max       0.381111     0.373208     0.388600     0.431641     0.442470   \n",
       "\n",
       "             50107        50108        50109        50110        50111  \n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean      0.298175     0.310036     0.308314     0.312029     0.292279  \n",
       "std       0.027097     0.027646     0.029096     0.029786     0.023826  \n",
       "min       0.225334     0.231037     0.231553     0.215236     0.187953  \n",
       "25%       0.279009     0.290256     0.288011     0.291608     0.276596  \n",
       "50%       0.297360     0.309224     0.307131     0.310641     0.291552  \n",
       "75%       0.315926     0.328515     0.327173     0.330728     0.307309  \n",
       "max       0.420939     0.427435     0.421934     0.413157     0.392608  \n",
       "\n",
       "[8 rows x 50113 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbptop_1_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Predictions with LGBP-TOP facial data</h1> \n",
    "<h3 align=\"center\"> Multiple Linear Regression</h3>\n",
    "\n",
    "#### Models that will be trained:\n",
    "- Model 1 = Job interview ~ Facial\n",
    "- Model 2 = Job interview ~ Facial + Gender\n",
    "- Model 3 = Job interview ~ Facial + Ethnicity\n",
    "- Model 4 = Job interview ~ Facial + Gender + Ethnicity\n",
    "\n",
    "## Model 1 - Interview ~ facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the (in)dependent variables\n",
    "X_1_train = lgbptop_1_train.drop(['filenames', 'interview'], axis=1).values\n",
    "y_1_train = lgbptop_1_train.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_1_test = lgbptop_1_test.drop(['filenames', 'interview'], axis=1).values\n",
    "y_1_test = lgbptop_1_test.loc[:,['interview']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830107663551402"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a baseline by taking the MAE of the test set outcome variable and the mean of the training set outcome variable\n",
    "1 - mean_absolute_error(y_1_test, np.full(2000, np.mean(y_1_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01629\n",
      "MAE 0.1005\n",
      "1 - MAE 0.8995\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_1_train, y_1_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_1_pred = regressor.predict(X_1_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_1_test, y_1_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_1_test, y_1_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1 - mean_absolute_error(y_1_test, y_1_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the importance of the added variable(s) (not necessary for Model 1)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.29155606\n",
      "0.29348245\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 1000 components, MSE Model 1 = 0.01342\n",
      "MLR with 1000 components, MAE Model 1 = 0.09189\n",
      "MLR with 1000 components, 1 - MAE Model 1 = 0.9081\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 1 = 0.01321\n",
      "MLR with 2000 components, MAE Model 1 = 0.09125\n",
      "MLR with 2000 components, 1 - MAE Model 1 = 0.9087\n",
      "-------------------------------------------\n",
      "MLR with 3000 components, MSE Model 1 = 0.01346\n",
      "MLR with 3000 components, MAE Model 1 = 0.09132\n",
      "MLR with 3000 components, 1 - MAE Model 1 = 0.9087\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [1000, 2000, 3000]     # 300, 500, 700, 4000, 6000 performed worse\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_mlr_PCA = pca.fit_transform(X_1_train)\n",
    "    X_1_test_mlr_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    regressor.fit(X_1_train_mlr_PCA, y_1_train)\n",
    "    mlr_y_1_PCA = regressor.predict(X_1_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 2000 components, MSE Model 1 = 0.01323\n",
      "MLR with 2000 components, MAE Model 1 = 0.09101\n",
      "MLR with 2000 components, 1 - MAE Model 1 = 0.909\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 1 for additional testing\n",
    "components = [2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_mlr_PCA = pca.fit_transform(X_1_train)\n",
    "    X_1_test_mlr_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    regressor.fit(X_1_train_mlr_PCA, y_1_train)\n",
    "    mlr_1_PCA_testing = regressor.predict(X_1_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, mlr_1_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, mlr_1_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, mlr_1_PCA_testing)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01601\n",
      "MAE 0.1008\n",
      "1 - MAE 0.8992\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_1_train, y_1_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_1 = rfr.predict(X_1_test)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_1_test, rfr_y_1)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_1_test, rfr_y_1)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_1_test, rfr_y_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 1000 components, MSE Model 1 = 0.01683\n",
      "RFR with 1000 components, MAE Model 1 = 0.1023\n",
      "RFR with 1000 components, 1 - MAE Model 1 = 0.8977\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 1 = 0.01724\n",
      "RFR with 2000 components, MAE Model 1 = 0.105\n",
      "RFR with 2000 components, 1 - MAE Model 1 = 0.895\n",
      "-------------------------------------------\n",
      "RFR with 3000 components, MSE Model 1 = 0.01776\n",
      "RFR with 3000 components, MAE Model 1 = 0.1065\n",
      "RFR with 3000 components, 1 - MAE Model 1 = 0.8935\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_1_train_PCA = pca.fit_transform(X_1_train)\n",
    "    rfr_X_1_test_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    rfr.fit(rfr_X_1_train_PCA, y_1_train.ravel())\n",
    "    rfr_y_1_PCA = rfr.predict(rfr_X_1_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "X_1_train_scale = X_sc.fit_transform(X_1_train)\n",
    "X_1_test_scale = X_sc.transform(X_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01179\n",
      "MAE 0.08667\n",
      "1 - MAE 0.9133\n",
      "Wall time: 19min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_1_train_scale, y_1_train.ravel())\n",
    "svr_y_1 = svr.predict(X_1_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_1_test, svr_y_1)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_1_test, svr_y_1)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_1_test, svr_y_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 1000 components, MSE Model 1 = 0.01159\n",
      "SVR with 1000 components, MAE Model 1 = 0.0857\n",
      "SVR with 1000 components, 1 - MAE Model 1 = 0.9143\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 1 = 0.01161\n",
      "SVR with 2000 components, MAE Model 1 = 0.08576\n",
      "SVR with 2000 components, 1 - MAE Model 1 = 0.9142\n",
      "-------------------------------------------\n",
      "SVR with 3000 components, MSE Model 1 = 0.01163\n",
      "SVR with 3000 components, MAE Model 1 = 0.08588\n",
      "SVR with 3000 components, 1 - MAE Model 1 = 0.9141\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for support vector regression\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_PCA = pca.fit_transform(X_1_train_scale)\n",
    "    X_1_test_PCA = pca.transform(X_1_test_scale)\n",
    "    \n",
    "    svr.fit(X_1_train_PCA, y_1_train.ravel())\n",
    "    svr_y_1_PCA = svr.predict(X_1_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Interview ~ facial + gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation (test) set for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training set for Model 2\n",
    "df_gen_int = excel_to_merge[['filenames', 'gender']]\n",
    "training_face_gen = pd.merge(lgbptop_1_train, df_gen_int, how = \"left\", on = [\"filenames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 2\n",
    "val_face_gen = pd.merge(lgbptop_1_test, df_gen_int, how = \"left\", on = [\"filenames\"])\n",
    "#val_face_gen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Splitting the (in)dependent variables\n",
    "X_2_train = training_face_gen.drop(['filenames','interview'], axis=1).values\n",
    "y_2_train = training_face_gen.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_2_test = val_face_gen.drop(['filenames', 'interview'], axis=1).values\n",
    "y_2_test = val_face_gen.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_2_train[:, -1] = le.fit_transform(X_2_train[:, -1])\n",
    "X_2_test[:, -1] = le.fit_transform(X_2_test[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01634\n",
      "MAE 0.1007\n",
      "1 - MAE 0.8993\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_2_train, y_2_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_2_pred = regressor.predict(X_2_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_2_test, y_2_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_2_test, y_2_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_2_test, y_2_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03159476334946262\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "# Position of gender variable\n",
    "print(importance[-1])\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.29185789516761973\n",
      "0.29580361483438145\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 1000 components, MSE Model 2 = 0.0135\n",
      "MLR with 1000 components, MAE Model 2 = 0.09199\n",
      "MLR with 1000 components, 1 - MAE Model 2 = 0.908\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 2 = 0.01323\n",
      "MLR with 2000 components, MAE Model 2 = 0.09109\n",
      "MLR with 2000 components, 1 - MAE Model 2 = 0.9089\n",
      "-------------------------------------------\n",
      "MLR with 3000 components, MSE Model 2 = 0.01327\n",
      "MLR with 3000 components, MAE Model 2 = 0.09094\n",
      "MLR with 3000 components, 1 - MAE Model 2 = 0.9091\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_mlr_PCA = pca.fit_transform(X_2_train)\n",
    "    X_2_test_mlr_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    regressor.fit(X_2_train_mlr_PCA, y_2_train)\n",
    "    mlr_y_2_PCA = regressor.predict(X_2_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1-mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 2000 components, MSE Model 2 = 0.01326\n",
      "MLR with 2000 components, MAE Model 2 = 0.09157\n",
      "MLR with 2000 components, 1 - MAE Model 2 = 0.9084\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 2 for additional testing\n",
    "components = [2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_mlr_PCA = pca.fit_transform(X_2_train)\n",
    "    X_2_test_mlr_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    regressor.fit(X_2_train_mlr_PCA, y_2_train)\n",
    "    mlr_2_PCA_testing = regressor.predict(X_2_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, mlr_2_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, mlr_2_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1-mean_absolute_error(y_2_test, mlr_2_PCA_testing)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01598\n",
      "MAE 0.1006\n",
      "1 - MAE 0.8994\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_2_train, y_2_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_2 = rfr.predict(X_2_test)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_2_test, rfr_y_2)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_2_test, rfr_y_2)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_2_test, rfr_y_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 1000 components, MSE Model 2 = 0.01676\n",
      "RFR with 1000 components, MAE Model 2 = 0.1036\n",
      "RFR with 1000 components, 1 - MAE Model 2 = 0.8964\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 2 = 0.01729\n",
      "RFR with 2000 components, MAE Model 2 = 0.105\n",
      "RFR with 2000 components, 1 - MAE Model 2 = 0.895\n",
      "-------------------------------------------\n",
      "RFR with 3000 components, MSE Model 2 = 0.01756\n",
      "RFR with 3000 components, MAE Model 2 = 0.1055\n",
      "RFR with 3000 components, 1 - MAE Model 2 = 0.8945\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_2_train_PCA = pca.fit_transform(X_2_train)\n",
    "    rfr_X_2_test_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    rfr.fit(rfr_X_2_train_PCA, y_2_train.ravel())\n",
    "    rfr_y_2_PCA = rfr.predict(rfr_X_2_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1-mean_absolute_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the gender dummies\n",
    "X_2_train_scale = X_2_train.copy()\n",
    "X_2_test_scale = X_2_test.copy()\n",
    "\n",
    "X_2_train_scale[:, :-1] = X_sc.fit_transform(X_2_train[:, :-1])\n",
    "X_2_test_scale[:, :-1] = X_sc.transform(X_2_test[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01179\n",
      "MAE 0.08668\n",
      "1 - MAE 0.9133\n",
      "Wall time: 18min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_2_train_scale, y_2_train.ravel())\n",
    "svr_y_2 = svr.predict(X_2_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_2_test, svr_y_2)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_2_test, svr_y_2)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_2_test, svr_y_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 1000 components, MSE Model 2 = 0.01159\n",
      "SVR with 1000 components, MAE Model 2 = 0.08572\n",
      "SVR with 1000 components, 1 - MAE Model 2 = 0.9143\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 2 = 0.01161\n",
      "SVR with 2000 components, MAE Model 2 = 0.08575\n",
      "SVR with 2000 components, 1 - MAE Model 2 = 0.9143\n",
      "-------------------------------------------\n",
      "SVR with 3000 components, MSE Model 2 = 0.01164\n",
      "SVR with 3000 components, MAE Model 2 = 0.08588\n",
      "SVR with 3000 components, 1 - MAE Model 2 = 0.9141\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for support vector regression\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_PCA = pca.fit_transform(X_2_train_scale)\n",
    "    X_2_test_PCA = pca.transform(X_2_test_scale)\n",
    "    \n",
    "    svr.fit(X_2_train_PCA, y_2_train.ravel())\n",
    "    svr_y_2_PCA = svr.predict(X_2_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1-mean_absolute_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Interview ~ facial + ethnicity\n",
    "#### Creating the training and validation (test) set for Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 3, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_eth_int = excel_to_merge[['filenames', 'ethnicity']]\n",
    "training_face_eth = pd.merge(lgbptop_1_train, df_eth_int, how = \"left\", on = [\"filenames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 3\n",
    "#df_val_eth_int = merged_val_df[['videoname', 'ethnicity', 'interview']]\n",
    "val_face_eth = pd.merge(lgbptop_1_test, df_eth_int, how = \"left\", on = [\"filenames\"])\n",
    "#val_face_eth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Splitting the (in)dependent variables\n",
    "X_3_train = training_face_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_3_train = training_face_eth.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_3_test = val_face_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_3_test = val_face_eth.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "X_3_train = np.array(ct.fit_transform(X_3_train))\n",
    "X_3_test = np.array(ct.fit_transform(X_3_test))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_3_train_dummy = X_3_train.copy()\n",
    "X_3_train_dummy = X_3_train_dummy[:,1:]\n",
    "\n",
    "X_3_test_dummy = X_3_test.copy()\n",
    "X_3_test_dummy = X_3_test_dummy[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01627\n",
      "MAE 0.1004\n",
      "1 - MAE 0.8996\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_3_train_dummy, y_3_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_3_pred = regressor.predict(X_3_test_dummy)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_3_test, y_3_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_3_test, y_3_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_3_test, y_3_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.027912389218651423\n",
      "-0.02177281987814038\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "# Postions of ethnicity dummies\n",
    "print(importance[0])\n",
    "print(importance[1])\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.29694016922460564\n",
      "0.29644844689831534\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 1000 components, MSE Model 3 = 0.01348\n",
      "MLR with 1000 components, MAE Model 3 = 0.09218\n",
      "MLR with 1000 components, 1 - MAE Model 3 = 0.9078\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 3 = 0.01313\n",
      "MLR with 2000 components, MAE Model 3 = 0.09115\n",
      "MLR with 2000 components, 1 - MAE Model 3 = 0.9089\n",
      "-------------------------------------------\n",
      "MLR with 3000 components, MSE Model 3 = 0.01341\n",
      "MLR with 3000 components, MAE Model 3 = 0.09137\n",
      "MLR with 3000 components, 1 - MAE Model 3 = 0.9086\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_mlr_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    X_3_test_mlr_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_3_train_mlr_PCA, y_3_train)\n",
    "    mlr_y_3_PCA = regressor.predict(X_3_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 2000 components, MSE Model 3 = 0.01328\n",
      "MLR with 2000 components, MAE Model 3 = 0.09121\n",
      "MLR with 2000 components, 1 - MAE Model 3 = 0.9088\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 3 for additional testing\n",
    "components = [2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_mlr_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    X_3_test_mlr_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_3_train_mlr_PCA, y_3_train)\n",
    "    mlr_3_PCA_testing = regressor.predict(X_3_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, mlr_3_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, mlr_3_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, mlr_3_PCA_testing)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01608\n",
      "MAE 0.1012\n",
      "1 - MAE 0.8988\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_3_train_dummy, y_3_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_3 = rfr.predict(X_3_test_dummy)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_3_test, rfr_y_3)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_3_test, rfr_y_3)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_3_test, rfr_y_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 1000 components, MSE Model 3 = 0.01676\n",
      "RFR with 1000 components, MAE Model 3 = 0.1038\n",
      "RFR with 1000 components, 1 - MAE Model 3 = 0.8962\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 3 = 0.01709\n",
      "RFR with 2000 components, MAE Model 3 = 0.1047\n",
      "RFR with 2000 components, 1 - MAE Model 3 = 0.8953\n",
      "-------------------------------------------\n",
      "RFR with 3000 components, MSE Model 3 = 0.0172\n",
      "RFR with 3000 components, MAE Model 3 = 0.1041\n",
      "RFR with 3000 components, 1 - MAE Model 3 = 0.8959\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_3_train_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    rfr_X_3_test_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    rfr.fit(rfr_X_3_train_PCA, y_3_train.ravel())\n",
    "    rfr_y_3_PCA = rfr.predict(rfr_X_3_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the ethnicity dummies\n",
    "X_3_train_scale = X_3_train_dummy.copy()\n",
    "X_3_test_scale = X_3_test_dummy.copy()\n",
    "\n",
    "X_3_train_scale[:, 2:] = X_sc.fit_transform(X_3_train_dummy[:, 2:])\n",
    "X_3_test_scale[:, 2:] = X_sc.transform(X_3_test_dummy[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01179\n",
      "MAE 0.08667\n",
      "1 - MAE 0.9133\n",
      "Wall time: 18min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_3_train_scale, y_3_train.ravel())\n",
    "svr_y_3 = svr.predict(X_3_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_3_test, svr_y_3)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_3_test, svr_y_3)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_3_test, svr_y_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 1000 components, MSE Model 3 = 0.0116\n",
      "SVR with 1000 components, MAE Model 3 = 0.08574\n",
      "SVR with 1000 components, 1 - MAE Model 3 = 0.9143\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 3 = 0.01161\n",
      "SVR with 2000 components, MAE Model 3 = 0.08578\n",
      "SVR with 2000 components, 1 - MAE Model 3 = 0.9142\n",
      "-------------------------------------------\n",
      "SVR with 3000 components, MSE Model 3 = 0.01163\n",
      "SVR with 3000 components, MAE Model 3 = 0.08585\n",
      "SVR with 3000 components, 1 - MAE Model 3 = 0.9142\n",
      "-------------------------------------------\n",
      "Wall time: 32min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [1000, 2000, 3000]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_PCA = pca.fit_transform(X_3_train_scale)\n",
    "    X_3_test_PCA = pca.transform(X_3_test_scale)\n",
    "    \n",
    "    svr.fit(X_3_train_PCA, y_3_train.ravel())\n",
    "    svr_y_3_PCA = svr.predict(X_3_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Interview ~ facial + gender + ethnicity\n",
    "#### Creating the training and validation (test) set for Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 4, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_gen_eth_int = excel_to_merge[['filenames', 'ethnicity', 'gender']]\n",
    "training_face_gen_eth = pd.merge(lgbptop_1_train, df_gen_eth_int, how = \"left\", on = [\"filenames\"])\n",
    "#training_face_gen_eth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 4\n",
    "val_face_gen_eth = pd.merge(lgbptop_1_test, df_gen_eth_int, how = \"left\", on = [\"filenames\"])\n",
    "#val_face_gen_eth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Splitting the (in)dependent variables\n",
    "X_4_train = training_face_gen_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_4_train = training_face_gen_eth.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_4_test = val_face_gen_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_4_test = val_face_gen_eth.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_4_train[:, -1] = le.fit_transform(X_4_train[:, -1])\n",
    "X_4_test[:, -1] = le.fit_transform(X_4_test[:, -1])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-2])], remainder='passthrough')\n",
    "X_4_train = ct.fit_transform(X_4_train)\n",
    "X_4_test = ct.fit_transform(X_4_test)     # np.array() weggehaald om ct... heen\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_4_train_dummy = X_4_train.copy()\n",
    "X_4_train_dummy = X_4_train_dummy[:,1:]\n",
    "\n",
    "X_4_test_dummy = X_4_test.copy()\n",
    "X_4_test_dummy = X_4_test_dummy[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01632\n",
      "MAE 0.1006\n",
      "1 - MAE 0.8994\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_4_train_dummy, y_4_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_4_pred = regressor.predict(X_4_test_dummy)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_4_test, y_4_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_4_test, y_4_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_4_test, y_4_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.025855189440576583\n",
      "-0.018492496185072704\n",
      "-0.03082542496441801\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "# Position of ethnicity and gender dummy variables\n",
    "print(importance[0])\n",
    "print(importance[1])\n",
    "print(importance[-1])\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.29637027557451034\n",
      "0.2984327157615102\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 1000 components, MSE Model 4 = 0.01343\n",
      "MLR with 1000 components, MAE Model 4 = 0.09164\n",
      "MLR with 1000 components, 1 - MAE Model 4 = 0.9084\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 4 = 0.01304\n",
      "MLR with 2000 components, MAE Model 4 = 0.09048\n",
      "MLR with 2000 components, 1 - MAE Model 4 = 0.9095\n",
      "-------------------------------------------\n",
      "MLR with 3000 components, MSE Model 4 = 0.01324\n",
      "MLR with 3000 components, MAE Model 4 = 0.09113\n",
      "MLR with 3000 components, 1 - MAE Model 4 = 0.9089\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_mlr_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    X_4_test_mlr_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_4_train_mlr_PCA, y_4_train)\n",
    "    mlr_y_4_PCA = regressor.predict(X_4_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 2000 components, MSE Model 4 = 0.01322\n",
      "MLR with 2000 components, MAE Model 4 = 0.09138\n",
      "MLR with 2000 components, 1 - MAE Model 4 = 0.9086\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 4 for additional testing\n",
    "components = [2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_mlr_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    X_4_test_mlr_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_4_train_mlr_PCA, y_4_train)\n",
    "    mlr_4_PCA_testing = regressor.predict(X_4_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, mlr_4_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, mlr_4_PCA_testing)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, mlr_4_PCA_testing)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01605\n",
      "MAE 0.1011\n",
      "1 - MAE 0.8989\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_4_train_dummy, y_4_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_4 = rfr.predict(X_4_test_dummy)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_4_test, rfr_y_4)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_4_test, rfr_y_4)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_4_test, rfr_y_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 1000 components, MSE Model 4 = 0.0169\n",
      "RFR with 1000 components, MAE Model 4 = 0.1028\n",
      "RFR with 1000 components, 1 - MAE Model 4 = 0.8972\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 4 = 0.01753\n",
      "RFR with 2000 components, MAE Model 4 = 0.1054\n",
      "RFR with 2000 components, 1 - MAE Model 4 = 0.8946\n",
      "-------------------------------------------\n",
      "RFR with 3000 components, MSE Model 4 = 0.01755\n",
      "RFR with 3000 components, MAE Model 4 = 0.1051\n",
      "RFR with 3000 components, 1 - MAE Model 4 = 0.8949\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "\n",
    "components = [1000, 2000, 3000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_4_train_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    rfr_X_4_test_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    rfr.fit(rfr_X_4_train_PCA, y_4_train.ravel())\n",
    "    rfr_y_4_PCA = rfr.predict(rfr_X_4_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the gender and ethnicity dummies\n",
    "X_4_train_scale = X_4_train_dummy.copy()\n",
    "X_4_test_scale = X_4_test_dummy.copy()\n",
    "\n",
    "X_4_train_scale[:, 2:-1] = X_sc.fit_transform(X_4_train_dummy[:, 2:-1])\n",
    "X_4_test_scale[:, 2:-1] = X_sc.transform(X_4_test_dummy[:, 2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01179\n",
      "MAE 0.08668\n",
      "1 - MAE 0.9133\n",
      "Wall time: 18min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_4_train_scale, y_4_train.ravel())\n",
    "svr_y_4 = svr.predict(X_4_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_4_test, svr_y_4)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_4_test, svr_y_4)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_4_test, svr_y_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 1000 components, MSE Model 4 = 0.01161\n",
      "SVR with 1000 components, MAE Model 4 = 0.08574\n",
      "SVR with 1000 components, 1 - MAE Model 4 = 0.9143\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 4 = 0.01161\n",
      "SVR with 2000 components, MAE Model 4 = 0.08575\n",
      "SVR with 2000 components, 1 - MAE Model 4 = 0.9142\n",
      "-------------------------------------------\n",
      "SVR with 3000 components, MSE Model 4 = 0.01163\n",
      "SVR with 3000 components, MAE Model 4 = 0.08586\n",
      "SVR with 3000 components, 1 - MAE Model 4 = 0.9141\n",
      "-------------------------------------------\n",
      "Wall time: 31min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [1000, 2000, 3000]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_PCA = pca.fit_transform(X_4_train_scale)\n",
    "    X_4_test_PCA = pca.transform(X_4_test_scale)\n",
    "    \n",
    "    svr.fit(X_4_train_PCA, y_4_train.ravel())\n",
    "    svr_y_4_PCA = svr.predict(X_4_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-tests between the models that have slight differences in MAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Related t-test MLR PCA\n",
    "print(scipy.stats.ttest_rel(mlr_1_PCA_testing, mlr_2_PCA_testing).pvalue)         # models 1 and 2\n",
    "print(scipy.stats.ttest_rel(mlr_1_PCA_testing, mlr_3_PCA_testing).pvalue)         # models 1 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scipy.stats.ttest_rel(mlr_1_PCA_testing, mlr_4_PCA_testing).pvalue)         # models 1 and 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
