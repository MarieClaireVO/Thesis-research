{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality traits predictions - OpenFace predictions\n",
    "### This scripts contains EDA, personality traits predictions, and OpenFace predictions.\n",
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries to load the data\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = # Please insert your own path\n",
    "#os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_annot_training = open(path+\"\\\\\"+\"annotation_training.pkl\", \"rb\")\n",
    "annotation_training = pickle.load(infile_annot_training, encoding = \"latin1\")\n",
    "\n",
    "infile_annot_validation = open(path+\"\\\\\"+\"annotation_validation.pkl\", \"rb\")\n",
    "annotation_validation = pickle.load(infile_annot_validation, encoding = \"latin1\")\n",
    "\n",
    "#infile_trans_test = open(path+\"\\\\\"+\"transcription_test.pkl\", \"rb\")\n",
    "#transcription_test = pickle.load(infile_trans_test, encoding = \"latin1\")\n",
    "\n",
    "#infile_trans_training = open(path+\"\\\\\"+\"transcription_training.pkl\", \"rb\")\n",
    "#transcription_training = pickle.load(infile_trans_training, encoding = \"latin1\")\n",
    "\n",
    "#infile_trans_validation = open(path+\"\\\\\"+\"transcription_validation.pkl\", \"rb\")\n",
    "#transcription_validation = pickle.load(infile_trans_validation, encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the ethnicity and gender data\n",
    "excel_data = pd.read_excel(r\"C:\\Users\\Marie-Claire\\Downloads\\eth_gender_anno_all.xlsx\")\n",
    "#excel_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3060"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking number of unique YoutTube vloggers\n",
    "excel_data['YouTubeID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the annotation data to dataframes and specifying the index column\n",
    "anno_df = pd.DataFrame(annotation_training)\n",
    "anno_df.reset_index(inplace = True)\n",
    "\n",
    "anno_test_df = pd.DataFrame(annotation_validation)\n",
    "anno_test_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column to merge the gender-ethnicity and annotation dataframes\n",
    "anno_df.rename(columns = {\"index\" : \"VideoName\", \n",
    "                         }, inplace = True)\n",
    "\n",
    "anno_test_df.rename(columns = {\"index\" : \"VideoName\", \n",
    "                         }, inplace = True)\n",
    "\n",
    "# Merging the dataframes anno_df and excel_data on the VideoName column as a training set\n",
    "merged_df = pd.merge(anno_df, excel_data, how = \"left\", on = [\"VideoName\"])\n",
    "#merged_df.info\n",
    "\n",
    "merged_df_test = pd.merge(anno_test_df, excel_data, how = \"left\", on = [\"VideoName\"])\n",
    "#merged_df_test.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating complete data frame for EDA\n",
    "anno_complete = pd.concat([anno_df, anno_test_df])\n",
    "all_data = pd.merge(anno_complete, excel_data, how = \"left\", on = [\"VideoName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoName            False\n",
      "extraversion         False\n",
      "neuroticism          False\n",
      "agreeableness        False\n",
      "conscientiousness    False\n",
      "interview            False\n",
      "openness             False\n",
      "YouTubeID            False\n",
      "Ethnicity            False\n",
      "Gender               False\n",
      "dtype: bool\n",
      "       extraversion  neuroticism  agreeableness  conscientiousness  \\\n",
      "count   8000.000000  8000.000000    8000.000000        8000.000000   \n",
      "mean       0.476313     0.520605       0.548898           0.524053   \n",
      "std        0.151226     0.152635       0.134234           0.155351   \n",
      "min        0.000000     0.000000       0.000000           0.000000   \n",
      "25%        0.373832     0.416667       0.461538           0.417476   \n",
      "50%        0.476636     0.531250       0.560440           0.524272   \n",
      "75%        0.579439     0.625000       0.637363           0.640777   \n",
      "max        1.000000     0.979167       1.000000           1.000000   \n",
      "\n",
      "         interview     openness    Ethnicity       Gender  \n",
      "count  8000.000000  8000.000000  8000.000000  8000.000000  \n",
      "mean      0.503998     0.566290     2.070500     1.543750  \n",
      "std       0.148908     0.146100     0.369184     0.498113  \n",
      "min       0.000000     0.000000     1.000000     1.000000  \n",
      "25%       0.401869     0.466667     2.000000     1.000000  \n",
      "50%       0.514019     0.566667     2.000000     2.000000  \n",
      "75%       0.607477     0.666667     2.000000     2.000000  \n",
      "max       1.000000     1.000000     3.000000     2.000000  \n"
     ]
    }
   ],
   "source": [
    "# Checking for missing data & outliers\n",
    "print(all_data.isnull().any())\n",
    "print(all_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to lowercase\n",
    "all_data.columns = all_data.columns.str.lower()\n",
    "merged_df.columns = merged_df.columns.str.lower()\n",
    "merged_df_test.columns = merged_df_test.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying distributions of gender per personality trait and the interview variable\n",
    "#cols = all_data[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'interview', 'openness']]\n",
    "cols = all_data[['interview']]\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "i = 0\n",
    "for column in cols:\n",
    "    #i += 1\n",
    "    #fig.add_subplot(3, 2, i)\n",
    "    plt.plot(1,1)\n",
    "    plt.hist(all_data[all_data[\"gender\"] == 1][column], label = \"Male\", bins = 30, color= 'blue')\n",
    "    plt.hist(all_data[all_data[\"gender\"] == 2][column], label = \"Female\", bins = 30, alpha = 0.6, color= 'orange')\n",
    "    plt.title(column.capitalize(), fontsize = 14, fontweight='bold')\n",
    "    plt.xlabel(\"Probability score\", fontsize= 13)\n",
    "    plt.ylabel(\"Amount of women and men\", fontsize= 13)\n",
    "    plt.legend(loc = \"upper right\", fontsize = \"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14284327704430083\n"
     ]
    }
   ],
   "source": [
    "# Descriptives of men for hireability\n",
    "men = all_data[all_data[\"gender\"] == 1][\"interview\"]\n",
    "#print(men)\n",
    "print(all_data[all_data[\"gender\"] == 1][\"interview\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15319215486254284\n"
     ]
    }
   ],
   "source": [
    "# Descriptives of women for hireability\n",
    "women = all_data[all_data[\"gender\"] == 2][\"interview\"]\n",
    "#print(women)\n",
    "print(all_data[all_data[\"gender\"] == 2][\"interview\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for a significant difference between gender groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=7264552.0, pvalue=2.80479970301992e-11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.mannwhitneyu(men, women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Distributions per ethnical group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying distributions of ethnicity per personality trait and the interview variable\n",
    "#cols = all_data[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'interview', 'openness']]\n",
    "cols = all_data[['interview']]\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "#i = 0\n",
    "for column in cols:\n",
    "    #i += 1\n",
    "    plt.plot(1,1)\n",
    "    plt.hist(all_data[all_data[\"ethnicity\"] == 1][column], label = \"Asian\", bins = 30, color = 'blue')\n",
    "    plt.hist(all_data[all_data[\"ethnicity\"] == 2][column], label = \"Caucasian\", bins = 30, alpha = 0.6, color= 'orange')\n",
    "    plt.hist(all_data[all_data[\"ethnicity\"] == 3][column], label = \"African American\", bins = 30, alpha = 0.6, color= 'yellow')\n",
    "    plt.title(column.capitalize(), fontsize = 14, fontweight='bold')\n",
    "    plt.xlabel(\"Probability score\", fontsize = 13)\n",
    "    plt.ylabel(\"Amount of people from ethnical group\", fontsize = 13)\n",
    "    plt.legend(loc = \"upper right\", fontsize = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying distributions of ethnicity per personality trait and the interview variable\n",
    "#cols = all_data[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'interview', 'openness']]\n",
    "cols = all_data[['interview']]\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "#i = 0\n",
    "for column in cols:\n",
    "    #i += 1\n",
    "    plt.plot(1,1)\n",
    "    plt.hist(all_data[all_data[\"ethnicity\"] == 1][column], label = \"Asian\", bins = 30, color = 'blue')\n",
    "    plt.hist(all_data[all_data[\"ethnicity\"] == 2][column], label = \"Caucasian\", bins = 30, alpha = 0.6, color= 'orange')\n",
    "    plt.hist(all_data[all_data[\"ethnicity\"] == 3][column], label = \"African American\", bins = 30, alpha = 0.6, color= 'yellow')\n",
    "    plt.title(column.capitalize(), fontsize = 14, fontweight='bold')\n",
    "    plt.xlabel(\"Probability score\", fontsize = 13)\n",
    "    plt.ylabel(\"Amount of people from ethnical group\", fontsize = 13)\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend(loc = \"upper right\", fontsize = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5152736039100426\n",
      "0.13448663229886249\n"
     ]
    }
   ],
   "source": [
    "# Descriptives of Asian ethnicity for hireability\n",
    "asian = all_data[all_data[\"ethnicity\"] == 1][\"interview\"]\n",
    "print(all_data[all_data[\"ethnicity\"] == 1][\"interview\"].mean())\n",
    "print(all_data[all_data[\"ethnicity\"] == 1][\"interview\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.507136541103812\n",
      "0.15026298490853182\n"
     ]
    }
   ],
   "source": [
    "# Descriptives of Caucasian ethnicity for hireability\n",
    "cauc = all_data[all_data[\"ethnicity\"] == 2][\"interview\"]\n",
    "print(all_data[all_data[\"ethnicity\"] == 2][\"interview\"].mean())\n",
    "print(all_data[all_data[\"ethnicity\"] == 2][\"interview\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47477076873848295\n",
      "0.13899587565563773\n"
     ]
    }
   ],
   "source": [
    "# Descriptives of African American ethnicity for hireability\n",
    "african = all_data[all_data[\"ethnicity\"] == 3][\"interview\"]\n",
    "print(all_data[all_data[\"ethnicity\"] == 3][\"interview\"].mean())\n",
    "print(all_data[all_data[\"ethnicity\"] == 3][\"interview\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for a significant difference between ethnical groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Mann Whitney U: Asian - Caucasian:            MannwhitneyuResult(statistic=951677.0, pvalue=0.27421346048040524)\n",
      "2) Mann Whitney U: Asian - African American:     MannwhitneyuResult(statistic=99782.5, pvalue=1.2056491003420835e-05)\n",
      "3) Mann Whitney U: African American - Caucasian: MannwhitneyuResult(statistic=2516504.5, pvalue=6.626649580056938e-11)\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Mann Whitney U: Asian - Caucasian:           \", scipy.stats.mannwhitneyu(asian, cauc))\n",
    "print(\"2) Mann Whitney U: Asian - African American:    \", scipy.stats.mannwhitneyu(asian, african))\n",
    "print(\"3) Mann Whitney U: African American - Caucasian:\", scipy.stats.mannwhitneyu(african, cauc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "1    3650\n",
       "2    4350\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the amount of men and women in train + test set\n",
    "all_data.groupby([\"gender\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ethnicity  gender\n",
       "1          1           82\n",
       "           2          201\n",
       "2          1         3300\n",
       "           2         3570\n",
       "3          1          268\n",
       "           2          579\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive statistics grouped by ethnicity and gender\n",
    "all_data.groupby(['ethnicity','gender']).size()           # imbalanced groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with personality traits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All models that will be trained:\n",
    "- Model 1 = Job interview ~ Personality traits\n",
    "- Model 2 = Job interview ~ Personality traits + Gender\n",
    "- Model 3 = Job interview ~ Personality traits + Ethnicity\n",
    "- Model 4 = Job interview ~ Personality traits + Gender + Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - Job interview ~ personality traits\n",
    "#### Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR MSE 0.00186\n",
      "MLR MAE 0.0344\n",
      "MLR 1 - MAE = 0.9656\n"
     ]
    }
   ],
   "source": [
    "# Splitting the (in)dependent variables of training set\n",
    "X_1_train_pers = merged_df[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness']].values\n",
    "y_1_train_pers = merged_df.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_1_test_pers = merged_df_test[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness']].values\n",
    "y_1_test_pers = merged_df_test.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_1_train_pers, y_1_train_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_1_pred_pers = regressor.predict(X_1_test_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MLR MSE {:.3}\".format(mean_squared_error(y_1_test_pers, y_1_pred_pers)))\n",
    "print(\"MLR MAE {:.3}\".format(mean_absolute_error(y_1_test_pers, y_1_pred_pers)))\n",
    "print(\"MLR 1 - MAE = {:.4}\".format(1-mean_absolute_error(y_1_test_pers, y_1_pred_pers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830107663551402"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a baseline by taking the MAE of the test set outcome variable and the mean of the training set outcome variable\n",
    "1 - mean_absolute_error(y_1_test_pers, np.full(2000, np.mean(y_1_train_pers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00235\n",
      "MAE 0.0382\n",
      "1 - MAE 0.9618\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_1_train_pers, y_1_train_pers.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_1_pers = rfr.predict(X_1_test_pers)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_1_test_pers, rfr_y_1_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_1_test_pers, rfr_y_1_pers)))\n",
    "print(\"1 - MAE {:.4}\".format((1-mean_absolute_error(y_1_test_pers, rfr_y_1_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00205\n",
      "MAE 0.0359\n",
      "1 - MAE 0.9641\n"
     ]
    }
   ],
   "source": [
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_1_train_pers, y_1_train_pers.ravel())\n",
    "svr_y_1_pers = svr.predict(X_1_test_pers)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_1_test_pers, svr_y_1_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_1_test_pers, svr_y_1_pers)))\n",
    "print(\"1 - MAE {:.4}\".format(1-(mean_absolute_error(y_1_test_pers, svr_y_1_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 - Job interview ~ personality traits + gender\n",
    "#### Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR MSE 0.00184\n",
      "MLR MAE 0.0342\n",
      "MLR 1 - MAE = 0.9658\n"
     ]
    }
   ],
   "source": [
    "# Splitting the (in)dependent variables of training set\n",
    "X_2_train_pers = merged_df[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness', 'gender']].values\n",
    "y_2_train_pers = merged_df.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_2_test_pers = merged_df_test[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness', 'gender']].values\n",
    "y_2_test_pers = merged_df_test.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_2_train_pers[:, -1] = le.fit_transform(X_2_train_pers[:, -1])\n",
    "X_2_test_pers[:, -1] = le.fit_transform(X_2_test_pers[:, -1])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_2_train_pers, y_2_train_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_2_pred_pers = regressor.predict(X_2_test_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MLR MSE {:.3}\".format(mean_squared_error(y_2_test_pers, y_2_pred_pers)))\n",
    "print(\"MLR MAE {:.3}\".format(mean_absolute_error(y_2_test_pers, y_2_pred_pers)))\n",
    "print(\"MLR 1 - MAE = {:.4}\".format(1-mean_absolute_error(y_2_test_pers, y_2_pred_pers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00236\n",
      "MAE 0.0383\n",
      "1 - MAE 0.9617\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_2_train_pers, y_2_train_pers.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_2_pers = rfr.predict(X_2_test_pers)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_2_test_pers, rfr_y_2_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_2_test_pers, rfr_y_2_pers)))\n",
    "print(\"1 - MAE {:.4}\".format((1-mean_absolute_error(y_2_test_pers, rfr_y_2_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00195\n",
      "MAE 0.0349\n",
      "1 - MAE 0.9651\n"
     ]
    }
   ],
   "source": [
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_2_train_pers, y_2_train_pers.ravel())\n",
    "svr_y_2_pers = svr.predict(X_2_test_pers)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_2_test_pers, svr_y_2_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_2_test_pers, svr_y_2_pers)))\n",
    "print(\"1 - MAE {:.4}\".format(1-(mean_absolute_error(y_2_test_pers, svr_y_2_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - Job interview ~ personality traits + ethnicity\n",
    "#### Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR MSE 0.00186\n",
      "MLR MAE 0.0344\n",
      "MLR 1 - MAE = 0.9656\n"
     ]
    }
   ],
   "source": [
    "# Splitting the (in)dependent variables of training set\n",
    "X_3_train_pers = merged_df[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness', 'ethnicity']].values\n",
    "y_3_train_pers = merged_df.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_3_test_pers = merged_df_test[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness', 'ethnicity']].values\n",
    "y_3_test_pers = merged_df_test.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "X_3_train_pers = np.array(ct.fit_transform(X_3_train_pers))\n",
    "X_3_test_pers = np.array(ct.fit_transform(X_3_test_pers))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_3_train_dummy_pers = X_3_train_pers.copy()\n",
    "X_3_train_dummy_pers = X_3_train_dummy_pers[:,1:]\n",
    "\n",
    "X_3_test_dummy_pers = X_3_test_pers.copy()\n",
    "X_3_test_dummy_pers = X_3_test_dummy_pers[:,1:]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_3_train_dummy_pers, y_3_train_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_3_pred_pers = regressor.predict(X_3_test_dummy_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MLR MSE {:.3}\".format(mean_squared_error(y_3_test_pers, y_3_pred_pers)))\n",
    "print(\"MLR MAE {:.3}\".format(mean_absolute_error(y_3_test_pers, y_3_pred_pers)))\n",
    "print(\"MLR 1 - MAE = {:.4}\".format(1-mean_absolute_error(y_3_test_pers, y_3_pred_pers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00233\n",
      "MAE 0.0381\n",
      "1 - MAE 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_3_train_dummy_pers, y_3_train_pers.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_3_pers = rfr.predict(X_3_test_dummy_pers)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_3_test_pers, rfr_y_3_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_3_test_pers, rfr_y_3_pers)))\n",
    "print(\"1 - MAE {:.4}\".format((1 - mean_absolute_error(y_3_test_pers, rfr_y_3_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0019\n",
      "MAE 0.0346\n",
      "1 - MAE 0.9654\n",
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vectore regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_3_train_pers, y_3_train_pers.ravel())\n",
    "svr_y_3_pers = svr.predict(X_3_test_pers)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_3_test_pers, svr_y_3_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_3_test_pers, svr_y_3_pers)))\n",
    "print(\"1 - MAE {:.4}\".format((1-mean_absolute_error(y_3_test_pers, svr_y_3_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - Job interview ~ personality traits + gender + ethnicity\n",
    "#### Multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR MSE 0.00184\n",
      "MLR MAE 0.0341\n",
      "MLR 1 - MAE = 0.9659\n"
     ]
    }
   ],
   "source": [
    "# Splitting the (in)dependent variables\n",
    "X_4_train_pers = merged_df[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness', 'ethnicity', 'gender']].values\n",
    "y_4_train_pers = merged_df.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_4_test_pers = merged_df_test[['extraversion', 'neuroticism', 'agreeableness','conscientiousness', 'openness', 'ethnicity', 'gender']].values\n",
    "y_4_test_pers = merged_df_test.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_4_train_pers[:, -1] = le.fit_transform(X_4_train_pers[:, -1])\n",
    "X_4_test_pers[:, -1] = le.fit_transform(X_4_test_pers[:, -1])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-2])], remainder='passthrough')\n",
    "X_4_train_pers = np.array(ct.fit_transform(X_4_train_pers))\n",
    "X_4_test_pers = np.array(ct.fit_transform(X_4_test_pers))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_4_train_dummy_pers = X_4_train_pers.copy()\n",
    "X_4_train_dummy_pers = X_4_train_dummy_pers[:,1:]\n",
    "\n",
    "X_4_test_dummy_pers = X_4_test_pers.copy()\n",
    "X_4_test_dummy_pers = X_4_test_dummy_pers[:,1:]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_4_train_dummy_pers, y_4_train_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_4_pred_pers = regressor.predict(X_4_test_dummy_pers)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MLR MSE {:.3}\".format(mean_squared_error(y_4_test_pers, y_4_pred_pers)))\n",
    "print(\"MLR MAE {:.3}\".format(mean_absolute_error(y_4_test_pers, y_4_pred_pers)))\n",
    "print(\"MLR 1 - MAE = {:.4}\".format(1-mean_absolute_error(y_4_test_pers, y_4_pred_pers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00232\n",
      "MAE 0.0381\n",
      "1 - MAE 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_4_train_dummy_pers, y_4_train_pers.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_4_pers = rfr.predict(X_4_test_dummy_pers)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_4_test_pers, rfr_y_4_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_4_test_pers, rfr_y_4_pers)))\n",
    "print(\"1 - MAE {:.4}\".format((1-mean_absolute_error(y_4_test_pers, rfr_y_4_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.00191\n",
      "MAE 0.0346\n",
      "MAE 0.9654\n"
     ]
    }
   ],
   "source": [
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_4_train_pers, y_4_train_pers.ravel())\n",
    "svr_y_4_pers = svr.predict(X_4_test_pers)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_4_test_pers, svr_y_4_pers)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_4_test_pers, svr_y_4_pers)))\n",
    "print(\"MAE {:.4}\".format((1-mean_absolute_error(y_4_test_pers, svr_y_4_pers))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-testing for significant differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.91912579]\n",
      "[0.51375095]\n"
     ]
    }
   ],
   "source": [
    "# Related t-test MLR\n",
    "print(scipy.stats.ttest_rel(y_1_pred_pers, y_2_pred_pers).pvalue)       # models 1 and 2\n",
    "print(scipy.stats.ttest_rel(y_1_pred_pers, y_4_pred_pers).pvalue)       # models 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0426634532375259\n",
      "1.1843273470201267e-07\n",
      "0.19685857924544387\n"
     ]
    }
   ],
   "source": [
    "# Related t-test SVR\n",
    "print(scipy.stats.ttest_rel(svr_y_1_pers, svr_y_2_pers).pvalue)         # models 1 and 2\n",
    "print(scipy.stats.ttest_rel(svr_y_1_pers, svr_y_3_pers).pvalue)         # models 1 and 3\n",
    "print(scipy.stats.ttest_rel(svr_y_1_pers, svr_y_4_pers).pvalue)         # models 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3372010023651444\n",
      "0.15419650695038237\n",
      "0.3424353057858508\n"
     ]
    }
   ],
   "source": [
    "# Related t-test RFR\n",
    "print(scipy.stats.ttest_rel(rfr_y_1_pers, rfr_y_2_pers).pvalue)         # models 1 and 2\n",
    "print(scipy.stats.ttest_rel(rfr_y_1_pers, rfr_y_3_pers).pvalue)         # models 1 and 3\n",
    "print(scipy.stats.ttest_rel(rfr_y_1_pers, rfr_y_4_pers).pvalue)         # models 1 and 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Data preparation OpenFace data</h1> \n",
    "<h3 align=\"center\">Creating datasets with mean, min, max and standard deviations for Amplitude, Velocity and Acceleration</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove unused columns, and rows with faces that were not succesfully detected\n",
    "# Subsequently calculates the Amplitude, Velocity and Acceleration\n",
    "# Rows = per video \n",
    "# Columns = all outcome scores for Amplitude, Velocity and Acceleration\n",
    "\n",
    "def clean(df):\n",
    "    # Dismiss features in which faces were not succesfully detected\n",
    "    df = df[df[' success'] == 1]\n",
    "    \n",
    "    # Removing landmark information that will not be used\n",
    "    col_list = []\n",
    "    for column in df.columns:\n",
    "        if \"eye\" in column:\n",
    "            col_list.append(column)\n",
    "        elif \"x_\" in column:\n",
    "            col_list.append(column)\n",
    "        elif \"y_\" in column:\n",
    "            col_list.append(column)\n",
    "        elif \"X_\" in column:\n",
    "            col_list.append(column)\n",
    "        elif \"Y_\" in column:\n",
    "            col_list.append(column)\n",
    "        elif \"Z_\" in column:\n",
    "            col_list.append(column)\n",
    "        elif \"p_\" in column:\n",
    "            col_list.append(column)\n",
    "    \n",
    "    df.drop(col_list, axis = 1, inplace = True)\n",
    "\n",
    "    # Copy dataframe\n",
    "    amplitude = df.copy()\n",
    "    # ---- Amplitude ----\n",
    "    amplitude_mean = amplitude.mean().add_suffix('_amplitude').add_suffix('_mean')\n",
    "    amplitude_min = amplitude.min().add_suffix('_amplitude').add_suffix('_min')\n",
    "    amplitude_max = amplitude.max().add_suffix('_amplitude').add_suffix('_max')\n",
    "    amplitude_std = amplitude.std().add_suffix('_amplitude').add_suffix('_std')\n",
    "    # Concatenate all\n",
    "    amplitude_df = pd.concat([amplitude_mean, amplitude_min, amplitude_max, amplitude_std])\n",
    "    #print(\"ampli shape:\", amplitude_df.shape)\n",
    "    \n",
    "# ---------------------------------\n",
    "    # Copy dataframe\n",
    "    velocity = df.copy()\n",
    "    # ---- Velocity ----\n",
    "    velocity = velocity.diff()\n",
    "\n",
    "    velocity_mean = velocity.mean().add_suffix('_velocity').add_suffix('_mean')\n",
    "    velocity_min = velocity.min().add_suffix('_velocity').add_suffix('_min')\n",
    "    velocity_max = velocity.max().add_suffix('_velocity').add_suffix('_max')\n",
    "    velocity_std = velocity.std().add_suffix('_velocity').add_suffix('_std')\n",
    "    # Concatenate all\n",
    "    velocity_df = pd.concat([velocity_mean, velocity_min, velocity_max, velocity_std])\n",
    "    #print(\"velo shape:\", velocity_df.shape)\n",
    "    \n",
    "# ---------------------------------\n",
    "    # Copy dataframe\n",
    "    acceleration = df.copy()\n",
    "    # ---- Acceleration ----\n",
    "    acceleration = acceleration.diff().diff()\n",
    "\n",
    "    acceleration_mean = acceleration.mean().add_suffix('_acceleration').add_suffix('_mean')\n",
    "    acceleration_min = acceleration.min().add_suffix('_acceleration').add_suffix('_min')\n",
    "    acceleration_max = acceleration.max().add_suffix('_acceleration').add_suffix('_max')\n",
    "    acceleration_std = acceleration.std().add_suffix('_acceleration').add_suffix('_std')\n",
    "    # Concatenate all\n",
    "    acceleration_df = pd.concat([acceleration_mean, acceleration_min, acceleration_max, acceleration_std])\n",
    "    #print(\"accel shape:\", acceleration_df.shape)\n",
    "\n",
    "# ---------------------------------\n",
    "    # Concatenate all outcomes into 1 dataframe\n",
    "    df_final = pd.concat([amplitude_df, velocity_df, acceleration_df])\n",
    "    df_final = pd.DataFrame(df_final)\n",
    "    \n",
    "    return df_final.T                             # Transpose to make the rows the columns and create one row per video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training dataset using the 'clean()' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: train_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marie-Claire\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: train_2\n",
      "Starting with: train_3\n",
      "Starting with: train_4\n",
      "Starting with: train_5\n",
      "Starting with: train_6\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6000 entries, 0 to 5999\n",
      "Columns: 649 entries, frame_amplitude_mean to videoname\n",
      "dtypes: float64(648), object(1)\n",
      "memory usage: 29.7+ MB\n",
      "Wall time: 18min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path_1 = # Please insert your own path\n",
    "\n",
    "# Looping over the training folders in the directory to create one training dataframe\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(path_1):\n",
    "    if \"train_\" in file and \"zip\" not in file:\n",
    "        print(\"Starting with:\", file)\n",
    "        for csv_file in os.listdir(path_1+\"/\"+file):\n",
    "            df = pd.read_csv(path_1+\"/\"+file+\"/\"+csv_file)\n",
    "            df = clean(df)\n",
    "            df[\"videoname\"] = csv_file\n",
    "            df_train = pd.concat([df_train, df]).copy()\n",
    "\n",
    "# Setting the dataframe index\n",
    "df_train.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# The videos were annotated as video.csv, changing this to .mp4 to be able to merge dataframes\n",
    "df_train['videoname'] = df_train['videoname'].str.replace('csv','mp4')\n",
    "    \n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the validation (test) dataset using the 'clean()' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: validation_1\n",
      "Starting with: validation_2\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 649 entries, frame_amplitude_mean to videoname\n",
      "dtypes: float64(648), object(1)\n",
      "memory usage: 9.9+ MB\n",
      "None\n",
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Looping over the training folders in the directory to create one validation (test) dataframe\n",
    "df_val = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(path_1):\n",
    "    if \"validation\" in file and \"zip\" not in file:\n",
    "        print(\"Starting with:\", file)\n",
    "        for csv_file in os.listdir(path_1+\"/\"+file):\n",
    "            df = pd.read_csv(path_1+\"/\"+file+\"/\"+csv_file)\n",
    "            df = clean(df)\n",
    "            df[\"videoname\"] = csv_file\n",
    "            df_val = pd.concat([df_val, df]).copy()\n",
    "\n",
    "# Setting the dataframe index\n",
    "df_val.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# The videos were annotated as video.csv, changing to .mp4 to be able to merge dataframes\n",
    "df_val['videoname'] = df_val['videoname'].str.replace('csv','mp4')\n",
    "\n",
    "print(df_val.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             videoname  extraversion  neuroticism  agreeableness  \\\n",
      "0  modNfUPt3F4.002.mp4      0.644860     0.593750       0.615385   \n",
      "1  h6LOjpCRXtY.005.mp4      0.439252     0.520833       0.417582   \n",
      "\n",
      "   conscientiousness  interview  openness    youtubeid  ethnicity  gender  \n",
      "0           0.640777   0.616822  0.555556  modNfUPt3F4          2       1  \n",
      "1           0.572816   0.439252  0.411111  h6LOjpCRXtY          1       2  \n"
     ]
    }
   ],
   "source": [
    "# Preparing the provided validation set to use it as the test set\n",
    "\n",
    "# Converting the annotation validation data to a dataframe and specifying the index column\n",
    "anno_val_df = pd.DataFrame(annotation_validation)\n",
    "anno_val_df.reset_index(inplace = True)\n",
    "\n",
    "# Renaming column to merge the dataframes\n",
    "anno_val_df.rename(columns = {\"index\" : \"VideoName\", \n",
    "                         }, inplace = True)\n",
    "\n",
    "# Merging the dataframes anno_val_df and excel_data on the VideoName column\n",
    "merged_val_df = pd.merge(anno_val_df, excel_data, how = \"left\", on = [\"VideoName\"])\n",
    "\n",
    "# Converting columns to lowercase\n",
    "merged_val_df.columns = merged_val_df.columns.str.lower()\n",
    "print(merged_val_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Predictions with OpenFace features</h1>\n",
    "\n",
    "#### Models that will be trained:\n",
    "- Model 1 = Job interview ~ Facial\n",
    "- Model 2 = Job interview ~ Facial + Gender\n",
    "- Model 3 = Job interview ~ Facial + Ethnicity\n",
    "- Model 4 = Job interview ~ Facial + Gender + Ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Interview ~ facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation (test) set for Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 1, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_int = merged_df[['videoname', 'interview']]\n",
    "training_face = pd.merge(df_train, df_int, how = \"left\", on = [\"videoname\"])\n",
    "#training_face.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 1\n",
    "df_val_int = merged_val_df[['videoname', 'interview']]\n",
    "val_face = pd.merge(df_val, df_val_int, how = \"left\", on = [\"videoname\"])\n",
    "#val_face.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the (in)dependent variables\n",
    "X_1_train = training_face.drop(['videoname', 'interview'], axis=1).values\n",
    "y_1_train = training_face.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_1_test = val_face.drop(['videoname', 'interview'], axis=1).values\n",
    "y_1_test = val_face.loc[:,['interview']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830107663551402"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a baseline by taking the MAE of the test set outcome variable and the mean of the training set outcome variable\n",
    "1 - mean_absolute_error(y_1_test, np.full(2000, np.mean(y_1_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0292\n",
      "MAE 0.101\n",
      "1 - MAE 0.899\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_1_train, y_1_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_1_pred = regressor.predict(X_1_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_1_test, y_1_pred)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_1_test, y_1_pred)))\n",
    "print(\"1 - MAE {:.3}\".format((1 - mean_absolute_error(y_1_test, y_1_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.00561\n",
      "Feature: 1, Score: 0.00000\n",
      "Feature: 2, Score: -0.21473\n",
      "Feature: 3, Score: -0.00363\n",
      "Feature: 4, Score: 0.00000\n",
      "Feature: 5, Score: 0.19894\n",
      "Feature: 6, Score: -0.75471\n",
      "Feature: 7, Score: 0.20148\n",
      "Feature: 8, Score: 0.01604\n",
      "Feature: 9, Score: -0.80042\n",
      "Feature: 10, Score: -0.56201\n",
      "Feature: 11, Score: -0.25391\n",
      "Feature: 12, Score: 1.55644\n",
      "Feature: 13, Score: 0.00003\n",
      "Feature: 14, Score: 0.00022\n",
      "Feature: 15, Score: 0.00008\n",
      "Feature: 16, Score: 0.08049\n",
      "Feature: 17, Score: 0.01543\n",
      "Feature: 18, Score: -0.06251\n",
      "Feature: 19, Score: 0.00246\n",
      "Feature: 20, Score: -0.06866\n",
      "Feature: 21, Score: 0.00204\n",
      "Feature: 22, Score: 0.04699\n",
      "Feature: 23, Score: -0.00237\n",
      "Feature: 24, Score: 0.00462\n",
      "Feature: 25, Score: 0.25508\n",
      "Feature: 26, Score: 0.02053\n",
      "Feature: 27, Score: -0.00554\n",
      "Feature: 28, Score: 0.05553\n",
      "Feature: 29, Score: -0.13140\n",
      "Feature: 30, Score: 0.01573\n",
      "Feature: 31, Score: 0.10742\n",
      "Feature: 32, Score: 0.15533\n",
      "Feature: 33, Score: 0.07980\n",
      "Feature: 34, Score: 0.04808\n",
      "Feature: 35, Score: -0.08219\n",
      "Feature: 36, Score: -0.10813\n",
      "Feature: 37, Score: -0.03493\n",
      "Feature: 38, Score: -0.01090\n",
      "Feature: 39, Score: 0.03927\n",
      "Feature: 40, Score: 0.04209\n",
      "Feature: 41, Score: -0.00689\n",
      "Feature: 42, Score: -0.02220\n",
      "Feature: 43, Score: 0.00361\n",
      "Feature: 44, Score: 0.04241\n",
      "Feature: 45, Score: -0.07106\n",
      "Feature: 46, Score: -0.09187\n",
      "Feature: 47, Score: -0.08044\n",
      "Feature: 48, Score: 0.05857\n",
      "Feature: 49, Score: -0.01409\n",
      "Feature: 50, Score: -0.07765\n",
      "Feature: 51, Score: -0.18547\n",
      "Feature: 52, Score: 0.00932\n",
      "Feature: 53, Score: 0.00384\n",
      "Feature: 54, Score: 0.00585\n",
      "Feature: 55, Score: -0.00000\n",
      "Feature: 56, Score: -0.18794\n",
      "Feature: 57, Score: 0.02256\n",
      "Feature: 58, Score: 0.00000\n",
      "Feature: 59, Score: 0.02239\n",
      "Feature: 60, Score: 0.11143\n",
      "Feature: 61, Score: 0.29338\n",
      "Feature: 62, Score: -0.01962\n",
      "Feature: 63, Score: 0.08045\n",
      "Feature: 64, Score: -0.24160\n",
      "Feature: 65, Score: -0.01723\n",
      "Feature: 66, Score: -0.14520\n",
      "Feature: 67, Score: -0.00000\n",
      "Feature: 68, Score: 0.00018\n",
      "Feature: 69, Score: 0.00020\n",
      "Feature: 70, Score: -0.01567\n",
      "Feature: 71, Score: -0.00515\n",
      "Feature: 72, Score: 0.00034\n",
      "Feature: 73, Score: -0.00000\n",
      "Feature: 74, Score: -0.00000\n",
      "Feature: 75, Score: -0.01582\n",
      "Feature: 76, Score: -0.00000\n",
      "Feature: 77, Score: -0.00995\n",
      "Feature: 78, Score: -0.01754\n",
      "Feature: 79, Score: -0.00000\n",
      "Feature: 80, Score: -0.02554\n",
      "Feature: 81, Score: 0.02169\n",
      "Feature: 82, Score: -0.00399\n",
      "Feature: 83, Score: -0.00000\n",
      "Feature: 84, Score: 0.00000\n",
      "Feature: 85, Score: -0.00000\n",
      "Feature: 86, Score: 0.00000\n",
      "Feature: 87, Score: -0.00000\n",
      "Feature: 88, Score: -0.00000\n",
      "Feature: 89, Score: 0.00000\n",
      "Feature: 90, Score: 0.00000\n",
      "Feature: 91, Score: 0.00000\n",
      "Feature: 92, Score: -0.06218\n",
      "Feature: 93, Score: 0.02653\n",
      "Feature: 94, Score: -0.06049\n",
      "Feature: 95, Score: 0.09269\n",
      "Feature: 96, Score: -0.00000\n",
      "Feature: 97, Score: -0.00154\n",
      "Feature: 98, Score: 0.12021\n",
      "Feature: 99, Score: 0.05040\n",
      "Feature: 100, Score: -0.00000\n",
      "Feature: 101, Score: -0.00000\n",
      "Feature: 102, Score: 0.00000\n",
      "Feature: 103, Score: 0.00035\n",
      "Feature: 104, Score: -0.00000\n",
      "Feature: 105, Score: -0.00000\n",
      "Feature: 106, Score: 0.00000\n",
      "Feature: 107, Score: -0.00000\n",
      "Feature: 108, Score: -0.00807\n",
      "Feature: 109, Score: -0.00000\n",
      "Feature: 110, Score: 0.28167\n",
      "Feature: 111, Score: 5.74828\n",
      "Feature: 112, Score: 0.00000\n",
      "Feature: 113, Score: -0.13501\n",
      "Feature: 114, Score: -0.13278\n",
      "Feature: 115, Score: -0.01180\n",
      "Feature: 116, Score: 0.06506\n",
      "Feature: 117, Score: -0.12248\n",
      "Feature: 118, Score: -0.16604\n",
      "Feature: 119, Score: 0.06131\n",
      "Feature: 120, Score: 0.23807\n",
      "Feature: 121, Score: -0.00009\n",
      "Feature: 122, Score: -0.00043\n",
      "Feature: 123, Score: -0.00020\n",
      "Feature: 124, Score: 0.04297\n",
      "Feature: 125, Score: -0.03387\n",
      "Feature: 126, Score: 0.02195\n",
      "Feature: 127, Score: -0.01094\n",
      "Feature: 128, Score: -0.00052\n",
      "Feature: 129, Score: 0.00186\n",
      "Feature: 130, Score: 0.00619\n",
      "Feature: 131, Score: 0.00455\n",
      "Feature: 132, Score: -0.00180\n",
      "Feature: 133, Score: 0.03084\n",
      "Feature: 134, Score: -0.01360\n",
      "Feature: 135, Score: 0.00489\n",
      "Feature: 136, Score: -0.01124\n",
      "Feature: 137, Score: -0.00427\n",
      "Feature: 138, Score: -0.00685\n",
      "Feature: 139, Score: -0.00092\n",
      "Feature: 140, Score: 0.01040\n",
      "Feature: 141, Score: 0.00047\n",
      "Feature: 142, Score: 0.00739\n",
      "Feature: 143, Score: -0.00225\n",
      "Feature: 144, Score: -0.17731\n",
      "Feature: 145, Score: -0.04187\n",
      "Feature: 146, Score: 0.02011\n",
      "Feature: 147, Score: -0.03163\n",
      "Feature: 148, Score: 0.01131\n",
      "Feature: 149, Score: -0.05447\n",
      "Feature: 150, Score: -0.01154\n",
      "Feature: 151, Score: 0.03910\n",
      "Feature: 152, Score: -0.08544\n",
      "Feature: 153, Score: -0.07644\n",
      "Feature: 154, Score: 0.11923\n",
      "Feature: 155, Score: -0.16203\n",
      "Feature: 156, Score: -0.03465\n",
      "Feature: 157, Score: -0.00073\n",
      "Feature: 158, Score: 0.04503\n",
      "Feature: 159, Score: -0.06452\n",
      "Feature: 160, Score: 0.08509\n",
      "Feature: 161, Score: 0.16703\n",
      "Feature: 162, Score: 0.01740\n",
      "Feature: 163, Score: 0.00000\n",
      "Feature: 164, Score: -0.56526\n",
      "Feature: 165, Score: -0.55748\n",
      "Feature: 166, Score: -0.00000\n",
      "Feature: 167, Score: 0.18048\n",
      "Feature: 168, Score: 0.57451\n",
      "Feature: 169, Score: 0.06990\n",
      "Feature: 170, Score: -0.23007\n",
      "Feature: 171, Score: 0.84648\n",
      "Feature: 172, Score: 0.90115\n",
      "Feature: 173, Score: -0.28595\n",
      "Feature: 174, Score: -1.16577\n",
      "Feature: 175, Score: 0.00004\n",
      "Feature: 176, Score: -0.00022\n",
      "Feature: 177, Score: -0.00006\n",
      "Feature: 178, Score: -0.17667\n",
      "Feature: 179, Score: 0.36734\n",
      "Feature: 180, Score: 0.06805\n",
      "Feature: 181, Score: 0.00199\n",
      "Feature: 182, Score: 0.04131\n",
      "Feature: 183, Score: -0.01566\n",
      "Feature: 184, Score: -0.04174\n",
      "Feature: 185, Score: -0.02827\n",
      "Feature: 186, Score: -0.00073\n",
      "Feature: 187, Score: -0.28685\n",
      "Feature: 188, Score: -0.00482\n",
      "Feature: 189, Score: 0.06835\n",
      "Feature: 190, Score: -0.00474\n",
      "Feature: 191, Score: 0.04768\n",
      "Feature: 192, Score: -0.02392\n",
      "Feature: 193, Score: -0.06419\n",
      "Feature: 194, Score: -0.08025\n",
      "Feature: 195, Score: -0.09779\n",
      "Feature: 196, Score: -0.10515\n",
      "Feature: 197, Score: 0.03228\n",
      "Feature: 198, Score: 0.16307\n",
      "Feature: 199, Score: 0.08948\n",
      "Feature: 200, Score: 0.04813\n",
      "Feature: 201, Score: 0.00961\n",
      "Feature: 202, Score: 0.01660\n",
      "Feature: 203, Score: 0.00206\n",
      "Feature: 204, Score: 0.06818\n",
      "Feature: 205, Score: 0.01857\n",
      "Feature: 206, Score: -0.06448\n",
      "Feature: 207, Score: 0.05143\n",
      "Feature: 208, Score: 0.06158\n",
      "Feature: 209, Score: 0.02972\n",
      "Feature: 210, Score: -0.05286\n",
      "Feature: 211, Score: 0.08718\n",
      "Feature: 212, Score: -0.04949\n",
      "Feature: 213, Score: 0.12548\n",
      "Feature: 214, Score: -0.04032\n",
      "Feature: 215, Score: -0.01200\n",
      "Feature: 216, Score: 0.20590\n",
      "Feature: 217, Score: 0.00000\n",
      "Feature: 218, Score: -5.68701\n",
      "Feature: 219, Score: 7.56581\n",
      "Feature: 220, Score: 0.00000\n",
      "Feature: 221, Score: 37.35126\n",
      "Feature: 222, Score: 18.63928\n",
      "Feature: 223, Score: 26.36095\n",
      "Feature: 224, Score: 29.28147\n",
      "Feature: 225, Score: 53.71890\n",
      "Feature: 226, Score: -21.82384\n",
      "Feature: 227, Score: -62.50918\n",
      "Feature: 228, Score: -82.84406\n",
      "Feature: 229, Score: -0.01229\n",
      "Feature: 230, Score: 0.01260\n",
      "Feature: 231, Score: 0.04008\n",
      "Feature: 232, Score: 8.64225\n",
      "Feature: 233, Score: 0.77680\n",
      "Feature: 234, Score: 5.06347\n",
      "Feature: 235, Score: 0.55052\n",
      "Feature: 236, Score: -0.52097\n",
      "Feature: 237, Score: 0.38652\n",
      "Feature: 238, Score: 1.65645\n",
      "Feature: 239, Score: 2.02807\n",
      "Feature: 240, Score: -0.11866\n",
      "Feature: 241, Score: -2.45469\n",
      "Feature: 242, Score: -1.48782\n",
      "Feature: 243, Score: -3.58282\n",
      "Feature: 244, Score: 1.39070\n",
      "Feature: 245, Score: 0.22511\n",
      "Feature: 246, Score: -1.79362\n",
      "Feature: 247, Score: 0.37856\n",
      "Feature: 248, Score: 1.51677\n",
      "Feature: 249, Score: -0.96112\n",
      "Feature: 250, Score: -0.12122\n",
      "Feature: 251, Score: 1.59613\n",
      "Feature: 252, Score: 0.03509\n",
      "Feature: 253, Score: 0.50499\n",
      "Feature: 254, Score: 1.50491\n",
      "Feature: 255, Score: -0.75149\n",
      "Feature: 256, Score: -2.63878\n",
      "Feature: 257, Score: 0.75917\n",
      "Feature: 258, Score: 3.31496\n",
      "Feature: 259, Score: 1.31414\n",
      "Feature: 260, Score: 2.92427\n",
      "Feature: 261, Score: -1.71287\n",
      "Feature: 262, Score: -0.26668\n",
      "Feature: 263, Score: 0.61121\n",
      "Feature: 264, Score: -2.77103\n",
      "Feature: 265, Score: -0.70104\n",
      "Feature: 266, Score: -0.02121\n",
      "Feature: 267, Score: -0.61175\n",
      "Feature: 268, Score: 7.71898\n",
      "Feature: 269, Score: -1.64089\n",
      "Feature: 270, Score: 0.00000\n",
      "Feature: 271, Score: 0.00000\n",
      "Feature: 272, Score: -4.03949\n",
      "Feature: 273, Score: -0.10128\n",
      "Feature: 274, Score: -0.00000\n",
      "Feature: 275, Score: 0.01430\n",
      "Feature: 276, Score: 0.05968\n",
      "Feature: 277, Score: -0.19313\n",
      "Feature: 278, Score: -0.05160\n",
      "Feature: 279, Score: -0.02068\n",
      "Feature: 280, Score: -0.07334\n",
      "Feature: 281, Score: 0.01683\n",
      "Feature: 282, Score: 0.01743\n",
      "Feature: 283, Score: 0.00009\n",
      "Feature: 284, Score: 0.00091\n",
      "Feature: 285, Score: -0.00015\n",
      "Feature: 286, Score: 0.06390\n",
      "Feature: 287, Score: 0.19300\n",
      "Feature: 288, Score: -0.17469\n",
      "Feature: 289, Score: 0.00785\n",
      "Feature: 290, Score: 0.00486\n",
      "Feature: 291, Score: -0.00383\n",
      "Feature: 292, Score: -0.02541\n",
      "Feature: 293, Score: 0.04342\n",
      "Feature: 294, Score: -0.01711\n",
      "Feature: 295, Score: 0.03300\n",
      "Feature: 296, Score: 0.02943\n",
      "Feature: 297, Score: -0.01827\n",
      "Feature: 298, Score: -0.00587\n",
      "Feature: 299, Score: -0.00118\n",
      "Feature: 300, Score: -0.00014\n",
      "Feature: 301, Score: 0.01132\n",
      "Feature: 302, Score: -0.00337\n",
      "Feature: 303, Score: -0.01278\n",
      "Feature: 304, Score: 0.00215\n",
      "Feature: 305, Score: -0.02281\n",
      "Feature: 306, Score: 0.00523\n",
      "Feature: 307, Score: -0.00126\n",
      "Feature: 308, Score: 0.00211\n",
      "Feature: 309, Score: -0.06564\n",
      "Feature: 310, Score: 0.00039\n",
      "Feature: 311, Score: -0.02264\n",
      "Feature: 312, Score: -0.01561\n",
      "Feature: 313, Score: 0.01823\n",
      "Feature: 314, Score: -0.03769\n",
      "Feature: 315, Score: -0.02531\n",
      "Feature: 316, Score: 0.01411\n",
      "Feature: 317, Score: -0.10988\n",
      "Feature: 318, Score: -0.02403\n",
      "Feature: 319, Score: -0.00255\n",
      "Feature: 320, Score: -0.04503\n",
      "Feature: 321, Score: -0.11452\n",
      "Feature: 322, Score: 0.05988\n",
      "Feature: 323, Score: 0.04375\n",
      "Feature: 324, Score: -0.17125\n",
      "Feature: 325, Score: 0.00000\n",
      "Feature: 326, Score: 4.41015\n",
      "Feature: 327, Score: 0.05012\n",
      "Feature: 328, Score: -0.00000\n",
      "Feature: 329, Score: -0.05861\n",
      "Feature: 330, Score: 0.03420\n",
      "Feature: 331, Score: 0.13784\n",
      "Feature: 332, Score: 0.07631\n",
      "Feature: 333, Score: 0.01260\n",
      "Feature: 334, Score: 0.07312\n",
      "Feature: 335, Score: 0.02918\n",
      "Feature: 336, Score: -0.12087\n",
      "Feature: 337, Score: -0.00006\n",
      "Feature: 338, Score: -0.00078\n",
      "Feature: 339, Score: -0.00027\n",
      "Feature: 340, Score: -0.09524\n",
      "Feature: 341, Score: -0.06217\n",
      "Feature: 342, Score: 0.20601\n",
      "Feature: 343, Score: -0.00874\n",
      "Feature: 344, Score: 0.01924\n",
      "Feature: 345, Score: -0.00130\n",
      "Feature: 346, Score: -0.03107\n",
      "Feature: 347, Score: -0.01434\n",
      "Feature: 348, Score: 0.02883\n",
      "Feature: 349, Score: 0.02185\n",
      "Feature: 350, Score: -0.03000\n",
      "Feature: 351, Score: -0.06176\n",
      "Feature: 352, Score: 0.01398\n",
      "Feature: 353, Score: -0.00410\n",
      "Feature: 354, Score: 0.00689\n",
      "Feature: 355, Score: 0.01414\n",
      "Feature: 356, Score: -0.00839\n",
      "Feature: 357, Score: -0.00343\n",
      "Feature: 358, Score: -0.00763\n",
      "Feature: 359, Score: 0.00524\n",
      "Feature: 360, Score: 0.07396\n",
      "Feature: 361, Score: -0.00526\n",
      "Feature: 362, Score: -0.00820\n",
      "Feature: 363, Score: -0.03655\n",
      "Feature: 364, Score: -0.00366\n",
      "Feature: 365, Score: 0.03399\n",
      "Feature: 366, Score: -0.01403\n",
      "Feature: 367, Score: -0.00591\n",
      "Feature: 368, Score: 0.05662\n",
      "Feature: 369, Score: 0.03613\n",
      "Feature: 370, Score: -0.11717\n",
      "Feature: 371, Score: 0.00610\n",
      "Feature: 372, Score: 0.01077\n",
      "Feature: 373, Score: 0.01654\n",
      "Feature: 374, Score: 0.01542\n",
      "Feature: 375, Score: -0.15279\n",
      "Feature: 376, Score: -0.00025\n",
      "Feature: 377, Score: -0.12196\n",
      "Feature: 378, Score: 1.42463\n",
      "Feature: 379, Score: -0.00000\n",
      "Feature: 380, Score: -36.65132\n",
      "Feature: 381, Score: -4.61327\n",
      "Feature: 382, Score: 0.00000\n",
      "Feature: 383, Score: 0.58365\n",
      "Feature: 384, Score: 0.58010\n",
      "Feature: 385, Score: -4.76733\n",
      "Feature: 386, Score: -1.40289\n",
      "Feature: 387, Score: -4.48914\n",
      "Feature: 388, Score: 1.33906\n",
      "Feature: 389, Score: 0.47858\n",
      "Feature: 390, Score: 5.78302\n",
      "Feature: 391, Score: 0.01500\n",
      "Feature: 392, Score: 0.05268\n",
      "Feature: 393, Score: -0.00393\n",
      "Feature: 394, Score: 0.13120\n",
      "Feature: 395, Score: 0.56614\n",
      "Feature: 396, Score: -1.99761\n",
      "Feature: 397, Score: 0.81830\n",
      "Feature: 398, Score: -0.01965\n",
      "Feature: 399, Score: -0.04539\n",
      "Feature: 400, Score: 0.68948\n",
      "Feature: 401, Score: 0.31604\n",
      "Feature: 402, Score: -0.33810\n",
      "Feature: 403, Score: -0.02842\n",
      "Feature: 404, Score: 0.38930\n",
      "Feature: 405, Score: 0.60200\n",
      "Feature: 406, Score: -0.19937\n",
      "Feature: 407, Score: 0.33087\n",
      "Feature: 408, Score: 0.08549\n",
      "Feature: 409, Score: 0.67877\n",
      "Feature: 410, Score: -0.69302\n",
      "Feature: 411, Score: -0.36701\n",
      "Feature: 412, Score: 0.34116\n",
      "Feature: 413, Score: 0.13907\n",
      "Feature: 414, Score: 2.02328\n",
      "Feature: 415, Score: 1.63452\n",
      "Feature: 416, Score: -1.11882\n",
      "Feature: 417, Score: 0.39539\n",
      "Feature: 418, Score: -0.46849\n",
      "Feature: 419, Score: -0.23426\n",
      "Feature: 420, Score: 1.00758\n",
      "Feature: 421, Score: 1.49465\n",
      "Feature: 422, Score: 0.33723\n",
      "Feature: 423, Score: -0.05681\n",
      "Feature: 424, Score: 0.56584\n",
      "Feature: 425, Score: 0.68221\n",
      "Feature: 426, Score: -0.27043\n",
      "Feature: 427, Score: -1.35145\n",
      "Feature: 428, Score: 0.52060\n",
      "Feature: 429, Score: -0.42864\n",
      "Feature: 430, Score: -0.22023\n",
      "Feature: 431, Score: 0.14087\n",
      "Feature: 432, Score: 5.20346\n",
      "Feature: 433, Score: -0.00000\n",
      "Feature: 434, Score: -49.67028\n",
      "Feature: 435, Score: -77.01138\n",
      "Feature: 436, Score: -0.00000\n",
      "Feature: 437, Score: 121.91135\n",
      "Feature: 438, Score: -19.91104\n",
      "Feature: 439, Score: 147.05999\n",
      "Feature: 440, Score: 110.80196\n",
      "Feature: 441, Score: 52.81415\n",
      "Feature: 442, Score: -77.14972\n",
      "Feature: 443, Score: -217.45759\n",
      "Feature: 444, Score: -15.76909\n",
      "Feature: 445, Score: 0.02076\n",
      "Feature: 446, Score: -0.23041\n",
      "Feature: 447, Score: -0.20560\n",
      "Feature: 448, Score: 1.54398\n",
      "Feature: 449, Score: -6.66154\n",
      "Feature: 450, Score: 36.63622\n",
      "Feature: 451, Score: -1.62228\n",
      "Feature: 452, Score: 1.24983\n",
      "Feature: 453, Score: -5.94831\n",
      "Feature: 454, Score: 7.42242\n",
      "Feature: 455, Score: 7.18533\n",
      "Feature: 456, Score: 0.99082\n",
      "Feature: 457, Score: 8.60695\n",
      "Feature: 458, Score: -1.00997\n",
      "Feature: 459, Score: -6.65462\n",
      "Feature: 460, Score: -0.33370\n",
      "Feature: 461, Score: 0.84234\n",
      "Feature: 462, Score: -0.36874\n",
      "Feature: 463, Score: 0.96570\n",
      "Feature: 464, Score: -4.54858\n",
      "Feature: 465, Score: -0.52787\n",
      "Feature: 466, Score: 0.90265\n",
      "Feature: 467, Score: 0.31118\n",
      "Feature: 468, Score: 1.89527\n",
      "Feature: 469, Score: 1.47691\n",
      "Feature: 470, Score: 2.90444\n",
      "Feature: 471, Score: -0.19034\n",
      "Feature: 472, Score: -3.52897\n",
      "Feature: 473, Score: -0.14873\n",
      "Feature: 474, Score: -3.57840\n",
      "Feature: 475, Score: -2.40322\n",
      "Feature: 476, Score: 3.13990\n",
      "Feature: 477, Score: 2.16588\n",
      "Feature: 478, Score: 1.43715\n",
      "Feature: 479, Score: -0.68987\n",
      "Feature: 480, Score: 0.58451\n",
      "Feature: 481, Score: 0.35634\n",
      "Feature: 482, Score: -0.09559\n",
      "Feature: 483, Score: 2.12267\n",
      "Feature: 484, Score: 3.45400\n",
      "Feature: 485, Score: -3.92691\n",
      "Feature: 486, Score: -0.12434\n",
      "Feature: 487, Score: -0.00000\n",
      "Feature: 488, Score: 3.14828\n",
      "Feature: 489, Score: 0.09712\n",
      "Feature: 490, Score: 0.00000\n",
      "Feature: 491, Score: -0.03262\n",
      "Feature: 492, Score: 0.03268\n",
      "Feature: 493, Score: 0.06958\n",
      "Feature: 494, Score: -0.07761\n",
      "Feature: 495, Score: 0.01167\n",
      "Feature: 496, Score: 0.04602\n",
      "Feature: 497, Score: 0.10528\n",
      "Feature: 498, Score: -0.13148\n",
      "Feature: 499, Score: -0.00010\n",
      "Feature: 500, Score: -0.00053\n",
      "Feature: 501, Score: 0.00018\n",
      "Feature: 502, Score: -0.00742\n",
      "Feature: 503, Score: 0.05330\n",
      "Feature: 504, Score: 0.11256\n",
      "Feature: 505, Score: -0.04666\n",
      "Feature: 506, Score: 0.01506\n",
      "Feature: 507, Score: 0.00550\n",
      "Feature: 508, Score: 0.00681\n",
      "Feature: 509, Score: -0.04704\n",
      "Feature: 510, Score: 0.02729\n",
      "Feature: 511, Score: 0.02933\n",
      "Feature: 512, Score: -0.02301\n",
      "Feature: 513, Score: 0.00146\n",
      "Feature: 514, Score: -0.00612\n",
      "Feature: 515, Score: 0.00257\n",
      "Feature: 516, Score: -0.00109\n",
      "Feature: 517, Score: -0.01125\n",
      "Feature: 518, Score: 0.01512\n",
      "Feature: 519, Score: 0.01092\n",
      "Feature: 520, Score: 0.01717\n",
      "Feature: 521, Score: 0.00583\n",
      "Feature: 522, Score: -0.02230\n",
      "Feature: 523, Score: -0.02370\n",
      "Feature: 524, Score: 0.00689\n",
      "Feature: 525, Score: -0.00594\n",
      "Feature: 526, Score: -0.00516\n",
      "Feature: 527, Score: -0.00463\n",
      "Feature: 528, Score: -0.01331\n",
      "Feature: 529, Score: -0.01339\n",
      "Feature: 530, Score: 0.00195\n",
      "Feature: 531, Score: -0.00257\n",
      "Feature: 532, Score: -0.00263\n",
      "Feature: 533, Score: 0.00120\n",
      "Feature: 534, Score: 0.00379\n",
      "Feature: 535, Score: 0.00838\n",
      "Feature: 536, Score: -0.00149\n",
      "Feature: 537, Score: 0.00031\n",
      "Feature: 538, Score: -0.00357\n",
      "Feature: 539, Score: 0.00203\n",
      "Feature: 540, Score: 0.02821\n",
      "Feature: 541, Score: 0.00000\n",
      "Feature: 542, Score: -0.61510\n",
      "Feature: 543, Score: -0.04418\n",
      "Feature: 544, Score: -0.00000\n",
      "Feature: 545, Score: -0.00209\n",
      "Feature: 546, Score: -0.02474\n",
      "Feature: 547, Score: -0.09294\n",
      "Feature: 548, Score: -0.03901\n",
      "Feature: 549, Score: 0.07847\n",
      "Feature: 550, Score: -0.04606\n",
      "Feature: 551, Score: 0.04933\n",
      "Feature: 552, Score: -0.02333\n",
      "Feature: 553, Score: 0.00003\n",
      "Feature: 554, Score: 0.00013\n",
      "Feature: 555, Score: 0.00031\n",
      "Feature: 556, Score: 0.05358\n",
      "Feature: 557, Score: 0.05988\n",
      "Feature: 558, Score: -0.08422\n",
      "Feature: 559, Score: 0.00535\n",
      "Feature: 560, Score: -0.01897\n",
      "Feature: 561, Score: 0.02431\n",
      "Feature: 562, Score: 0.02709\n",
      "Feature: 563, Score: -0.01540\n",
      "Feature: 564, Score: -0.01200\n",
      "Feature: 565, Score: 0.03037\n",
      "Feature: 566, Score: 0.02603\n",
      "Feature: 567, Score: 0.02906\n",
      "Feature: 568, Score: -0.00153\n",
      "Feature: 569, Score: -0.01471\n",
      "Feature: 570, Score: 0.01286\n",
      "Feature: 571, Score: 0.01506\n",
      "Feature: 572, Score: -0.02353\n",
      "Feature: 573, Score: -0.00471\n",
      "Feature: 574, Score: -0.02814\n",
      "Feature: 575, Score: 0.00826\n",
      "Feature: 576, Score: 0.01129\n",
      "Feature: 577, Score: 0.01754\n",
      "Feature: 578, Score: -0.00063\n",
      "Feature: 579, Score: -0.00310\n",
      "Feature: 580, Score: -0.00648\n",
      "Feature: 581, Score: 0.00063\n",
      "Feature: 582, Score: 0.00566\n",
      "Feature: 583, Score: -0.00213\n",
      "Feature: 584, Score: -0.00091\n",
      "Feature: 585, Score: 0.00385\n",
      "Feature: 586, Score: 0.00902\n",
      "Feature: 587, Score: -0.00121\n",
      "Feature: 588, Score: -0.00683\n",
      "Feature: 589, Score: -0.00775\n",
      "Feature: 590, Score: -0.00519\n",
      "Feature: 591, Score: -0.00769\n",
      "Feature: 592, Score: -0.03623\n",
      "Feature: 593, Score: 0.01445\n",
      "Feature: 594, Score: -0.85406\n",
      "Feature: 595, Score: -0.00000\n",
      "Feature: 596, Score: 20.06650\n",
      "Feature: 597, Score: 3.15372\n",
      "Feature: 598, Score: -0.00000\n",
      "Feature: 599, Score: 0.52541\n",
      "Feature: 600, Score: 0.36253\n",
      "Feature: 601, Score: 1.61087\n",
      "Feature: 602, Score: 0.53952\n",
      "Feature: 603, Score: 1.94907\n",
      "Feature: 604, Score: 0.46739\n",
      "Feature: 605, Score: -2.04894\n",
      "Feature: 606, Score: -3.72132\n",
      "Feature: 607, Score: -0.00900\n",
      "Feature: 608, Score: -0.02628\n",
      "Feature: 609, Score: 0.00533\n",
      "Feature: 610, Score: 0.14305\n",
      "Feature: 611, Score: 1.40996\n",
      "Feature: 612, Score: 0.84173\n",
      "Feature: 613, Score: -0.89895\n",
      "Feature: 614, Score: -0.00346\n",
      "Feature: 615, Score: -0.16590\n",
      "Feature: 616, Score: -1.16434\n",
      "Feature: 617, Score: -0.15341\n",
      "Feature: 618, Score: 0.32745\n",
      "Feature: 619, Score: -0.20203\n",
      "Feature: 620, Score: -0.75703\n",
      "Feature: 621, Score: -0.52148\n",
      "Feature: 622, Score: 0.05872\n",
      "Feature: 623, Score: 0.01978\n",
      "Feature: 624, Score: 0.04536\n",
      "Feature: 625, Score: -0.89957\n",
      "Feature: 626, Score: 1.12023\n",
      "Feature: 627, Score: 0.59231\n",
      "Feature: 628, Score: 0.23280\n",
      "Feature: 629, Score: 0.07730\n",
      "Feature: 630, Score: -1.48610\n",
      "Feature: 631, Score: -1.31761\n",
      "Feature: 632, Score: 0.66264\n",
      "Feature: 633, Score: -0.28552\n",
      "Feature: 634, Score: 0.32364\n",
      "Feature: 635, Score: 0.17403\n",
      "Feature: 636, Score: -0.76914\n",
      "Feature: 637, Score: -0.97214\n",
      "Feature: 638, Score: -0.19513\n",
      "Feature: 639, Score: 0.04958\n",
      "Feature: 640, Score: -0.43286\n",
      "Feature: 641, Score: -0.35382\n",
      "Feature: 642, Score: 0.25503\n",
      "Feature: 643, Score: 0.82366\n",
      "Feature: 644, Score: -0.16559\n",
      "Feature: 645, Score: 0.30310\n",
      "Feature: 646, Score: 0.23536\n",
      "Feature: 647, Score: -0.09626\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "for i,j in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-217.45759246549807\n",
      "147.05998846251677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.09625861299751597"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())\n",
    "importance[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 50 components, MSE Model 1 = 0.0368\n",
      "MLR with 50 components, MAE Model 1 = 0.11\n",
      "MLR with 50 components, 1 - MAE Model 1 = 0.89\n",
      "-------------------------------------------\n",
      "MLR with 100 components, MSE Model 1 = 0.054\n",
      "MLR with 100 components, MAE Model 1 = 0.107\n",
      "MLR with 100 components, 1 - MAE Model 1 = 0.893\n",
      "-------------------------------------------\n",
      "MLR with 200 components, MSE Model 1 = 0.0445\n",
      "MLR with 200 components, MAE Model 1 = 0.105\n",
      "MLR with 200 components, 1 - MAE Model 1 = 0.895\n",
      "-------------------------------------------\n",
      "MLR with 300 components, MSE Model 1 = 0.026\n",
      "MLR with 300 components, MAE Model 1 = 0.103\n",
      "MLR with 300 components, 1 - MAE Model 1 = 0.897\n",
      "-------------------------------------------\n",
      "MLR with 400 components, MSE Model 1 = 0.0385\n",
      "MLR with 400 components, MAE Model 1 = 0.103\n",
      "MLR with 400 components, 1 - MAE Model 1 = 0.897\n",
      "-------------------------------------------\n",
      "MLR with 500 components, MSE Model 1 = 0.031\n",
      "MLR with 500 components, MAE Model 1 = 0.102\n",
      "MLR with 500 components, 1 - MAE Model 1 = 0.898\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_mlr_PCA = pca.fit_transform(X_1_train)\n",
    "    X_1_test_mlr_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    regressor.fit(X_1_train_mlr_PCA, y_1_train)\n",
    "    mlr_y_1_PCA = regressor.predict(X_1_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 1 = {:.3}\".format(mean_squared_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 1 = {:.3}\".format(mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 1 = {:.3}\".format(1-mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0157\n",
      "MAE 0.1\n",
      "1 - MAE 0.9\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_1_train, y_1_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_1 = rfr.predict(X_1_test)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_1_test, rfr_y_1)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_1_test, rfr_y_1)))\n",
    "print(\"1 - MAE {:.3}\".format((1-mean_absolute_error(y_1_test, rfr_y_1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 50 components, MSE Model 1 = 0.0178\n",
      "RFR with 50 components, MAE Model 1 = 0.107\n",
      "RFR with 50 components, 1 - MAE Model 1 = 0.893\n",
      "-------------------------------------------\n",
      "RFR with 100 components, MSE Model 1 = 0.018\n",
      "RFR with 100 components, MAE Model 1 = 0.108\n",
      "RFR with 100 components, 1 - MAE Model 1 = 0.892\n",
      "-------------------------------------------\n",
      "RFR with 200 components, MSE Model 1 = 0.0184\n",
      "RFR with 200 components, MAE Model 1 = 0.109\n",
      "RFR with 200 components, 1 - MAE Model 1 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 300 components, MSE Model 1 = 0.019\n",
      "RFR with 300 components, MAE Model 1 = 0.111\n",
      "RFR with 300 components, 1 - MAE Model 1 = 0.889\n",
      "-------------------------------------------\n",
      "RFR with 400 components, MSE Model 1 = 0.0183\n",
      "RFR with 400 components, MAE Model 1 = 0.109\n",
      "RFR with 400 components, 1 - MAE Model 1 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 500 components, MSE Model 1 = 0.0188\n",
      "RFR with 500 components, MAE Model 1 = 0.11\n",
      "RFR with 500 components, 1 - MAE Model 1 = 0.89\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_1_train_PCA = pca.fit_transform(X_1_train)\n",
    "    rfr_X_1_test_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    rfr.fit(rfr_X_1_train_PCA, y_1_train.ravel())\n",
    "    rfr_y_1_PCA = rfr.predict(rfr_X_1_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 1 = {:.3}\".format(mean_squared_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 1 = {:.3}\".format(mean_absolute_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 1 = {:.3}\".format(1-mean_absolute_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "X_1_train_scale = X_sc.fit_transform(X_1_train)\n",
    "X_1_test_scale = X_sc.transform(X_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0148\n",
      "MAE 0.0971\n",
      "1 - MAE 0.903\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_1_train_scale, y_1_train.ravel())\n",
    "svr_y_1 = svr.predict(X_1_test_scale)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_1_test, svr_y_1)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_1_test, svr_y_1)))\n",
    "print(\"1 - MAE {:.3}\".format(1-(mean_absolute_error(y_1_test, svr_y_1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 50 components, MSE Model 1 = 0.0158\n",
      "SVR with 50 components, MAE Model 1 = 0.1\n",
      "SVR with 50 components, 1 - MAE Model 1 = 0.9\n",
      "-------------------------------------------\n",
      "SVR with 100 components, MSE Model 1 = 0.0154\n",
      "SVR with 100 components, MAE Model 1 = 0.0986\n",
      "SVR with 100 components, 1 - MAE Model 1 = 0.901\n",
      "-------------------------------------------\n",
      "SVR with 200 components, MSE Model 1 = 0.0152\n",
      "SVR with 200 components, MAE Model 1 = 0.0984\n",
      "SVR with 200 components, 1 - MAE Model 1 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 300 components, MSE Model 1 = 0.015\n",
      "SVR with 300 components, MAE Model 1 = 0.0976\n",
      "SVR with 300 components, 1 - MAE Model 1 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 400 components, MSE Model 1 = 0.0149\n",
      "SVR with 400 components, MAE Model 1 = 0.0974\n",
      "SVR with 400 components, 1 - MAE Model 1 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 500 components, MSE Model 1 = 0.0148\n",
      "SVR with 500 components, MAE Model 1 = 0.0972\n",
      "SVR with 500 components, 1 - MAE Model 1 = 0.903\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for support vector regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_PCA = pca.fit_transform(X_1_train_scale)\n",
    "    X_1_test_PCA = pca.transform(X_1_test_scale)\n",
    "    \n",
    "    svr.fit(X_1_train_PCA, y_1_train.ravel())\n",
    "    svr_y_1_PCA = svr.predict(X_1_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 1 = {:.3}\".format(mean_squared_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 1 = {:.3}\".format(mean_absolute_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 1 = {:.3}\".format(1-mean_absolute_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Interview ~ facial + gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation (test) set for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 2, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_gen_int = merged_df[['videoname', 'gender', 'interview']]\n",
    "training_face_gen = pd.merge(df_train, df_gen_int, how = \"left\", on = [\"videoname\"])\n",
    "#training_face_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 2\n",
    "df_val_gen_int = merged_val_df[['videoname', 'gender', 'interview']]\n",
    "val_face_gen = pd.merge(df_val, df_val_gen_int, how = \"left\", on = [\"videoname\"])\n",
    "#val_face_gen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 109 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Splitting the (in)dependent variables\n",
    "X_2_train = training_face_gen.drop(['videoname','interview'], axis=1).values\n",
    "y_2_train = training_face_gen.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_2_test = val_face_gen.drop(['videoname', 'interview'], axis=1).values\n",
    "y_2_test = val_face_gen.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_2_train[:, -1] = le.fit_transform(X_2_train[:, -1])\n",
    "X_2_test[:, -1] = le.fit_transform(X_2_test[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0254\n",
      "MAE 0.101\n",
      "1 - MAE 0.899\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_2_train, y_2_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_2_pred = regressor.predict(X_2_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_2_test, y_2_pred)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_2_test, y_2_pred)))\n",
    "print(\"1 - MAE {:.3}\".format((1 - mean_absolute_error(y_2_test, y_2_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: 0.00502\n",
      "Feature: 1, Score: 5559424408.32846\n",
      "Feature: 2, Score: -0.20456\n",
      "Feature: 3, Score: 0.21849\n",
      "Feature: 4, Score: 28772316498.41384\n",
      "Feature: 5, Score: 0.19169\n",
      "Feature: 6, Score: -0.83089\n",
      "Feature: 7, Score: 0.05843\n",
      "Feature: 8, Score: 0.00491\n",
      "Feature: 9, Score: -0.95110\n",
      "Feature: 10, Score: -0.51650\n",
      "Feature: 11, Score: -0.22262\n",
      "Feature: 12, Score: 1.80268\n",
      "Feature: 13, Score: 0.00007\n",
      "Feature: 14, Score: 0.00030\n",
      "Feature: 15, Score: 0.00009\n",
      "Feature: 16, Score: 0.07850\n",
      "Feature: 17, Score: 0.01242\n",
      "Feature: 18, Score: -0.06506\n",
      "Feature: 19, Score: 0.00408\n",
      "Feature: 20, Score: -0.06408\n",
      "Feature: 21, Score: 0.00279\n",
      "Feature: 22, Score: 0.06504\n",
      "Feature: 23, Score: -0.00297\n",
      "Feature: 24, Score: 0.00490\n",
      "Feature: 25, Score: 0.25902\n",
      "Feature: 26, Score: 0.01983\n",
      "Feature: 27, Score: -0.00302\n",
      "Feature: 28, Score: 0.05528\n",
      "Feature: 29, Score: -0.13295\n",
      "Feature: 30, Score: 0.01258\n",
      "Feature: 31, Score: 0.10827\n",
      "Feature: 32, Score: 0.15362\n",
      "Feature: 33, Score: 0.08273\n",
      "Feature: 34, Score: 0.04970\n",
      "Feature: 35, Score: -0.07900\n",
      "Feature: 36, Score: -0.11192\n",
      "Feature: 37, Score: -0.03524\n",
      "Feature: 38, Score: -0.01160\n",
      "Feature: 39, Score: 0.04035\n",
      "Feature: 40, Score: 0.04348\n",
      "Feature: 41, Score: -0.00644\n",
      "Feature: 42, Score: -0.02654\n",
      "Feature: 43, Score: 0.00250\n",
      "Feature: 44, Score: 0.04253\n",
      "Feature: 45, Score: -0.07201\n",
      "Feature: 46, Score: -0.09197\n",
      "Feature: 47, Score: -0.07820\n",
      "Feature: 48, Score: 0.05756\n",
      "Feature: 49, Score: -0.01386\n",
      "Feature: 50, Score: -0.07678\n",
      "Feature: 51, Score: -0.18825\n",
      "Feature: 52, Score: 0.02086\n",
      "Feature: 53, Score: 0.00170\n",
      "Feature: 54, Score: 0.00628\n",
      "Feature: 55, Score: 379069781.58287\n",
      "Feature: 56, Score: -0.20083\n",
      "Feature: 57, Score: 0.03170\n",
      "Feature: 58, Score: 1637927573.84493\n",
      "Feature: 59, Score: 0.02520\n",
      "Feature: 60, Score: 0.10738\n",
      "Feature: 61, Score: 0.25921\n",
      "Feature: 62, Score: -0.02096\n",
      "Feature: 63, Score: 0.07429\n",
      "Feature: 64, Score: -0.22974\n",
      "Feature: 65, Score: -0.01941\n",
      "Feature: 66, Score: -0.13988\n",
      "Feature: 67, Score: -0.00002\n",
      "Feature: 68, Score: 0.00019\n",
      "Feature: 69, Score: 0.00021\n",
      "Feature: 70, Score: -0.01180\n",
      "Feature: 71, Score: -0.00649\n",
      "Feature: 72, Score: 0.00140\n",
      "Feature: 73, Score: -29329422.24293\n",
      "Feature: 74, Score: 161121403.43473\n",
      "Feature: 75, Score: -0.01669\n",
      "Feature: 76, Score: 206844815.39660\n",
      "Feature: 77, Score: -0.01040\n",
      "Feature: 78, Score: -0.01846\n",
      "Feature: 79, Score: 61698016.31894\n",
      "Feature: 80, Score: -0.02623\n",
      "Feature: 81, Score: 0.01806\n",
      "Feature: 82, Score: 0.00007\n",
      "Feature: 83, Score: 273336681.39054\n",
      "Feature: 84, Score: 293983136.84915\n",
      "Feature: 85, Score: -105537932.12381\n",
      "Feature: 86, Score: -156609830.10093\n",
      "Feature: 87, Score: -272870786.46064\n",
      "Feature: 88, Score: 180524932.31159\n",
      "Feature: 89, Score: -205017435.16191\n",
      "Feature: 90, Score: 50830630.23820\n",
      "Feature: 91, Score: 9762502.21779\n",
      "Feature: 92, Score: -0.06267\n",
      "Feature: 93, Score: 0.02860\n",
      "Feature: 94, Score: -0.05795\n",
      "Feature: 95, Score: 0.09204\n",
      "Feature: 96, Score: -158434177.59955\n",
      "Feature: 97, Score: -0.00032\n",
      "Feature: 98, Score: 0.11929\n",
      "Feature: 99, Score: 0.04950\n",
      "Feature: 100, Score: -9678391.57965\n",
      "Feature: 101, Score: 107772364.96821\n",
      "Feature: 102, Score: 36826277.53971\n",
      "Feature: 103, Score: 0.00435\n",
      "Feature: 104, Score: 61392298.47374\n",
      "Feature: 105, Score: -137176034.52491\n",
      "Feature: 106, Score: -167923215.97809\n",
      "Feature: 107, Score: -108563278.15529\n",
      "Feature: 108, Score: -0.00787\n",
      "Feature: 109, Score: 114829383.21775\n",
      "Feature: 110, Score: 0.27490\n",
      "Feature: 111, Score: 5.81549\n",
      "Feature: 112, Score: -88945365.42331\n",
      "Feature: 113, Score: -0.13975\n",
      "Feature: 114, Score: -0.13819\n",
      "Feature: 115, Score: -0.01158\n",
      "Feature: 116, Score: 0.05738\n",
      "Feature: 117, Score: -0.12103\n",
      "Feature: 118, Score: -0.16000\n",
      "Feature: 119, Score: 0.06997\n",
      "Feature: 120, Score: 0.24194\n",
      "Feature: 121, Score: -0.00006\n",
      "Feature: 122, Score: -0.00047\n",
      "Feature: 123, Score: -0.00018\n",
      "Feature: 124, Score: 0.04137\n",
      "Feature: 125, Score: -0.03285\n",
      "Feature: 126, Score: 0.02280\n",
      "Feature: 127, Score: -0.01090\n",
      "Feature: 128, Score: -0.00089\n",
      "Feature: 129, Score: 0.00238\n",
      "Feature: 130, Score: 0.00641\n",
      "Feature: 131, Score: 0.00447\n",
      "Feature: 132, Score: -0.00189\n",
      "Feature: 133, Score: 0.03219\n",
      "Feature: 134, Score: -0.01292\n",
      "Feature: 135, Score: 0.00448\n",
      "Feature: 136, Score: -0.01097\n",
      "Feature: 137, Score: -0.00456\n",
      "Feature: 138, Score: -0.00752\n",
      "Feature: 139, Score: -0.00084\n",
      "Feature: 140, Score: 0.01105\n",
      "Feature: 141, Score: 0.00060\n",
      "Feature: 142, Score: 0.00732\n",
      "Feature: 143, Score: -0.00232\n",
      "Feature: 144, Score: -0.17910\n",
      "Feature: 145, Score: -0.03985\n",
      "Feature: 146, Score: 0.02006\n",
      "Feature: 147, Score: -0.03546\n",
      "Feature: 148, Score: 0.00706\n",
      "Feature: 149, Score: -0.05353\n",
      "Feature: 150, Score: -0.00885\n",
      "Feature: 151, Score: 0.03796\n",
      "Feature: 152, Score: -0.08716\n",
      "Feature: 153, Score: -0.07638\n",
      "Feature: 154, Score: 0.12441\n",
      "Feature: 155, Score: -0.16198\n",
      "Feature: 156, Score: -0.03493\n",
      "Feature: 157, Score: -0.00125\n",
      "Feature: 158, Score: -28816817.68546\n",
      "Feature: 159, Score: -0.06781\n",
      "Feature: 160, Score: 0.08486\n",
      "Feature: 161, Score: 0.16718\n",
      "Feature: 162, Score: 0.01774\n",
      "Feature: 163, Score: -26805882.82144\n",
      "Feature: 164, Score: -0.57503\n",
      "Feature: 165, Score: -0.55056\n",
      "Feature: 166, Score: -34296270.29964\n",
      "Feature: 167, Score: 0.18985\n",
      "Feature: 168, Score: 0.57418\n",
      "Feature: 169, Score: 0.06257\n",
      "Feature: 170, Score: -0.22999\n",
      "Feature: 171, Score: 0.84105\n",
      "Feature: 172, Score: 0.91037\n",
      "Feature: 173, Score: -0.30434\n",
      "Feature: 174, Score: -1.16507\n",
      "Feature: 175, Score: -0.00003\n",
      "Feature: 176, Score: -0.00014\n",
      "Feature: 177, Score: -0.00000\n",
      "Feature: 178, Score: -0.17937\n",
      "Feature: 179, Score: 0.37168\n",
      "Feature: 180, Score: 0.07146\n",
      "Feature: 181, Score: 0.00088\n",
      "Feature: 182, Score: 0.03894\n",
      "Feature: 183, Score: -0.01824\n",
      "Feature: 184, Score: -0.04537\n",
      "Feature: 185, Score: -0.02833\n",
      "Feature: 186, Score: -0.00041\n",
      "Feature: 187, Score: -0.29280\n",
      "Feature: 188, Score: -0.00854\n",
      "Feature: 189, Score: 0.07093\n",
      "Feature: 190, Score: -0.00564\n",
      "Feature: 191, Score: 0.04811\n",
      "Feature: 192, Score: -0.01860\n",
      "Feature: 193, Score: -0.06146\n",
      "Feature: 194, Score: -0.08323\n",
      "Feature: 195, Score: -0.10251\n",
      "Feature: 196, Score: -0.10517\n",
      "Feature: 197, Score: 0.03187\n",
      "Feature: 198, Score: 0.16808\n",
      "Feature: 199, Score: 0.08809\n",
      "Feature: 200, Score: 0.04628\n",
      "Feature: 201, Score: 0.01133\n",
      "Feature: 202, Score: 0.01877\n",
      "Feature: 203, Score: 0.00067\n",
      "Feature: 204, Score: 0.07591\n",
      "Feature: 205, Score: 0.01758\n",
      "Feature: 206, Score: -0.06621\n",
      "Feature: 207, Score: 0.05108\n",
      "Feature: 208, Score: 0.06293\n",
      "Feature: 209, Score: 0.02840\n",
      "Feature: 210, Score: -0.05527\n",
      "Feature: 211, Score: 0.08596\n",
      "Feature: 212, Score: -0.05256\n",
      "Feature: 213, Score: 0.13546\n",
      "Feature: 214, Score: -0.04164\n",
      "Feature: 215, Score: -0.01301\n",
      "Feature: 216, Score: 0.21824\n",
      "Feature: 217, Score: 53676862.51390\n",
      "Feature: 218, Score: -5.24827\n",
      "Feature: 219, Score: 7.29610\n",
      "Feature: 220, Score: 78740937.41230\n",
      "Feature: 221, Score: 37.42106\n",
      "Feature: 222, Score: 18.39781\n",
      "Feature: 223, Score: 26.53138\n",
      "Feature: 224, Score: 29.38366\n",
      "Feature: 225, Score: 53.75384\n",
      "Feature: 226, Score: -22.00956\n",
      "Feature: 227, Score: -62.94820\n",
      "Feature: 228, Score: -82.51880\n",
      "Feature: 229, Score: -0.01039\n",
      "Feature: 230, Score: 0.01718\n",
      "Feature: 231, Score: 0.04026\n",
      "Feature: 232, Score: 8.51506\n",
      "Feature: 233, Score: 0.38226\n",
      "Feature: 234, Score: 5.03972\n",
      "Feature: 235, Score: 0.53826\n",
      "Feature: 236, Score: -0.53177\n",
      "Feature: 237, Score: 0.37036\n",
      "Feature: 238, Score: 1.67042\n",
      "Feature: 239, Score: 2.07254\n",
      "Feature: 240, Score: -0.07484\n",
      "Feature: 241, Score: -2.41194\n",
      "Feature: 242, Score: -1.47539\n",
      "Feature: 243, Score: -3.60406\n",
      "Feature: 244, Score: 1.37584\n",
      "Feature: 245, Score: 0.24871\n",
      "Feature: 246, Score: -1.81026\n",
      "Feature: 247, Score: 0.41049\n",
      "Feature: 248, Score: 1.48489\n",
      "Feature: 249, Score: -0.91312\n",
      "Feature: 250, Score: -0.14561\n",
      "Feature: 251, Score: 1.56605\n",
      "Feature: 252, Score: 0.03297\n",
      "Feature: 253, Score: 0.52299\n",
      "Feature: 254, Score: 1.60063\n",
      "Feature: 255, Score: -0.76247\n",
      "Feature: 256, Score: -2.66033\n",
      "Feature: 257, Score: 0.78092\n",
      "Feature: 258, Score: 3.24798\n",
      "Feature: 259, Score: 1.26320\n",
      "Feature: 260, Score: 2.87753\n",
      "Feature: 261, Score: -1.75904\n",
      "Feature: 262, Score: -0.26394\n",
      "Feature: 263, Score: 0.68143\n",
      "Feature: 264, Score: -2.71863\n",
      "Feature: 265, Score: -0.69896\n",
      "Feature: 266, Score: 0.06108\n",
      "Feature: 267, Score: -0.59019\n",
      "Feature: 268, Score: 7.37272\n",
      "Feature: 269, Score: -1.67117\n",
      "Feature: 270, Score: -32689744.10118\n",
      "Feature: 271, Score: -44076432.50052\n",
      "Feature: 272, Score: -4.50599\n",
      "Feature: 273, Score: -0.09703\n",
      "Feature: 274, Score: 47099891.41097\n",
      "Feature: 275, Score: 0.00757\n",
      "Feature: 276, Score: 0.05535\n",
      "Feature: 277, Score: -0.18865\n",
      "Feature: 278, Score: -0.05468\n",
      "Feature: 279, Score: -0.01589\n",
      "Feature: 280, Score: -0.07501\n",
      "Feature: 281, Score: 0.02432\n",
      "Feature: 282, Score: 0.01757\n",
      "Feature: 283, Score: 0.00009\n",
      "Feature: 284, Score: 0.00088\n",
      "Feature: 285, Score: -0.00014\n",
      "Feature: 286, Score: 0.06747\n",
      "Feature: 287, Score: 0.19257\n",
      "Feature: 288, Score: -0.17361\n",
      "Feature: 289, Score: 0.00773\n",
      "Feature: 290, Score: 0.00437\n",
      "Feature: 291, Score: -0.00468\n",
      "Feature: 292, Score: -0.02373\n",
      "Feature: 293, Score: 0.04513\n",
      "Feature: 294, Score: -0.01808\n",
      "Feature: 295, Score: 0.03394\n",
      "Feature: 296, Score: 0.03147\n",
      "Feature: 297, Score: -0.02008\n",
      "Feature: 298, Score: -0.00480\n",
      "Feature: 299, Score: -0.00098\n",
      "Feature: 300, Score: -0.00043\n",
      "Feature: 301, Score: 0.01180\n",
      "Feature: 302, Score: -0.00333\n",
      "Feature: 303, Score: -0.01303\n",
      "Feature: 304, Score: 0.00218\n",
      "Feature: 305, Score: -0.02280\n",
      "Feature: 306, Score: 0.00353\n",
      "Feature: 307, Score: -0.00125\n",
      "Feature: 308, Score: 0.00164\n",
      "Feature: 309, Score: -0.06850\n",
      "Feature: 310, Score: -0.00110\n",
      "Feature: 311, Score: -0.02120\n",
      "Feature: 312, Score: -0.01636\n",
      "Feature: 313, Score: 0.01637\n",
      "Feature: 314, Score: -0.04102\n",
      "Feature: 315, Score: -0.02446\n",
      "Feature: 316, Score: 0.00900\n",
      "Feature: 317, Score: -0.10580\n",
      "Feature: 318, Score: -0.02592\n",
      "Feature: 319, Score: -0.00316\n",
      "Feature: 320, Score: -28816817.76622\n",
      "Feature: 321, Score: -0.10203\n",
      "Feature: 322, Score: 0.05841\n",
      "Feature: 323, Score: 0.04385\n",
      "Feature: 324, Score: -0.17645\n",
      "Feature: 325, Score: -38265584.00234\n",
      "Feature: 326, Score: 4.54280\n",
      "Feature: 327, Score: 0.05741\n",
      "Feature: 328, Score: -16943597.30400\n",
      "Feature: 329, Score: -0.06180\n",
      "Feature: 330, Score: 0.04229\n",
      "Feature: 331, Score: 0.13289\n",
      "Feature: 332, Score: 0.07750\n",
      "Feature: 333, Score: 0.00682\n",
      "Feature: 334, Score: 0.07657\n",
      "Feature: 335, Score: 0.02526\n",
      "Feature: 336, Score: -0.12355\n",
      "Feature: 337, Score: -0.00004\n",
      "Feature: 338, Score: -0.00076\n",
      "Feature: 339, Score: -0.00029\n",
      "Feature: 340, Score: -0.09377\n",
      "Feature: 341, Score: -0.06440\n",
      "Feature: 342, Score: 0.20812\n",
      "Feature: 343, Score: -0.00941\n",
      "Feature: 344, Score: 0.01902\n",
      "Feature: 345, Score: -0.00165\n",
      "Feature: 346, Score: -0.03053\n",
      "Feature: 347, Score: -0.01446\n",
      "Feature: 348, Score: 0.02950\n",
      "Feature: 349, Score: 0.01877\n",
      "Feature: 350, Score: -0.03058\n",
      "Feature: 351, Score: -0.06086\n",
      "Feature: 352, Score: 0.01301\n",
      "Feature: 353, Score: -0.00396\n",
      "Feature: 354, Score: 0.00731\n",
      "Feature: 355, Score: 0.01178\n",
      "Feature: 356, Score: -0.00742\n",
      "Feature: 357, Score: -0.00265\n",
      "Feature: 358, Score: -0.00710\n",
      "Feature: 359, Score: 0.00570\n",
      "Feature: 360, Score: 0.07374\n",
      "Feature: 361, Score: -0.00517\n",
      "Feature: 362, Score: -0.00782\n",
      "Feature: 363, Score: -0.03564\n",
      "Feature: 364, Score: -0.00095\n",
      "Feature: 365, Score: 0.03426\n",
      "Feature: 366, Score: -0.01720\n",
      "Feature: 367, Score: -0.00624\n",
      "Feature: 368, Score: 0.05493\n",
      "Feature: 369, Score: 0.03710\n",
      "Feature: 370, Score: -0.12345\n",
      "Feature: 371, Score: 0.01305\n",
      "Feature: 372, Score: 0.00992\n",
      "Feature: 373, Score: 0.01605\n",
      "Feature: 374, Score: 0.02153\n",
      "Feature: 375, Score: -0.15164\n",
      "Feature: 376, Score: -0.00384\n",
      "Feature: 377, Score: -0.12614\n",
      "Feature: 378, Score: 1.50151\n",
      "Feature: 379, Score: 18777671.05007\n",
      "Feature: 380, Score: -38.78260\n",
      "Feature: 381, Score: -4.60165\n",
      "Feature: 382, Score: -9979438.60668\n",
      "Feature: 383, Score: 0.56995\n",
      "Feature: 384, Score: 0.59337\n",
      "Feature: 385, Score: -4.74967\n",
      "Feature: 386, Score: -1.47900\n",
      "Feature: 387, Score: -4.42358\n",
      "Feature: 388, Score: 1.25064\n",
      "Feature: 389, Score: 0.60496\n",
      "Feature: 390, Score: 5.88857\n",
      "Feature: 391, Score: 0.01399\n",
      "Feature: 392, Score: 0.05287\n",
      "Feature: 393, Score: -0.00389\n",
      "Feature: 394, Score: 0.00140\n",
      "Feature: 395, Score: 0.46086\n",
      "Feature: 396, Score: -1.87305\n",
      "Feature: 397, Score: 0.80666\n",
      "Feature: 398, Score: -0.00067\n",
      "Feature: 399, Score: -0.04578\n",
      "Feature: 400, Score: 0.67983\n",
      "Feature: 401, Score: 0.33337\n",
      "Feature: 402, Score: -0.33852\n",
      "Feature: 403, Score: -0.01561\n",
      "Feature: 404, Score: 0.39446\n",
      "Feature: 405, Score: 0.60553\n",
      "Feature: 406, Score: -0.18158\n",
      "Feature: 407, Score: 0.33013\n",
      "Feature: 408, Score: 0.08386\n",
      "Feature: 409, Score: 0.67316\n",
      "Feature: 410, Score: -0.71369\n",
      "Feature: 411, Score: -0.37019\n",
      "Feature: 412, Score: 0.34109\n",
      "Feature: 413, Score: 0.13081\n",
      "Feature: 414, Score: 2.06020\n",
      "Feature: 415, Score: 1.61379\n",
      "Feature: 416, Score: -1.16599\n",
      "Feature: 417, Score: 0.40108\n",
      "Feature: 418, Score: -0.43852\n",
      "Feature: 419, Score: -0.22636\n",
      "Feature: 420, Score: 0.95892\n",
      "Feature: 421, Score: 1.48938\n",
      "Feature: 422, Score: 0.31789\n",
      "Feature: 423, Score: -0.05987\n",
      "Feature: 424, Score: 0.58908\n",
      "Feature: 425, Score: 0.72143\n",
      "Feature: 426, Score: -0.32341\n",
      "Feature: 427, Score: -1.32440\n",
      "Feature: 428, Score: 0.46389\n",
      "Feature: 429, Score: -0.44793\n",
      "Feature: 430, Score: -0.18173\n",
      "Feature: 431, Score: 0.13722\n",
      "Feature: 432, Score: 4.96697\n",
      "Feature: 433, Score: 4748636.24272\n",
      "Feature: 434, Score: -44.88839\n",
      "Feature: 435, Score: -77.17146\n",
      "Feature: 436, Score: 2791852.59984\n",
      "Feature: 437, Score: 119.08718\n",
      "Feature: 438, Score: -21.27154\n",
      "Feature: 439, Score: 145.41377\n",
      "Feature: 440, Score: 106.82484\n",
      "Feature: 441, Score: 50.16874\n",
      "Feature: 442, Score: -76.52488\n",
      "Feature: 443, Score: -210.72782\n",
      "Feature: 444, Score: -12.48929\n",
      "Feature: 445, Score: 0.02077\n",
      "Feature: 446, Score: -0.23213\n",
      "Feature: 447, Score: -0.20332\n",
      "Feature: 448, Score: 1.20358\n",
      "Feature: 449, Score: -6.49893\n",
      "Feature: 450, Score: 35.69717\n",
      "Feature: 451, Score: -1.55341\n",
      "Feature: 452, Score: 1.05245\n",
      "Feature: 453, Score: -6.07700\n",
      "Feature: 454, Score: 7.43753\n",
      "Feature: 455, Score: 7.24418\n",
      "Feature: 456, Score: 0.94678\n",
      "Feature: 457, Score: 8.65742\n",
      "Feature: 458, Score: -0.97011\n",
      "Feature: 459, Score: -6.70774\n",
      "Feature: 460, Score: -0.32126\n",
      "Feature: 461, Score: 0.83280\n",
      "Feature: 462, Score: -0.39793\n",
      "Feature: 463, Score: 0.86967\n",
      "Feature: 464, Score: -4.36860\n",
      "Feature: 465, Score: -0.54119\n",
      "Feature: 466, Score: 0.84525\n",
      "Feature: 467, Score: 0.31343\n",
      "Feature: 468, Score: 1.92639\n",
      "Feature: 469, Score: 1.45787\n",
      "Feature: 470, Score: 2.99041\n",
      "Feature: 471, Score: -0.14202\n",
      "Feature: 472, Score: -3.57775\n",
      "Feature: 473, Score: -0.10449\n",
      "Feature: 474, Score: -3.55980\n",
      "Feature: 475, Score: -2.41923\n",
      "Feature: 476, Score: 3.13800\n",
      "Feature: 477, Score: 2.21059\n",
      "Feature: 478, Score: 1.40872\n",
      "Feature: 479, Score: -0.68961\n",
      "Feature: 480, Score: 0.56799\n",
      "Feature: 481, Score: 0.31443\n",
      "Feature: 482, Score: -0.06290\n",
      "Feature: 483, Score: 2.16519\n",
      "Feature: 484, Score: 3.51526\n",
      "Feature: 485, Score: -3.91513\n",
      "Feature: 486, Score: -0.12665\n",
      "Feature: 487, Score: -439847.32238\n",
      "Feature: 488, Score: 3.20617\n",
      "Feature: 489, Score: 0.09423\n",
      "Feature: 490, Score: -2354726.55071\n",
      "Feature: 491, Score: -0.03426\n",
      "Feature: 492, Score: 0.03397\n",
      "Feature: 493, Score: 0.06587\n",
      "Feature: 494, Score: -0.07735\n",
      "Feature: 495, Score: 0.00779\n",
      "Feature: 496, Score: 0.04230\n",
      "Feature: 497, Score: 0.10612\n",
      "Feature: 498, Score: -0.12819\n",
      "Feature: 499, Score: -0.00010\n",
      "Feature: 500, Score: -0.00050\n",
      "Feature: 501, Score: 0.00020\n",
      "Feature: 502, Score: -0.01170\n",
      "Feature: 503, Score: 0.05449\n",
      "Feature: 504, Score: 0.11503\n",
      "Feature: 505, Score: -0.04830\n",
      "Feature: 506, Score: 0.01555\n",
      "Feature: 507, Score: 0.00446\n",
      "Feature: 508, Score: 0.00369\n",
      "Feature: 509, Score: -0.04606\n",
      "Feature: 510, Score: 0.02866\n",
      "Feature: 511, Score: 0.02763\n",
      "Feature: 512, Score: -0.02346\n",
      "Feature: 513, Score: -0.00138\n",
      "Feature: 514, Score: -0.00689\n",
      "Feature: 515, Score: 0.00236\n",
      "Feature: 516, Score: -0.00099\n",
      "Feature: 517, Score: -0.01231\n",
      "Feature: 518, Score: 0.01550\n",
      "Feature: 519, Score: 0.01189\n",
      "Feature: 520, Score: 0.01728\n",
      "Feature: 521, Score: 0.00615\n",
      "Feature: 522, Score: -0.02275\n",
      "Feature: 523, Score: -0.02353\n",
      "Feature: 524, Score: 0.00732\n",
      "Feature: 525, Score: -0.00582\n",
      "Feature: 526, Score: -0.00572\n",
      "Feature: 527, Score: -0.00451\n",
      "Feature: 528, Score: -0.01342\n",
      "Feature: 529, Score: -0.01303\n",
      "Feature: 530, Score: 0.00169\n",
      "Feature: 531, Score: -0.00250\n",
      "Feature: 532, Score: -0.00271\n",
      "Feature: 533, Score: 0.00087\n",
      "Feature: 534, Score: 0.00439\n",
      "Feature: 535, Score: 0.00829\n",
      "Feature: 536, Score: -0.00117\n",
      "Feature: 537, Score: 0.00049\n",
      "Feature: 538, Score: -0.00417\n",
      "Feature: 539, Score: 0.00201\n",
      "Feature: 540, Score: 0.03019\n",
      "Feature: 541, Score: 212125.94140\n",
      "Feature: 542, Score: -0.66343\n",
      "Feature: 543, Score: -0.04403\n",
      "Feature: 544, Score: -80054.83645\n",
      "Feature: 545, Score: -0.00245\n",
      "Feature: 546, Score: -0.02766\n",
      "Feature: 547, Score: -0.08540\n",
      "Feature: 548, Score: -0.03768\n",
      "Feature: 549, Score: 0.07874\n",
      "Feature: 550, Score: -0.04503\n",
      "Feature: 551, Score: 0.05373\n",
      "Feature: 552, Score: -0.02447\n",
      "Feature: 553, Score: 0.00005\n",
      "Feature: 554, Score: 0.00011\n",
      "Feature: 555, Score: 0.00031\n",
      "Feature: 556, Score: 0.05202\n",
      "Feature: 557, Score: 0.05904\n",
      "Feature: 558, Score: -0.08330\n",
      "Feature: 559, Score: 0.00460\n",
      "Feature: 560, Score: -0.01839\n",
      "Feature: 561, Score: 0.02261\n",
      "Feature: 562, Score: 0.02730\n",
      "Feature: 563, Score: -0.01415\n",
      "Feature: 564, Score: -0.01100\n",
      "Feature: 565, Score: 0.03037\n",
      "Feature: 566, Score: 0.02631\n",
      "Feature: 567, Score: 0.02557\n",
      "Feature: 568, Score: -0.00107\n",
      "Feature: 569, Score: -0.01452\n",
      "Feature: 570, Score: 0.01251\n",
      "Feature: 571, Score: 0.01668\n",
      "Feature: 572, Score: -0.02388\n",
      "Feature: 573, Score: -0.00459\n",
      "Feature: 574, Score: -0.02822\n",
      "Feature: 575, Score: 0.00884\n",
      "Feature: 576, Score: 0.01135\n",
      "Feature: 577, Score: 0.01765\n",
      "Feature: 578, Score: -0.00134\n",
      "Feature: 579, Score: -0.00301\n",
      "Feature: 580, Score: -0.00640\n",
      "Feature: 581, Score: 0.00067\n",
      "Feature: 582, Score: 0.00481\n",
      "Feature: 583, Score: -0.00228\n",
      "Feature: 584, Score: -0.00061\n",
      "Feature: 585, Score: 0.00362\n",
      "Feature: 586, Score: 0.00933\n",
      "Feature: 587, Score: -0.00096\n",
      "Feature: 588, Score: -0.00725\n",
      "Feature: 589, Score: -0.00771\n",
      "Feature: 590, Score: -0.00557\n",
      "Feature: 591, Score: -0.00799\n",
      "Feature: 592, Score: -0.03445\n",
      "Feature: 593, Score: 0.01380\n",
      "Feature: 594, Score: -0.90214\n",
      "Feature: 595, Score: 1066.20178\n",
      "Feature: 596, Score: 21.40050\n",
      "Feature: 597, Score: 3.15292\n",
      "Feature: 598, Score: 3.88078\n",
      "Feature: 599, Score: 0.51458\n",
      "Feature: 600, Score: 0.41183\n",
      "Feature: 601, Score: 1.58980\n",
      "Feature: 602, Score: 0.54790\n",
      "Feature: 603, Score: 1.98884\n",
      "Feature: 604, Score: 0.46687\n",
      "Feature: 605, Score: -2.11366\n",
      "Feature: 606, Score: -3.85223\n",
      "Feature: 607, Score: -0.00898\n",
      "Feature: 608, Score: -0.02679\n",
      "Feature: 609, Score: 0.00533\n",
      "Feature: 610, Score: 0.21046\n",
      "Feature: 611, Score: 1.50476\n",
      "Feature: 612, Score: 0.80381\n",
      "Feature: 613, Score: -0.89311\n",
      "Feature: 614, Score: -0.02319\n",
      "Feature: 615, Score: -0.16614\n",
      "Feature: 616, Score: -1.19156\n",
      "Feature: 617, Score: -0.15859\n",
      "Feature: 618, Score: 0.31746\n",
      "Feature: 619, Score: -0.19913\n",
      "Feature: 620, Score: -0.74775\n",
      "Feature: 621, Score: -0.54700\n",
      "Feature: 622, Score: 0.04202\n",
      "Feature: 623, Score: 0.01631\n",
      "Feature: 624, Score: 0.04688\n",
      "Feature: 625, Score: -0.90021\n",
      "Feature: 626, Score: 1.13325\n",
      "Feature: 627, Score: 0.58834\n",
      "Feature: 628, Score: 0.23361\n",
      "Feature: 629, Score: 0.08358\n",
      "Feature: 630, Score: -1.50894\n",
      "Feature: 631, Score: -1.30714\n",
      "Feature: 632, Score: 0.69987\n",
      "Feature: 633, Score: -0.29015\n",
      "Feature: 634, Score: 0.29548\n",
      "Feature: 635, Score: 0.17131\n",
      "Feature: 636, Score: -0.73763\n",
      "Feature: 637, Score: -0.96286\n",
      "Feature: 638, Score: -0.18418\n",
      "Feature: 639, Score: 0.05463\n",
      "Feature: 640, Score: -0.45007\n",
      "Feature: 641, Score: -0.38366\n",
      "Feature: 642, Score: 0.29443\n",
      "Feature: 643, Score: 0.80626\n",
      "Feature: 644, Score: -0.12575\n",
      "Feature: 645, Score: 0.31736\n",
      "Feature: 646, Score: 0.20871\n",
      "Feature: 647, Score: -0.09140\n",
      "Feature: 648, Score: -0.00750\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "for i,j in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-272870786.46064144\n",
      "28772316498.413837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.007503639906644821"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())\n",
    "importance[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 50 components, MSE Model 2 = 0.038\n",
      "MLR with 50 components, MAE Model 2 = 0.11\n",
      "MLR with 50 components, 1 - MAE Model 2 = 0.89\n",
      "-------------------------------------------\n",
      "MLR with 100 components, MSE Model 2 = 0.06\n",
      "MLR with 100 components, MAE Model 2 = 0.108\n",
      "MLR with 100 components, 1 - MAE Model 2 = 0.892\n",
      "-------------------------------------------\n",
      "MLR with 200 components, MSE Model 2 = 0.0404\n",
      "MLR with 200 components, MAE Model 2 = 0.105\n",
      "MLR with 200 components, 1 - MAE Model 2 = 0.895\n",
      "-------------------------------------------\n",
      "MLR with 300 components, MSE Model 2 = 0.024\n",
      "MLR with 300 components, MAE Model 2 = 0.102\n",
      "MLR with 300 components, 1 - MAE Model 2 = 0.898\n",
      "-------------------------------------------\n",
      "MLR with 400 components, MSE Model 2 = 0.0398\n",
      "MLR with 400 components, MAE Model 2 = 0.103\n",
      "MLR with 400 components, 1 - MAE Model 2 = 0.897\n",
      "-------------------------------------------\n",
      "MLR with 500 components, MSE Model 2 = 0.0303\n",
      "MLR with 500 components, MAE Model 2 = 0.102\n",
      "MLR with 500 components, 1 - MAE Model 2 = 0.898\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_mlr_PCA = pca.fit_transform(X_2_train)\n",
    "    X_2_test_mlr_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    regressor.fit(X_2_train_mlr_PCA, y_2_train)\n",
    "    mlr_y_2_PCA = regressor.predict(X_2_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 2 = {:.3}\".format(mean_squared_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 2 = {:.3}\".format(mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 2 = {:.3}\".format(1-mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.016\n",
      "MAE 0.101\n",
      "1 - MAE 0.899\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_2_train, y_2_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_2 = rfr.predict(X_2_test)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_2_test, rfr_y_2)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_2_test, rfr_y_2)))\n",
    "print(\"1 - MAE {:.3}\".format((1-mean_absolute_error(y_2_test, rfr_y_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 50 components, MSE Model 2 = 0.0179\n",
      "RFR with 50 components, MAE Model 2 = 0.107\n",
      "RFR with 50 components, 1 - MAE Model 2 = 0.893\n",
      "-------------------------------------------\n",
      "RFR with 100 components, MSE Model 2 = 0.0181\n",
      "RFR with 100 components, MAE Model 2 = 0.108\n",
      "RFR with 100 components, 1 - MAE Model 2 = 0.892\n",
      "-------------------------------------------\n",
      "RFR with 200 components, MSE Model 2 = 0.0186\n",
      "RFR with 200 components, MAE Model 2 = 0.109\n",
      "RFR with 200 components, 1 - MAE Model 2 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 300 components, MSE Model 2 = 0.0185\n",
      "RFR with 300 components, MAE Model 2 = 0.11\n",
      "RFR with 300 components, 1 - MAE Model 2 = 0.89\n",
      "-------------------------------------------\n",
      "RFR with 400 components, MSE Model 2 = 0.0185\n",
      "RFR with 400 components, MAE Model 2 = 0.109\n",
      "RFR with 400 components, 1 - MAE Model 2 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 500 components, MSE Model 2 = 0.019\n",
      "RFR with 500 components, MAE Model 2 = 0.111\n",
      "RFR with 500 components, 1 - MAE Model 2 = 0.889\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_2_train_PCA = pca.fit_transform(X_2_train)\n",
    "    rfr_X_2_test_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    rfr.fit(rfr_X_2_train_PCA, y_2_train.ravel())\n",
    "    rfr_y_2_PCA = rfr.predict(rfr_X_2_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 2 = {:.3}\".format(mean_squared_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 2 = {:.3}\".format(mean_absolute_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 2 = {:.3}\".format(1-mean_absolute_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the gender dummies\n",
    "X_2_train_scale = X_2_train.copy()\n",
    "X_2_test_scale = X_2_test.copy()\n",
    "\n",
    "X_2_train_scale[:, :-1] = X_sc.fit_transform(X_2_train[:, :-1])\n",
    "X_2_test_scale[:, :-1] = X_sc.transform(X_2_test[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0148\n",
      "MAE 0.0971\n",
      "1 - MAE 0.903\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_2_train_scale, y_2_train.ravel())\n",
    "svr_y_2 = svr.predict(X_2_test_scale)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_2_test, svr_y_2)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_2_test, svr_y_2)))\n",
    "print(\"1 - MAE {:.3}\".format(1-(mean_absolute_error(y_2_test, svr_y_2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 50 components, MSE Model 2 = 0.0158\n",
      "SVR with 50 components, MAE Model 2 = 0.1\n",
      "SVR with 50 components, 1 - MAE Model 2 = 0.9\n",
      "-------------------------------------------\n",
      "SVR with 100 components, MSE Model 2 = 0.0154\n",
      "SVR with 100 components, MAE Model 2 = 0.0985\n",
      "SVR with 100 components, 1 - MAE Model 2 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 200 components, MSE Model 2 = 0.0152\n",
      "SVR with 200 components, MAE Model 2 = 0.0981\n",
      "SVR with 200 components, 1 - MAE Model 2 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 300 components, MSE Model 2 = 0.015\n",
      "SVR with 300 components, MAE Model 2 = 0.0974\n",
      "SVR with 300 components, 1 - MAE Model 2 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 400 components, MSE Model 2 = 0.0149\n",
      "SVR with 400 components, MAE Model 2 = 0.0973\n",
      "SVR with 400 components, 1 - MAE Model 2 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 500 components, MSE Model 2 = 0.0148\n",
      "SVR with 500 components, MAE Model 2 = 0.0971\n",
      "SVR with 500 components, 1 - MAE Model 2 = 0.903\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for support vector regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_PCA = pca.fit_transform(X_2_train_scale)\n",
    "    X_2_test_PCA = pca.transform(X_2_test_scale)\n",
    "    \n",
    "    svr.fit(X_2_train_PCA, y_2_train.ravel())\n",
    "    svr_y_2_PCA = svr.predict(X_2_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 2 = {:.3}\".format(mean_squared_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 2 = {:.3}\".format(mean_absolute_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 2 = {:.3}\".format(1-mean_absolute_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Interview ~ facial + ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation (test) set for Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 3, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_eth_int = merged_df[['videoname', 'ethnicity', 'interview']]\n",
    "training_face_eth = pd.merge(df_train, df_eth_int, how = \"left\", on = [\"videoname\"])\n",
    "#training_face_eth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 3\n",
    "df_val_eth_int = merged_val_df[['videoname', 'ethnicity', 'interview']]\n",
    "val_face_eth = pd.merge(df_val, df_val_eth_int, how = \"left\", on = [\"videoname\"])\n",
    "#val_face_eth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Splitting the (in)dependent variables\n",
    "X_3_train = training_face_eth.drop(['videoname', 'interview'], axis=1).values\n",
    "y_3_train = training_face_eth.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_3_test = val_face_eth.drop(['videoname', 'interview'], axis=1).values\n",
    "y_3_test = val_face_eth.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "X_3_train = np.array(ct.fit_transform(X_3_train))\n",
    "X_3_test = np.array(ct.fit_transform(X_3_test))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_3_train_dummy = X_3_train.copy()\n",
    "X_3_train_dummy = X_3_train_dummy[:,1:]\n",
    "\n",
    "X_3_test_dummy = X_3_test.copy()\n",
    "X_3_test_dummy = X_3_test_dummy[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0307\n",
      "MAE 0.102\n",
      "1 - MAE 0.898\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_3_train_dummy, y_3_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_3_pred = regressor.predict(X_3_test_dummy)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_3_test, y_3_pred)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_3_test, y_3_pred)))\n",
    "print(\"1 - MAE {:.3}\".format((1-mean_absolute_error(y_3_test, y_3_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -0.02643\n",
      "Feature: 1, Score: -0.05497\n",
      "Feature: 2, Score: 0.00709\n",
      "Feature: 3, Score: -507945340.10695\n",
      "Feature: 4, Score: -0.26112\n",
      "Feature: 5, Score: -0.00312\n",
      "Feature: 6, Score: -430139912.47887\n",
      "Feature: 7, Score: 0.19689\n",
      "Feature: 8, Score: -0.79318\n",
      "Feature: 9, Score: 0.16036\n",
      "Feature: 10, Score: 0.03414\n",
      "Feature: 11, Score: -0.81862\n",
      "Feature: 12, Score: -0.51509\n",
      "Feature: 13, Score: -0.25963\n",
      "Feature: 14, Score: 1.61808\n",
      "Feature: 15, Score: 0.00003\n",
      "Feature: 16, Score: 0.00018\n",
      "Feature: 17, Score: 0.00008\n",
      "Feature: 18, Score: 0.06304\n",
      "Feature: 19, Score: 0.01467\n",
      "Feature: 20, Score: -0.05810\n",
      "Feature: 21, Score: 0.00065\n",
      "Feature: 22, Score: -0.06201\n",
      "Feature: 23, Score: 0.00200\n",
      "Feature: 24, Score: 0.04411\n",
      "Feature: 25, Score: -0.00521\n",
      "Feature: 26, Score: 0.00457\n",
      "Feature: 27, Score: 0.26996\n",
      "Feature: 28, Score: 0.01956\n",
      "Feature: 29, Score: -0.00572\n",
      "Feature: 30, Score: 0.05705\n",
      "Feature: 31, Score: -0.13649\n",
      "Feature: 32, Score: 0.01313\n",
      "Feature: 33, Score: 0.14380\n",
      "Feature: 34, Score: 0.17287\n",
      "Feature: 35, Score: 0.06797\n",
      "Feature: 36, Score: 0.05247\n",
      "Feature: 37, Score: -0.08021\n",
      "Feature: 38, Score: -0.11172\n",
      "Feature: 39, Score: -0.02969\n",
      "Feature: 40, Score: -0.01295\n",
      "Feature: 41, Score: 0.03861\n",
      "Feature: 42, Score: 0.04141\n",
      "Feature: 43, Score: -0.00646\n",
      "Feature: 44, Score: -0.03464\n",
      "Feature: 45, Score: 0.00350\n",
      "Feature: 46, Score: 0.04426\n",
      "Feature: 47, Score: -0.06785\n",
      "Feature: 48, Score: -0.09143\n",
      "Feature: 49, Score: -0.07750\n",
      "Feature: 50, Score: 0.05402\n",
      "Feature: 51, Score: -0.01310\n",
      "Feature: 52, Score: -0.05991\n",
      "Feature: 53, Score: -0.18366\n",
      "Feature: 54, Score: 0.00214\n",
      "Feature: 55, Score: -0.00362\n",
      "Feature: 56, Score: 0.00525\n",
      "Feature: 57, Score: -9975.85006\n",
      "Feature: 58, Score: -0.16832\n",
      "Feature: 59, Score: 0.02665\n",
      "Feature: 60, Score: -179539.29600\n",
      "Feature: 61, Score: 0.01777\n",
      "Feature: 62, Score: 0.10041\n",
      "Feature: 63, Score: 0.32332\n",
      "Feature: 64, Score: -0.01951\n",
      "Feature: 65, Score: 0.07024\n",
      "Feature: 66, Score: -0.26577\n",
      "Feature: 67, Score: -0.01092\n",
      "Feature: 68, Score: -0.12955\n",
      "Feature: 69, Score: -0.00001\n",
      "Feature: 70, Score: 0.00023\n",
      "Feature: 71, Score: 0.00021\n",
      "Feature: 72, Score: -0.01884\n",
      "Feature: 73, Score: -0.00715\n",
      "Feature: 74, Score: 0.00335\n",
      "Feature: 75, Score: 21368.91199\n",
      "Feature: 76, Score: -83529.38750\n",
      "Feature: 77, Score: -0.01284\n",
      "Feature: 78, Score: 105255.79302\n",
      "Feature: 79, Score: -0.00595\n",
      "Feature: 80, Score: -0.01886\n",
      "Feature: 81, Score: -99391.87413\n",
      "Feature: 82, Score: -0.02521\n",
      "Feature: 83, Score: 0.02105\n",
      "Feature: 84, Score: -0.00133\n",
      "Feature: 85, Score: 46413.45155\n",
      "Feature: 86, Score: 54452.12575\n",
      "Feature: 87, Score: -23968.15045\n",
      "Feature: 88, Score: 16154.34968\n",
      "Feature: 89, Score: -19901.63669\n",
      "Feature: 90, Score: -50079.77086\n",
      "Feature: 91, Score: -11062.31667\n",
      "Feature: 92, Score: 30461.64109\n",
      "Feature: 93, Score: 41876.08809\n",
      "Feature: 94, Score: -0.06345\n",
      "Feature: 95, Score: 0.02892\n",
      "Feature: 96, Score: -0.05620\n",
      "Feature: 97, Score: 0.09272\n",
      "Feature: 98, Score: -51874.10147\n",
      "Feature: 99, Score: -0.00333\n",
      "Feature: 100, Score: 0.12143\n",
      "Feature: 101, Score: 0.04656\n",
      "Feature: 102, Score: 11950.88852\n",
      "Feature: 103, Score: 31456.37669\n",
      "Feature: 104, Score: -46150.17379\n",
      "Feature: 105, Score: -0.00105\n",
      "Feature: 106, Score: -13459.17392\n",
      "Feature: 107, Score: 56588.05609\n",
      "Feature: 108, Score: 52965.50930\n",
      "Feature: 109, Score: -47250.83107\n",
      "Feature: 110, Score: -0.00878\n",
      "Feature: 111, Score: -4201.76768\n",
      "Feature: 112, Score: 0.30201\n",
      "Feature: 113, Score: 5.63057\n",
      "Feature: 114, Score: 11632.41166\n",
      "Feature: 115, Score: -0.12703\n",
      "Feature: 116, Score: -0.12489\n",
      "Feature: 117, Score: -0.02135\n",
      "Feature: 118, Score: 0.06432\n",
      "Feature: 119, Score: -0.10688\n",
      "Feature: 120, Score: -0.17342\n",
      "Feature: 121, Score: 0.05314\n",
      "Feature: 122, Score: 0.22723\n",
      "Feature: 123, Score: -0.00008\n",
      "Feature: 124, Score: -0.00045\n",
      "Feature: 125, Score: -0.00020\n",
      "Feature: 126, Score: 0.04688\n",
      "Feature: 127, Score: -0.02918\n",
      "Feature: 128, Score: 0.01992\n",
      "Feature: 129, Score: -0.01104\n",
      "Feature: 130, Score: 0.00000\n",
      "Feature: 131, Score: 0.00108\n",
      "Feature: 132, Score: 0.00723\n",
      "Feature: 133, Score: 0.00567\n",
      "Feature: 134, Score: -0.00212\n",
      "Feature: 135, Score: 0.03181\n",
      "Feature: 136, Score: -0.01330\n",
      "Feature: 137, Score: 0.00418\n",
      "Feature: 138, Score: -0.01141\n",
      "Feature: 139, Score: -0.00490\n",
      "Feature: 140, Score: -0.00696\n",
      "Feature: 141, Score: 0.00052\n",
      "Feature: 142, Score: 0.01142\n",
      "Feature: 143, Score: 0.00081\n",
      "Feature: 144, Score: 0.00668\n",
      "Feature: 145, Score: -0.00206\n",
      "Feature: 146, Score: -0.17338\n",
      "Feature: 147, Score: -0.05048\n",
      "Feature: 148, Score: 0.02158\n",
      "Feature: 149, Score: -0.03481\n",
      "Feature: 150, Score: 0.00828\n",
      "Feature: 151, Score: -0.05457\n",
      "Feature: 152, Score: -0.01307\n",
      "Feature: 153, Score: 0.03972\n",
      "Feature: 154, Score: -0.08576\n",
      "Feature: 155, Score: -0.07437\n",
      "Feature: 156, Score: 0.12540\n",
      "Feature: 157, Score: -0.16586\n",
      "Feature: 158, Score: -0.02252\n",
      "Feature: 159, Score: -0.00595\n",
      "Feature: 160, Score: 7635.55468\n",
      "Feature: 161, Score: -0.06403\n",
      "Feature: 162, Score: 0.08507\n",
      "Feature: 163, Score: 0.17837\n",
      "Feature: 164, Score: 0.01728\n",
      "Feature: 165, Score: 41844.09205\n",
      "Feature: 166, Score: -0.55511\n",
      "Feature: 167, Score: -0.47998\n",
      "Feature: 168, Score: -36177.73175\n",
      "Feature: 169, Score: 0.16138\n",
      "Feature: 170, Score: 0.64820\n",
      "Feature: 171, Score: 0.07838\n",
      "Feature: 172, Score: -0.26153\n",
      "Feature: 173, Score: 0.89405\n",
      "Feature: 174, Score: 0.95011\n",
      "Feature: 175, Score: -0.24497\n",
      "Feature: 176, Score: -1.29894\n",
      "Feature: 177, Score: -0.00000\n",
      "Feature: 178, Score: -0.00002\n",
      "Feature: 179, Score: -0.00007\n",
      "Feature: 180, Score: -0.20732\n",
      "Feature: 181, Score: 0.35287\n",
      "Feature: 182, Score: 0.06195\n",
      "Feature: 183, Score: 0.00384\n",
      "Feature: 184, Score: 0.03502\n",
      "Feature: 185, Score: -0.01538\n",
      "Feature: 186, Score: -0.04105\n",
      "Feature: 187, Score: -0.03372\n",
      "Feature: 188, Score: 0.00248\n",
      "Feature: 189, Score: -0.29567\n",
      "Feature: 190, Score: 0.00122\n",
      "Feature: 191, Score: 0.06928\n",
      "Feature: 192, Score: -0.00683\n",
      "Feature: 193, Score: 0.04792\n",
      "Feature: 194, Score: -0.01951\n",
      "Feature: 195, Score: -0.08875\n",
      "Feature: 196, Score: -0.09151\n",
      "Feature: 197, Score: -0.08754\n",
      "Feature: 198, Score: -0.10566\n",
      "Feature: 199, Score: 0.03019\n",
      "Feature: 200, Score: 0.16503\n",
      "Feature: 201, Score: 0.08606\n",
      "Feature: 202, Score: 0.05201\n",
      "Feature: 203, Score: 0.00666\n",
      "Feature: 204, Score: 0.01578\n",
      "Feature: 205, Score: 0.00154\n",
      "Feature: 206, Score: 0.07994\n",
      "Feature: 207, Score: 0.01517\n",
      "Feature: 208, Score: -0.06261\n",
      "Feature: 209, Score: 0.05247\n",
      "Feature: 210, Score: 0.06408\n",
      "Feature: 211, Score: 0.02218\n",
      "Feature: 212, Score: -0.05161\n",
      "Feature: 213, Score: 0.07891\n",
      "Feature: 214, Score: -0.06241\n",
      "Feature: 215, Score: 0.12714\n",
      "Feature: 216, Score: -0.04469\n",
      "Feature: 217, Score: -0.00066\n",
      "Feature: 218, Score: 0.21849\n",
      "Feature: 219, Score: 13543.37401\n",
      "Feature: 220, Score: -7.14739\n",
      "Feature: 221, Score: 6.91192\n",
      "Feature: 222, Score: -4436.30479\n",
      "Feature: 223, Score: 43.10982\n",
      "Feature: 224, Score: 5.26077\n",
      "Feature: 225, Score: 21.91195\n",
      "Feature: 226, Score: 34.40989\n",
      "Feature: 227, Score: 39.68417\n",
      "Feature: 228, Score: -25.24002\n",
      "Feature: 229, Score: -72.73837\n",
      "Feature: 230, Score: -54.73723\n",
      "Feature: 231, Score: -0.01109\n",
      "Feature: 232, Score: 0.01202\n",
      "Feature: 233, Score: 0.04217\n",
      "Feature: 234, Score: 8.39898\n",
      "Feature: 235, Score: 0.70826\n",
      "Feature: 236, Score: 5.52997\n",
      "Feature: 237, Score: 0.50632\n",
      "Feature: 238, Score: -0.30079\n",
      "Feature: 239, Score: 0.11185\n",
      "Feature: 240, Score: 1.73727\n",
      "Feature: 241, Score: 1.70240\n",
      "Feature: 242, Score: -0.06083\n",
      "Feature: 243, Score: -2.37712\n",
      "Feature: 244, Score: -1.40277\n",
      "Feature: 245, Score: -3.59921\n",
      "Feature: 246, Score: 1.28516\n",
      "Feature: 247, Score: 0.38511\n",
      "Feature: 248, Score: -1.87309\n",
      "Feature: 249, Score: 0.11741\n",
      "Feature: 250, Score: 1.49143\n",
      "Feature: 251, Score: -0.88596\n",
      "Feature: 252, Score: -0.15872\n",
      "Feature: 253, Score: 1.52123\n",
      "Feature: 254, Score: 0.00891\n",
      "Feature: 255, Score: 0.53157\n",
      "Feature: 256, Score: 1.73017\n",
      "Feature: 257, Score: -0.86099\n",
      "Feature: 258, Score: -2.48688\n",
      "Feature: 259, Score: 0.59609\n",
      "Feature: 260, Score: 3.28546\n",
      "Feature: 261, Score: 1.34286\n",
      "Feature: 262, Score: 2.80216\n",
      "Feature: 263, Score: -1.54344\n",
      "Feature: 264, Score: -0.25553\n",
      "Feature: 265, Score: 0.52457\n",
      "Feature: 266, Score: -2.57982\n",
      "Feature: 267, Score: -0.50889\n",
      "Feature: 268, Score: -0.03270\n",
      "Feature: 269, Score: -0.70943\n",
      "Feature: 270, Score: 8.06208\n",
      "Feature: 271, Score: -1.66921\n",
      "Feature: 272, Score: -424.74348\n",
      "Feature: 273, Score: -836.15585\n",
      "Feature: 274, Score: -2.30118\n",
      "Feature: 275, Score: -0.09870\n",
      "Feature: 276, Score: -994.15123\n",
      "Feature: 277, Score: 0.01704\n",
      "Feature: 278, Score: 0.06037\n",
      "Feature: 279, Score: -0.19098\n",
      "Feature: 280, Score: -0.04995\n",
      "Feature: 281, Score: -0.01516\n",
      "Feature: 282, Score: -0.09083\n",
      "Feature: 283, Score: 0.01471\n",
      "Feature: 284, Score: 0.00763\n",
      "Feature: 285, Score: 0.00008\n",
      "Feature: 286, Score: 0.00089\n",
      "Feature: 287, Score: -0.00014\n",
      "Feature: 288, Score: 0.07762\n",
      "Feature: 289, Score: 0.19478\n",
      "Feature: 290, Score: -0.17216\n",
      "Feature: 291, Score: 0.00846\n",
      "Feature: 292, Score: 0.00696\n",
      "Feature: 293, Score: -0.00298\n",
      "Feature: 294, Score: -0.02320\n",
      "Feature: 295, Score: 0.04673\n",
      "Feature: 296, Score: -0.01789\n",
      "Feature: 297, Score: 0.03611\n",
      "Feature: 298, Score: 0.02807\n",
      "Feature: 299, Score: -0.01970\n",
      "Feature: 300, Score: -0.00581\n",
      "Feature: 301, Score: -0.00006\n",
      "Feature: 302, Score: -0.00066\n",
      "Feature: 303, Score: 0.01330\n",
      "Feature: 304, Score: -0.00524\n",
      "Feature: 305, Score: -0.01411\n",
      "Feature: 306, Score: 0.00218\n",
      "Feature: 307, Score: -0.02296\n",
      "Feature: 308, Score: 0.00588\n",
      "Feature: 309, Score: -0.00230\n",
      "Feature: 310, Score: 0.00288\n",
      "Feature: 311, Score: -0.06454\n",
      "Feature: 312, Score: -0.00232\n",
      "Feature: 313, Score: -0.02281\n",
      "Feature: 314, Score: -0.01633\n",
      "Feature: 315, Score: 0.02015\n",
      "Feature: 316, Score: -0.03846\n",
      "Feature: 317, Score: -0.02700\n",
      "Feature: 318, Score: 0.01541\n",
      "Feature: 319, Score: -0.10814\n",
      "Feature: 320, Score: -0.01593\n",
      "Feature: 321, Score: -0.00623\n",
      "Feature: 322, Score: 7635.46930\n",
      "Feature: 323, Score: -0.10966\n",
      "Feature: 324, Score: 0.05872\n",
      "Feature: 325, Score: 0.05897\n",
      "Feature: 326, Score: -0.16119\n",
      "Feature: 327, Score: 245.61580\n",
      "Feature: 328, Score: 4.10749\n",
      "Feature: 329, Score: 0.04380\n",
      "Feature: 330, Score: -1664.93561\n",
      "Feature: 331, Score: -0.06975\n",
      "Feature: 332, Score: 0.04815\n",
      "Feature: 333, Score: 0.13472\n",
      "Feature: 334, Score: 0.07640\n",
      "Feature: 335, Score: 0.01835\n",
      "Feature: 336, Score: 0.07526\n",
      "Feature: 337, Score: 0.04214\n",
      "Feature: 338, Score: -0.11617\n",
      "Feature: 339, Score: -0.00003\n",
      "Feature: 340, Score: -0.00075\n",
      "Feature: 341, Score: -0.00029\n",
      "Feature: 342, Score: -0.11090\n",
      "Feature: 343, Score: -0.06310\n",
      "Feature: 344, Score: 0.20912\n",
      "Feature: 345, Score: -0.01061\n",
      "Feature: 346, Score: 0.01899\n",
      "Feature: 347, Score: -0.00268\n",
      "Feature: 348, Score: -0.02928\n",
      "Feature: 349, Score: -0.01208\n",
      "Feature: 350, Score: 0.02991\n",
      "Feature: 351, Score: 0.02185\n",
      "Feature: 352, Score: -0.02903\n",
      "Feature: 353, Score: -0.06212\n",
      "Feature: 354, Score: 0.01316\n",
      "Feature: 355, Score: -0.00493\n",
      "Feature: 356, Score: 0.00635\n",
      "Feature: 357, Score: 0.01043\n",
      "Feature: 358, Score: -0.00735\n",
      "Feature: 359, Score: -0.00228\n",
      "Feature: 360, Score: -0.00793\n",
      "Feature: 361, Score: 0.00520\n",
      "Feature: 362, Score: 0.07180\n",
      "Feature: 363, Score: 0.00001\n",
      "Feature: 364, Score: -0.00876\n",
      "Feature: 365, Score: -0.03521\n",
      "Feature: 366, Score: -0.00486\n",
      "Feature: 367, Score: 0.03390\n",
      "Feature: 368, Score: -0.01531\n",
      "Feature: 369, Score: -0.00649\n",
      "Feature: 370, Score: 0.05619\n",
      "Feature: 371, Score: 0.03431\n",
      "Feature: 372, Score: -0.12443\n",
      "Feature: 373, Score: 0.01083\n",
      "Feature: 374, Score: 0.00757\n",
      "Feature: 375, Score: 0.01595\n",
      "Feature: 376, Score: 0.00698\n",
      "Feature: 377, Score: -0.15226\n",
      "Feature: 378, Score: -0.00336\n",
      "Feature: 379, Score: -0.11673\n",
      "Feature: 380, Score: 1.39758\n",
      "Feature: 381, Score: 439.47849\n",
      "Feature: 382, Score: -35.71947\n",
      "Feature: 383, Score: -4.31582\n",
      "Feature: 384, Score: -597.84217\n",
      "Feature: 385, Score: 0.73906\n",
      "Feature: 386, Score: -0.41018\n",
      "Feature: 387, Score: -4.63976\n",
      "Feature: 388, Score: -1.33595\n",
      "Feature: 389, Score: -5.16263\n",
      "Feature: 390, Score: 1.14439\n",
      "Feature: 391, Score: 0.34075\n",
      "Feature: 392, Score: 7.35244\n",
      "Feature: 393, Score: 0.01517\n",
      "Feature: 394, Score: 0.05226\n",
      "Feature: 395, Score: -0.00423\n",
      "Feature: 396, Score: 0.21687\n",
      "Feature: 397, Score: 0.56118\n",
      "Feature: 398, Score: -1.76125\n",
      "Feature: 399, Score: 0.80085\n",
      "Feature: 400, Score: -0.00795\n",
      "Feature: 401, Score: 0.00556\n",
      "Feature: 402, Score: 0.61025\n",
      "Feature: 403, Score: 0.42032\n",
      "Feature: 404, Score: -0.36320\n",
      "Feature: 405, Score: -0.03674\n",
      "Feature: 406, Score: 0.29889\n",
      "Feature: 407, Score: 0.53422\n",
      "Feature: 408, Score: -0.08666\n",
      "Feature: 409, Score: 0.37609\n",
      "Feature: 410, Score: 0.06600\n",
      "Feature: 411, Score: 0.72520\n",
      "Feature: 412, Score: -0.73795\n",
      "Feature: 413, Score: -0.38331\n",
      "Feature: 414, Score: 0.31977\n",
      "Feature: 415, Score: 0.16738\n",
      "Feature: 416, Score: 1.91903\n",
      "Feature: 417, Score: 1.65052\n",
      "Feature: 418, Score: -1.17416\n",
      "Feature: 419, Score: 0.46568\n",
      "Feature: 420, Score: -0.35041\n",
      "Feature: 421, Score: -0.17920\n",
      "Feature: 422, Score: 0.95137\n",
      "Feature: 423, Score: 1.48304\n",
      "Feature: 424, Score: 0.34265\n",
      "Feature: 425, Score: -0.12229\n",
      "Feature: 426, Score: 0.66450\n",
      "Feature: 427, Score: 0.66361\n",
      "Feature: 428, Score: -0.27259\n",
      "Feature: 429, Score: -1.18854\n",
      "Feature: 430, Score: 0.63260\n",
      "Feature: 431, Score: -0.48141\n",
      "Feature: 432, Score: -0.06719\n",
      "Feature: 433, Score: 0.14148\n",
      "Feature: 434, Score: 5.46035\n",
      "Feature: 435, Score: -1144.38139\n",
      "Feature: 436, Score: -58.86061\n",
      "Feature: 437, Score: -76.46942\n",
      "Feature: 438, Score: 37.89260\n",
      "Feature: 439, Score: 122.99817\n",
      "Feature: 440, Score: -27.82926\n",
      "Feature: 441, Score: 141.64461\n",
      "Feature: 442, Score: 114.74570\n",
      "Feature: 443, Score: 44.88371\n",
      "Feature: 444, Score: -75.50571\n",
      "Feature: 445, Score: -221.80620\n",
      "Feature: 446, Score: -0.60811\n",
      "Feature: 447, Score: 0.02441\n",
      "Feature: 448, Score: -0.22384\n",
      "Feature: 449, Score: -0.20682\n",
      "Feature: 450, Score: 2.01213\n",
      "Feature: 451, Score: -7.38127\n",
      "Feature: 452, Score: 34.74518\n",
      "Feature: 453, Score: -1.65114\n",
      "Feature: 454, Score: 1.00827\n",
      "Feature: 455, Score: -5.89380\n",
      "Feature: 456, Score: 7.91061\n",
      "Feature: 457, Score: 7.12812\n",
      "Feature: 458, Score: 0.80810\n",
      "Feature: 459, Score: 8.62609\n",
      "Feature: 460, Score: -0.63958\n",
      "Feature: 461, Score: -7.27486\n",
      "Feature: 462, Score: 0.09079\n",
      "Feature: 463, Score: 0.85240\n",
      "Feature: 464, Score: -0.56636\n",
      "Feature: 465, Score: 0.94868\n",
      "Feature: 466, Score: -4.61134\n",
      "Feature: 467, Score: -0.46460\n",
      "Feature: 468, Score: 1.36023\n",
      "Feature: 469, Score: 0.48079\n",
      "Feature: 470, Score: 2.25656\n",
      "Feature: 471, Score: 1.34025\n",
      "Feature: 472, Score: 2.95051\n",
      "Feature: 473, Score: -0.07371\n",
      "Feature: 474, Score: -3.32203\n",
      "Feature: 475, Score: -0.04197\n",
      "Feature: 476, Score: -3.24581\n",
      "Feature: 477, Score: -2.74588\n",
      "Feature: 478, Score: 3.39752\n",
      "Feature: 479, Score: 2.16060\n",
      "Feature: 480, Score: 1.50722\n",
      "Feature: 481, Score: -0.88103\n",
      "Feature: 482, Score: 0.50033\n",
      "Feature: 483, Score: 0.30677\n",
      "Feature: 484, Score: -0.08993\n",
      "Feature: 485, Score: 1.90690\n",
      "Feature: 486, Score: 3.23507\n",
      "Feature: 487, Score: -3.97769\n",
      "Feature: 488, Score: -0.11842\n",
      "Feature: 489, Score: 56.15944\n",
      "Feature: 490, Score: 2.98260\n",
      "Feature: 491, Score: 0.09558\n",
      "Feature: 492, Score: -83.39705\n",
      "Feature: 493, Score: -0.03331\n",
      "Feature: 494, Score: 0.03486\n",
      "Feature: 495, Score: 0.06742\n",
      "Feature: 496, Score: -0.08221\n",
      "Feature: 497, Score: 0.01287\n",
      "Feature: 498, Score: 0.04635\n",
      "Feature: 499, Score: 0.10961\n",
      "Feature: 500, Score: -0.13778\n",
      "Feature: 501, Score: -0.00012\n",
      "Feature: 502, Score: -0.00056\n",
      "Feature: 503, Score: 0.00019\n",
      "Feature: 504, Score: -0.00315\n",
      "Feature: 505, Score: 0.05682\n",
      "Feature: 506, Score: 0.10286\n",
      "Feature: 507, Score: -0.04657\n",
      "Feature: 508, Score: 0.01480\n",
      "Feature: 509, Score: 0.00321\n",
      "Feature: 510, Score: 0.00841\n",
      "Feature: 511, Score: -0.04515\n",
      "Feature: 512, Score: 0.02623\n",
      "Feature: 513, Score: 0.02900\n",
      "Feature: 514, Score: -0.02133\n",
      "Feature: 515, Score: 0.00331\n",
      "Feature: 516, Score: -0.00620\n",
      "Feature: 517, Score: 0.00210\n",
      "Feature: 518, Score: -0.00152\n",
      "Feature: 519, Score: -0.01008\n",
      "Feature: 520, Score: 0.01632\n",
      "Feature: 521, Score: 0.01204\n",
      "Feature: 522, Score: 0.01596\n",
      "Feature: 523, Score: 0.00366\n",
      "Feature: 524, Score: -0.02220\n",
      "Feature: 525, Score: -0.02315\n",
      "Feature: 526, Score: 0.00781\n",
      "Feature: 527, Score: -0.00613\n",
      "Feature: 528, Score: -0.00572\n",
      "Feature: 529, Score: -0.00503\n",
      "Feature: 530, Score: -0.01293\n",
      "Feature: 531, Score: -0.01388\n",
      "Feature: 532, Score: 0.00191\n",
      "Feature: 533, Score: -0.00270\n",
      "Feature: 534, Score: -0.00336\n",
      "Feature: 535, Score: 0.00127\n",
      "Feature: 536, Score: 0.00385\n",
      "Feature: 537, Score: 0.00743\n",
      "Feature: 538, Score: -0.00213\n",
      "Feature: 539, Score: 0.00078\n",
      "Feature: 540, Score: -0.00516\n",
      "Feature: 541, Score: 0.00189\n",
      "Feature: 542, Score: 0.02739\n",
      "Feature: 543, Score: 5.37887\n",
      "Feature: 544, Score: -0.57671\n",
      "Feature: 545, Score: -0.03896\n",
      "Feature: 546, Score: 2.43203\n",
      "Feature: 547, Score: 0.00328\n",
      "Feature: 548, Score: -0.02817\n",
      "Feature: 549, Score: -0.08818\n",
      "Feature: 550, Score: -0.03889\n",
      "Feature: 551, Score: 0.07307\n",
      "Feature: 552, Score: -0.05766\n",
      "Feature: 553, Score: 0.04804\n",
      "Feature: 554, Score: -0.02571\n",
      "Feature: 555, Score: 0.00005\n",
      "Feature: 556, Score: 0.00013\n",
      "Feature: 557, Score: 0.00031\n",
      "Feature: 558, Score: 0.05336\n",
      "Feature: 559, Score: 0.06244\n",
      "Feature: 560, Score: -0.07852\n",
      "Feature: 561, Score: 0.00765\n",
      "Feature: 562, Score: -0.02064\n",
      "Feature: 563, Score: 0.02378\n",
      "Feature: 564, Score: 0.02800\n",
      "Feature: 565, Score: -0.01620\n",
      "Feature: 566, Score: -0.01225\n",
      "Feature: 567, Score: 0.03121\n",
      "Feature: 568, Score: 0.02576\n",
      "Feature: 569, Score: 0.02802\n",
      "Feature: 570, Score: -0.00034\n",
      "Feature: 571, Score: -0.01412\n",
      "Feature: 572, Score: 0.01292\n",
      "Feature: 573, Score: 0.01876\n",
      "Feature: 574, Score: -0.02483\n",
      "Feature: 575, Score: -0.00491\n",
      "Feature: 576, Score: -0.02840\n",
      "Feature: 577, Score: 0.00642\n",
      "Feature: 578, Score: 0.01106\n",
      "Feature: 579, Score: 0.01812\n",
      "Feature: 580, Score: -0.00039\n",
      "Feature: 581, Score: -0.00221\n",
      "Feature: 582, Score: -0.00506\n",
      "Feature: 583, Score: 0.00103\n",
      "Feature: 584, Score: 0.00589\n",
      "Feature: 585, Score: -0.00228\n",
      "Feature: 586, Score: -0.00057\n",
      "Feature: 587, Score: 0.00315\n",
      "Feature: 588, Score: 0.00993\n",
      "Feature: 589, Score: -0.00154\n",
      "Feature: 590, Score: -0.00760\n",
      "Feature: 591, Score: -0.00709\n",
      "Feature: 592, Score: -0.00454\n",
      "Feature: 593, Score: -0.00892\n",
      "Feature: 594, Score: -0.03824\n",
      "Feature: 595, Score: 0.01364\n",
      "Feature: 596, Score: -0.87334\n",
      "Feature: 597, Score: -0.26944\n",
      "Feature: 598, Score: 20.59533\n",
      "Feature: 599, Score: 2.99291\n",
      "Feature: 600, Score: -0.39707\n",
      "Feature: 601, Score: 0.51895\n",
      "Feature: 602, Score: 0.75360\n",
      "Feature: 603, Score: 1.62151\n",
      "Feature: 604, Score: 0.47582\n",
      "Feature: 605, Score: 2.39018\n",
      "Feature: 606, Score: 0.51333\n",
      "Feature: 607, Score: -2.02200\n",
      "Feature: 608, Score: -4.55943\n",
      "Feature: 609, Score: -0.01007\n",
      "Feature: 610, Score: -0.02685\n",
      "Feature: 611, Score: 0.00598\n",
      "Feature: 612, Score: 0.30913\n",
      "Feature: 613, Score: 1.42293\n",
      "Feature: 614, Score: 0.56976\n",
      "Feature: 615, Score: -0.89040\n",
      "Feature: 616, Score: 0.03413\n",
      "Feature: 617, Score: -0.21780\n",
      "Feature: 618, Score: -1.09987\n",
      "Feature: 619, Score: -0.20314\n",
      "Feature: 620, Score: 0.32836\n",
      "Feature: 621, Score: -0.19356\n",
      "Feature: 622, Score: -0.69809\n",
      "Feature: 623, Score: -0.45992\n",
      "Feature: 624, Score: -0.03003\n",
      "Feature: 625, Score: -0.01331\n",
      "Feature: 626, Score: 0.05816\n",
      "Feature: 627, Score: -0.92421\n",
      "Feature: 628, Score: 1.17307\n",
      "Feature: 629, Score: 0.60515\n",
      "Feature: 630, Score: 0.24087\n",
      "Feature: 631, Score: 0.03827\n",
      "Feature: 632, Score: -1.42176\n",
      "Feature: 633, Score: -1.32670\n",
      "Feature: 634, Score: 0.69719\n",
      "Feature: 635, Score: -0.33027\n",
      "Feature: 636, Score: 0.23528\n",
      "Feature: 637, Score: 0.13350\n",
      "Feature: 638, Score: -0.73080\n",
      "Feature: 639, Score: -0.95408\n",
      "Feature: 640, Score: -0.20085\n",
      "Feature: 641, Score: 0.08797\n",
      "Feature: 642, Score: -0.49823\n",
      "Feature: 643, Score: -0.34122\n",
      "Feature: 644, Score: 0.26441\n",
      "Feature: 645, Score: 0.72628\n",
      "Feature: 646, Score: -0.23814\n",
      "Feature: 647, Score: 0.34498\n",
      "Feature: 648, Score: 0.15656\n",
      "Feature: 649, Score: -0.09911\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "for i,j in enumerate(importance):\n",
    "    #if j > 10 or j < -10:\n",
    "    print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-507945340.1069534\n",
      "105255.79301787948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0991121024126187"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())\n",
    "importance[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 50 components, MSE Model 3 = 0.0368\n",
      "MLR with 50 components, MAE Model 3 = 0.11\n",
      "MLR with 50 components, 1 - MAE Model 3 = 0.89\n",
      "-------------------------------------------\n",
      "MLR with 100 components, MSE Model 3 = 0.0562\n",
      "MLR with 100 components, MAE Model 3 = 0.107\n",
      "MLR with 100 components, 1 - MAE Model 3 = 0.893\n",
      "-------------------------------------------\n",
      "MLR with 200 components, MSE Model 3 = 0.0473\n",
      "MLR with 200 components, MAE Model 3 = 0.106\n",
      "MLR with 200 components, 1 - MAE Model 3 = 0.894\n",
      "-------------------------------------------\n",
      "MLR with 300 components, MSE Model 3 = 0.0255\n",
      "MLR with 300 components, MAE Model 3 = 0.103\n",
      "MLR with 300 components, 1 - MAE Model 3 = 0.897\n",
      "-------------------------------------------\n",
      "MLR with 400 components, MSE Model 3 = 0.0399\n",
      "MLR with 400 components, MAE Model 3 = 0.103\n",
      "MLR with 400 components, 1 - MAE Model 3 = 0.897\n",
      "-------------------------------------------\n",
      "MLR with 500 components, MSE Model 3 = 0.0322\n",
      "MLR with 500 components, MAE Model 3 = 0.102\n",
      "MLR with 500 components, 1 - MAE Model 3 = 0.898\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_mlr_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    X_3_test_mlr_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_3_train_mlr_PCA, y_3_train)\n",
    "    mlr_y_3_PCA = regressor.predict(X_3_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 3 = {:.3}\".format(mean_squared_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 3 = {:.3}\".format(mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 3 = {:.3}\".format(1-mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0158\n",
      "MAE 0.101\n",
      "1 - MAE 0.899\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_3_train_dummy, y_3_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_3 = rfr.predict(X_3_test_dummy)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_3_test, rfr_y_3)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_3_test, rfr_y_3)))\n",
    "print(\"1 - MAE {:.3}\".format((1 - mean_absolute_error(y_3_test, rfr_y_3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 50 components, MSE Model 3 = 0.0182\n",
      "RFR with 50 components, MAE Model 3 = 0.108\n",
      "RFR with 50 components, 1 - MAE Model 3 = 0.892\n",
      "-------------------------------------------\n",
      "RFR with 100 components, MSE Model 3 = 0.0182\n",
      "RFR with 100 components, MAE Model 3 = 0.108\n",
      "RFR with 100 components, 1 - MAE Model 3 = 0.892\n",
      "-------------------------------------------\n",
      "RFR with 200 components, MSE Model 3 = 0.0188\n",
      "RFR with 200 components, MAE Model 3 = 0.11\n",
      "RFR with 200 components, 1 - MAE Model 3 = 0.89\n",
      "-------------------------------------------\n",
      "RFR with 300 components, MSE Model 3 = 0.0188\n",
      "RFR with 300 components, MAE Model 3 = 0.11\n",
      "RFR with 300 components, 1 - MAE Model 3 = 0.89\n",
      "-------------------------------------------\n",
      "RFR with 400 components, MSE Model 3 = 0.0186\n",
      "RFR with 400 components, MAE Model 3 = 0.109\n",
      "RFR with 400 components, 1 - MAE Model 3 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 500 components, MSE Model 3 = 0.0193\n",
      "RFR with 500 components, MAE Model 3 = 0.111\n",
      "RFR with 500 components, 1 - MAE Model 3 = 0.889\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_3_train_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    rfr_X_3_test_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    rfr.fit(rfr_X_3_train_PCA, y_3_train.ravel())\n",
    "    rfr_y_3_PCA = rfr.predict(rfr_X_3_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 3 = {:.3}\".format(mean_squared_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 3 = {:.3}\".format(mean_absolute_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 3 = {:.3}\".format(1-mean_absolute_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the ethnicity dummies\n",
    "X_3_train_scale = X_3_train_dummy.copy()\n",
    "X_3_test_scale = X_3_test_dummy.copy()\n",
    "\n",
    "X_3_train_scale[:, 2:] = X_sc.fit_transform(X_3_train_dummy[:, 2:])\n",
    "X_3_test_scale[:, 2:] = X_sc.transform(X_3_test_dummy[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0148\n",
      "MAE 0.097\n",
      "1 - MAE 0.903\n",
      "Wall time: 19.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vectore regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_3_train_scale, y_3_train.ravel())\n",
    "svr_y_3 = svr.predict(X_3_test_scale)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_3_test, svr_y_3)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_3_test, svr_y_3)))\n",
    "print(\"1 - MAE {:.3}\".format((1-mean_absolute_error(y_3_test, svr_y_3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 50 components, MSE Model 3 = 0.0159\n",
      "SVR with 50 components, MAE Model 3 = 0.101\n",
      "SVR with 50 components, 1 - MAE Model 3 = 0.899\n",
      "-------------------------------------------\n",
      "SVR with 100 components, MSE Model 3 = 0.0153\n",
      "SVR with 100 components, MAE Model 3 = 0.0984\n",
      "SVR with 100 components, 1 - MAE Model 3 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 200 components, MSE Model 3 = 0.0151\n",
      "SVR with 200 components, MAE Model 3 = 0.098\n",
      "SVR with 200 components, 1 - MAE Model 3 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 300 components, MSE Model 3 = 0.015\n",
      "SVR with 300 components, MAE Model 3 = 0.0974\n",
      "SVR with 300 components, 1 - MAE Model 3 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 400 components, MSE Model 3 = 0.0149\n",
      "SVR with 400 components, MAE Model 3 = 0.0972\n",
      "SVR with 400 components, 1 - MAE Model 3 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 500 components, MSE Model 3 = 0.0148\n",
      "SVR with 500 components, MAE Model 3 = 0.097\n",
      "SVR with 500 components, 1 - MAE Model 3 = 0.903\n",
      "-------------------------------------------\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_PCA = pca.fit_transform(X_3_train_scale)\n",
    "    X_3_test_PCA = pca.transform(X_3_test_scale)\n",
    "    \n",
    "    svr.fit(X_3_train_PCA, y_3_train.ravel())\n",
    "    svr_y_3_PCA = svr.predict(X_3_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 3 = {:.3}\".format(mean_squared_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 3 = {:.3}\".format(mean_absolute_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 3 = {:.3}\".format(1-mean_absolute_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Interview ~ facial + gender + ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the training and validation (test) set for Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 4, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_gen_eth_int = merged_df[['videoname', 'ethnicity', 'gender', 'interview']]\n",
    "training_face_gen_eth = pd.merge(df_train, df_gen_eth_int, how = \"left\", on = [\"videoname\"])\n",
    "#training_face_gen_eth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 4\n",
    "df_val_gen_eth_int = merged_val_df[['videoname', 'ethnicity', 'gender', 'interview']]\n",
    "val_face_gen_eth = pd.merge(df_val, df_val_gen_eth_int, how = \"left\", on = [\"videoname\"])\n",
    "#val_face_gen_eth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Splitting the (in)dependent variables\n",
    "X_4_train = training_face_gen_eth.drop(['videoname', 'interview'], axis=1).values\n",
    "y_4_train = training_face_gen_eth.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_4_test = val_face_gen_eth.drop(['videoname', 'interview'], axis=1).values\n",
    "y_4_test = val_face_gen_eth.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_4_train[:, -1] = le.fit_transform(X_4_train[:, -1])\n",
    "X_4_test[:, -1] = le.fit_transform(X_4_test[:, -1])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-2])], remainder='passthrough')\n",
    "X_4_train = np.array(ct.fit_transform(X_4_train))\n",
    "X_4_test = np.array(ct.fit_transform(X_4_test))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_4_train_dummy = X_4_train.copy()\n",
    "X_4_train_dummy = X_4_train_dummy[:,1:]\n",
    "\n",
    "X_4_test_dummy = X_4_test.copy()\n",
    "X_4_test_dummy = X_4_test_dummy[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0304\n",
      "MAE 0.102\n",
      "1 - MAE 0.898\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_4_train_dummy, y_4_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_4_pred = regressor.predict(X_4_test_dummy)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_4_test, y_4_pred)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_4_test, y_4_pred)))\n",
    "print(\"1 - MAE {:.3}\".format((1-mean_absolute_error(y_4_test, y_4_pred))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 0, Score: -0.02449\n",
      "Feature: 1, Score: -0.05148\n",
      "Feature: 2, Score: 0.00663\n",
      "Feature: 3, Score: -194126435.35250\n",
      "Feature: 4, Score: -0.24773\n",
      "Feature: 5, Score: -0.00163\n",
      "Feature: 6, Score: -3031105910.65956\n",
      "Feature: 7, Score: 0.21354\n",
      "Feature: 8, Score: -0.79262\n",
      "Feature: 9, Score: 0.16759\n",
      "Feature: 10, Score: 0.03264\n",
      "Feature: 11, Score: -0.81788\n",
      "Feature: 12, Score: -0.52267\n",
      "Feature: 13, Score: -0.27242\n",
      "Feature: 14, Score: 1.61464\n",
      "Feature: 15, Score: 0.00003\n",
      "Feature: 16, Score: 0.00017\n",
      "Feature: 17, Score: 0.00008\n",
      "Feature: 18, Score: 0.06473\n",
      "Feature: 19, Score: 0.01521\n",
      "Feature: 20, Score: -0.05866\n",
      "Feature: 21, Score: 0.00101\n",
      "Feature: 22, Score: -0.06314\n",
      "Feature: 23, Score: 0.00229\n",
      "Feature: 24, Score: 0.04799\n",
      "Feature: 25, Score: -0.00595\n",
      "Feature: 26, Score: 0.00471\n",
      "Feature: 27, Score: 0.27255\n",
      "Feature: 28, Score: 0.01943\n",
      "Feature: 29, Score: -0.00460\n",
      "Feature: 30, Score: 0.05693\n",
      "Feature: 31, Score: -0.13751\n",
      "Feature: 32, Score: 0.01204\n",
      "Feature: 33, Score: 0.14752\n",
      "Feature: 34, Score: 0.17179\n",
      "Feature: 35, Score: 0.07080\n",
      "Feature: 36, Score: 0.05264\n",
      "Feature: 37, Score: -0.08074\n",
      "Feature: 38, Score: -0.11226\n",
      "Feature: 39, Score: -0.03078\n",
      "Feature: 40, Score: -0.01352\n",
      "Feature: 41, Score: 0.03937\n",
      "Feature: 42, Score: 0.04245\n",
      "Feature: 43, Score: -0.00631\n",
      "Feature: 44, Score: -0.03643\n",
      "Feature: 45, Score: 0.00310\n",
      "Feature: 46, Score: 0.04327\n",
      "Feature: 47, Score: -0.06832\n",
      "Feature: 48, Score: -0.08956\n",
      "Feature: 49, Score: -0.07655\n",
      "Feature: 50, Score: 0.05315\n",
      "Feature: 51, Score: -0.01317\n",
      "Feature: 52, Score: -0.05958\n",
      "Feature: 53, Score: -0.18570\n",
      "Feature: 54, Score: 0.00296\n",
      "Feature: 55, Score: -0.00358\n",
      "Feature: 56, Score: 0.00551\n",
      "Feature: 57, Score: 101063.16184\n",
      "Feature: 58, Score: -0.17573\n",
      "Feature: 59, Score: 0.02811\n",
      "Feature: 60, Score: 91022.55921\n",
      "Feature: 61, Score: 0.01680\n",
      "Feature: 62, Score: 0.09860\n",
      "Feature: 63, Score: 0.31350\n",
      "Feature: 64, Score: -0.02043\n",
      "Feature: 65, Score: 0.06735\n",
      "Feature: 66, Score: -0.26747\n",
      "Feature: 67, Score: -0.00954\n",
      "Feature: 68, Score: -0.12489\n",
      "Feature: 69, Score: -0.00001\n",
      "Feature: 70, Score: 0.00023\n",
      "Feature: 71, Score: 0.00021\n",
      "Feature: 72, Score: -0.01776\n",
      "Feature: 73, Score: -0.00724\n",
      "Feature: 74, Score: 0.00179\n",
      "Feature: 75, Score: 72755.65752\n",
      "Feature: 76, Score: 38415.10667\n",
      "Feature: 77, Score: -0.01281\n",
      "Feature: 78, Score: 13082.12401\n",
      "Feature: 79, Score: -0.00563\n",
      "Feature: 80, Score: -0.01943\n",
      "Feature: 81, Score: 87558.18593\n",
      "Feature: 82, Score: -0.02521\n",
      "Feature: 83, Score: 0.02109\n",
      "Feature: 84, Score: -0.00044\n",
      "Feature: 85, Score: -116906.24210\n",
      "Feature: 86, Score: 19298.24894\n",
      "Feature: 87, Score: 33139.34311\n",
      "Feature: 88, Score: -84833.01953\n",
      "Feature: 89, Score: -6549.97130\n",
      "Feature: 90, Score: -131827.82198\n",
      "Feature: 91, Score: 7864.95515\n",
      "Feature: 92, Score: -39004.47435\n",
      "Feature: 93, Score: -14350.66562\n",
      "Feature: 94, Score: -0.06399\n",
      "Feature: 95, Score: 0.02958\n",
      "Feature: 96, Score: -0.05635\n",
      "Feature: 97, Score: 0.09284\n",
      "Feature: 98, Score: -37768.69595\n",
      "Feature: 99, Score: -0.00383\n",
      "Feature: 100, Score: 0.12040\n",
      "Feature: 101, Score: 0.04672\n",
      "Feature: 102, Score: -55610.38954\n",
      "Feature: 103, Score: -72140.34588\n",
      "Feature: 104, Score: -51529.99776\n",
      "Feature: 105, Score: -0.00073\n",
      "Feature: 106, Score: -29157.30053\n",
      "Feature: 107, Score: 21144.02305\n",
      "Feature: 108, Score: 61164.43956\n",
      "Feature: 109, Score: 28198.57385\n",
      "Feature: 110, Score: -0.00867\n",
      "Feature: 111, Score: 41350.26686\n",
      "Feature: 112, Score: 0.29904\n",
      "Feature: 113, Score: 5.66249\n",
      "Feature: 114, Score: -25722.36229\n",
      "Feature: 115, Score: -0.12760\n",
      "Feature: 116, Score: -0.12699\n",
      "Feature: 117, Score: -0.02152\n",
      "Feature: 118, Score: 0.06332\n",
      "Feature: 119, Score: -0.11162\n",
      "Feature: 120, Score: -0.17179\n",
      "Feature: 121, Score: 0.05434\n",
      "Feature: 122, Score: 0.23152\n",
      "Feature: 123, Score: -0.00007\n",
      "Feature: 124, Score: -0.00045\n",
      "Feature: 125, Score: -0.00020\n",
      "Feature: 126, Score: 0.04598\n",
      "Feature: 127, Score: -0.02802\n",
      "Feature: 128, Score: 0.02193\n",
      "Feature: 129, Score: -0.01100\n",
      "Feature: 130, Score: -0.00003\n",
      "Feature: 131, Score: 0.00132\n",
      "Feature: 132, Score: 0.00750\n",
      "Feature: 133, Score: 0.00584\n",
      "Feature: 134, Score: -0.00223\n",
      "Feature: 135, Score: 0.03212\n",
      "Feature: 136, Score: -0.01337\n",
      "Feature: 137, Score: 0.00412\n",
      "Feature: 138, Score: -0.01155\n",
      "Feature: 139, Score: -0.00514\n",
      "Feature: 140, Score: -0.00706\n",
      "Feature: 141, Score: 0.00060\n",
      "Feature: 142, Score: 0.01124\n",
      "Feature: 143, Score: 0.00082\n",
      "Feature: 144, Score: 0.00666\n",
      "Feature: 145, Score: -0.00227\n",
      "Feature: 146, Score: -0.17518\n",
      "Feature: 147, Score: -0.05023\n",
      "Feature: 148, Score: 0.02169\n",
      "Feature: 149, Score: -0.03527\n",
      "Feature: 150, Score: 0.00744\n",
      "Feature: 151, Score: -0.05467\n",
      "Feature: 152, Score: -0.01241\n",
      "Feature: 153, Score: 0.04040\n",
      "Feature: 154, Score: -0.08604\n",
      "Feature: 155, Score: -0.07399\n",
      "Feature: 156, Score: 0.12530\n",
      "Feature: 157, Score: -0.16484\n",
      "Feature: 158, Score: -0.02437\n",
      "Feature: 159, Score: -0.00648\n",
      "Feature: 160, Score: -6681.43824\n",
      "Feature: 161, Score: -0.06728\n",
      "Feature: 162, Score: 0.08536\n",
      "Feature: 163, Score: 0.17497\n",
      "Feature: 164, Score: 0.01772\n",
      "Feature: 165, Score: 6954.18833\n",
      "Feature: 166, Score: -0.56752\n",
      "Feature: 167, Score: -0.46633\n",
      "Feature: 168, Score: -841.64482\n",
      "Feature: 169, Score: 0.16443\n",
      "Feature: 170, Score: 0.64975\n",
      "Feature: 171, Score: 0.07976\n",
      "Feature: 172, Score: -0.26933\n",
      "Feature: 173, Score: 0.89960\n",
      "Feature: 174, Score: 0.95681\n",
      "Feature: 175, Score: -0.24266\n",
      "Feature: 176, Score: -1.30914\n",
      "Feature: 177, Score: -0.00002\n",
      "Feature: 178, Score: 0.00002\n",
      "Feature: 179, Score: -0.00006\n",
      "Feature: 180, Score: -0.20954\n",
      "Feature: 181, Score: 0.35377\n",
      "Feature: 182, Score: 0.05955\n",
      "Feature: 183, Score: 0.00340\n",
      "Feature: 184, Score: 0.03554\n",
      "Feature: 185, Score: -0.01694\n",
      "Feature: 186, Score: -0.04285\n",
      "Feature: 187, Score: -0.03382\n",
      "Feature: 188, Score: 0.00264\n",
      "Feature: 189, Score: -0.29766\n",
      "Feature: 190, Score: 0.00081\n",
      "Feature: 191, Score: 0.06991\n",
      "Feature: 192, Score: -0.00633\n",
      "Feature: 193, Score: 0.04922\n",
      "Feature: 194, Score: -0.01891\n",
      "Feature: 195, Score: -0.09016\n",
      "Feature: 196, Score: -0.09070\n",
      "Feature: 197, Score: -0.09068\n",
      "Feature: 198, Score: -0.10455\n",
      "Feature: 199, Score: 0.03154\n",
      "Feature: 200, Score: 0.16724\n",
      "Feature: 201, Score: 0.08634\n",
      "Feature: 202, Score: 0.05246\n",
      "Feature: 203, Score: 0.00683\n",
      "Feature: 204, Score: 0.01645\n",
      "Feature: 205, Score: 0.00090\n",
      "Feature: 206, Score: 0.08115\n",
      "Feature: 207, Score: 0.01578\n",
      "Feature: 208, Score: -0.06178\n",
      "Feature: 209, Score: 0.05369\n",
      "Feature: 210, Score: 0.06230\n",
      "Feature: 211, Score: 0.02229\n",
      "Feature: 212, Score: -0.05159\n",
      "Feature: 213, Score: 0.07903\n",
      "Feature: 214, Score: -0.05796\n",
      "Feature: 215, Score: 0.13242\n",
      "Feature: 216, Score: -0.04509\n",
      "Feature: 217, Score: 0.00111\n",
      "Feature: 218, Score: 0.22511\n",
      "Feature: 219, Score: -1570.70494\n",
      "Feature: 220, Score: -7.03633\n",
      "Feature: 221, Score: 6.87316\n",
      "Feature: 222, Score: -10381.93009\n",
      "Feature: 223, Score: 42.91793\n",
      "Feature: 224, Score: 5.12864\n",
      "Feature: 225, Score: 22.02198\n",
      "Feature: 226, Score: 34.31758\n",
      "Feature: 227, Score: 39.61400\n",
      "Feature: 228, Score: -25.61435\n",
      "Feature: 229, Score: -72.57557\n",
      "Feature: 230, Score: -54.43404\n",
      "Feature: 231, Score: -0.01113\n",
      "Feature: 232, Score: 0.01427\n",
      "Feature: 233, Score: 0.04266\n",
      "Feature: 234, Score: 8.27966\n",
      "Feature: 235, Score: 0.44705\n",
      "Feature: 236, Score: 5.53940\n",
      "Feature: 237, Score: 0.49538\n",
      "Feature: 238, Score: -0.25068\n",
      "Feature: 239, Score: 0.10281\n",
      "Feature: 240, Score: 1.74589\n",
      "Feature: 241, Score: 1.73131\n",
      "Feature: 242, Score: -0.06550\n",
      "Feature: 243, Score: -2.36340\n",
      "Feature: 244, Score: -1.38738\n",
      "Feature: 245, Score: -3.60596\n",
      "Feature: 246, Score: 1.28240\n",
      "Feature: 247, Score: 0.38815\n",
      "Feature: 248, Score: -1.88329\n",
      "Feature: 249, Score: 0.08908\n",
      "Feature: 250, Score: 1.48814\n",
      "Feature: 251, Score: -0.87303\n",
      "Feature: 252, Score: -0.16752\n",
      "Feature: 253, Score: 1.51511\n",
      "Feature: 254, Score: 0.01406\n",
      "Feature: 255, Score: 0.54555\n",
      "Feature: 256, Score: 1.77849\n",
      "Feature: 257, Score: -0.84754\n",
      "Feature: 258, Score: -2.50377\n",
      "Feature: 259, Score: 0.62214\n",
      "Feature: 260, Score: 3.24393\n",
      "Feature: 261, Score: 1.31360\n",
      "Feature: 262, Score: 2.79850\n",
      "Feature: 263, Score: -1.57868\n",
      "Feature: 264, Score: -0.25074\n",
      "Feature: 265, Score: 0.58118\n",
      "Feature: 266, Score: -2.53002\n",
      "Feature: 267, Score: -0.55727\n",
      "Feature: 268, Score: -0.01098\n",
      "Feature: 269, Score: -0.70156\n",
      "Feature: 270, Score: 7.85112\n",
      "Feature: 271, Score: -1.68382\n",
      "Feature: 272, Score: -15264.07554\n",
      "Feature: 273, Score: -5524.30580\n",
      "Feature: 274, Score: -2.45157\n",
      "Feature: 275, Score: -0.09713\n",
      "Feature: 276, Score: -2694.37559\n",
      "Feature: 277, Score: 0.01669\n",
      "Feature: 278, Score: 0.05775\n",
      "Feature: 279, Score: -0.18669\n",
      "Feature: 280, Score: -0.05165\n",
      "Feature: 281, Score: -0.01354\n",
      "Feature: 282, Score: -0.08869\n",
      "Feature: 283, Score: 0.01602\n",
      "Feature: 284, Score: 0.00674\n",
      "Feature: 285, Score: 0.00007\n",
      "Feature: 286, Score: 0.00089\n",
      "Feature: 287, Score: -0.00015\n",
      "Feature: 288, Score: 0.07722\n",
      "Feature: 289, Score: 0.19516\n",
      "Feature: 290, Score: -0.17155\n",
      "Feature: 291, Score: 0.00857\n",
      "Feature: 292, Score: 0.00651\n",
      "Feature: 293, Score: -0.00291\n",
      "Feature: 294, Score: -0.02311\n",
      "Feature: 295, Score: 0.04763\n",
      "Feature: 296, Score: -0.01801\n",
      "Feature: 297, Score: 0.03636\n",
      "Feature: 298, Score: 0.02856\n",
      "Feature: 299, Score: -0.02022\n",
      "Feature: 300, Score: -0.00616\n",
      "Feature: 301, Score: 0.00001\n",
      "Feature: 302, Score: -0.00079\n",
      "Feature: 303, Score: 0.01370\n",
      "Feature: 304, Score: -0.00555\n",
      "Feature: 305, Score: -0.01417\n",
      "Feature: 306, Score: 0.00200\n",
      "Feature: 307, Score: -0.02282\n",
      "Feature: 308, Score: 0.00545\n",
      "Feature: 309, Score: -0.00253\n",
      "Feature: 310, Score: 0.00294\n",
      "Feature: 311, Score: -0.06496\n",
      "Feature: 312, Score: -0.00263\n",
      "Feature: 313, Score: -0.02268\n",
      "Feature: 314, Score: -0.01607\n",
      "Feature: 315, Score: 0.02004\n",
      "Feature: 316, Score: -0.03854\n",
      "Feature: 317, Score: -0.02694\n",
      "Feature: 318, Score: 0.01499\n",
      "Feature: 319, Score: -0.10779\n",
      "Feature: 320, Score: -0.01813\n",
      "Feature: 321, Score: -0.00624\n",
      "Feature: 322, Score: -6681.52219\n",
      "Feature: 323, Score: -0.10888\n",
      "Feature: 324, Score: 0.05907\n",
      "Feature: 325, Score: 0.05836\n",
      "Feature: 326, Score: -0.16583\n",
      "Feature: 327, Score: 4145.37989\n",
      "Feature: 328, Score: 4.23934\n",
      "Feature: 329, Score: 0.04624\n",
      "Feature: 330, Score: -2930.23891\n",
      "Feature: 331, Score: -0.07223\n",
      "Feature: 332, Score: 0.04996\n",
      "Feature: 333, Score: 0.13653\n",
      "Feature: 334, Score: 0.07712\n",
      "Feature: 335, Score: 0.01896\n",
      "Feature: 336, Score: 0.07648\n",
      "Feature: 337, Score: 0.04194\n",
      "Feature: 338, Score: -0.11668\n",
      "Feature: 339, Score: -0.00002\n",
      "Feature: 340, Score: -0.00076\n",
      "Feature: 341, Score: -0.00028\n",
      "Feature: 342, Score: -0.11031\n",
      "Feature: 343, Score: -0.06402\n",
      "Feature: 344, Score: 0.20887\n",
      "Feature: 345, Score: -0.01090\n",
      "Feature: 346, Score: 0.01839\n",
      "Feature: 347, Score: -0.00266\n",
      "Feature: 348, Score: -0.02927\n",
      "Feature: 349, Score: -0.01280\n",
      "Feature: 350, Score: 0.03022\n",
      "Feature: 351, Score: 0.02123\n",
      "Feature: 352, Score: -0.02885\n",
      "Feature: 353, Score: -0.06252\n",
      "Feature: 354, Score: 0.01283\n",
      "Feature: 355, Score: -0.00495\n",
      "Feature: 356, Score: 0.00607\n",
      "Feature: 357, Score: 0.01059\n",
      "Feature: 358, Score: -0.00724\n",
      "Feature: 359, Score: -0.00190\n",
      "Feature: 360, Score: -0.00782\n",
      "Feature: 361, Score: 0.00490\n",
      "Feature: 362, Score: 0.07219\n",
      "Feature: 363, Score: 0.00001\n",
      "Feature: 364, Score: -0.00860\n",
      "Feature: 365, Score: -0.03480\n",
      "Feature: 366, Score: -0.00449\n",
      "Feature: 367, Score: 0.03425\n",
      "Feature: 368, Score: -0.01542\n",
      "Feature: 369, Score: -0.00714\n",
      "Feature: 370, Score: 0.05624\n",
      "Feature: 371, Score: 0.03396\n",
      "Feature: 372, Score: -0.12437\n",
      "Feature: 373, Score: 0.00994\n",
      "Feature: 374, Score: 0.00765\n",
      "Feature: 375, Score: 0.01632\n",
      "Feature: 376, Score: 0.00620\n",
      "Feature: 377, Score: -0.15255\n",
      "Feature: 378, Score: -0.00342\n",
      "Feature: 379, Score: -0.11510\n",
      "Feature: 380, Score: 1.44445\n",
      "Feature: 381, Score: -874.06854\n",
      "Feature: 382, Score: -37.08451\n",
      "Feature: 383, Score: -4.35912\n",
      "Feature: 384, Score: -1.11222\n",
      "Feature: 385, Score: 0.78138\n",
      "Feature: 386, Score: -0.35941\n",
      "Feature: 387, Score: -4.63885\n",
      "Feature: 388, Score: -1.33131\n",
      "Feature: 389, Score: -5.11643\n",
      "Feature: 390, Score: 1.09226\n",
      "Feature: 391, Score: 0.31628\n",
      "Feature: 392, Score: 7.37964\n",
      "Feature: 393, Score: 0.01436\n",
      "Feature: 394, Score: 0.05250\n",
      "Feature: 395, Score: -0.00414\n",
      "Feature: 396, Score: 0.13483\n",
      "Feature: 397, Score: 0.50613\n",
      "Feature: 398, Score: -1.68099\n",
      "Feature: 399, Score: 0.79440\n",
      "Feature: 400, Score: -0.01197\n",
      "Feature: 401, Score: 0.01099\n",
      "Feature: 402, Score: 0.61404\n",
      "Feature: 403, Score: 0.43487\n",
      "Feature: 404, Score: -0.36271\n",
      "Feature: 405, Score: -0.02500\n",
      "Feature: 406, Score: 0.29470\n",
      "Feature: 407, Score: 0.53777\n",
      "Feature: 408, Score: -0.07711\n",
      "Feature: 409, Score: 0.37119\n",
      "Feature: 410, Score: 0.06356\n",
      "Feature: 411, Score: 0.73476\n",
      "Feature: 412, Score: -0.74459\n",
      "Feature: 413, Score: -0.38572\n",
      "Feature: 414, Score: 0.32840\n",
      "Feature: 415, Score: 0.16590\n",
      "Feature: 416, Score: 1.96367\n",
      "Feature: 417, Score: 1.64738\n",
      "Feature: 418, Score: -1.19753\n",
      "Feature: 419, Score: 0.46169\n",
      "Feature: 420, Score: -0.33216\n",
      "Feature: 421, Score: -0.18315\n",
      "Feature: 422, Score: 0.92884\n",
      "Feature: 423, Score: 1.49327\n",
      "Feature: 424, Score: 0.33631\n",
      "Feature: 425, Score: -0.11324\n",
      "Feature: 426, Score: 0.66137\n",
      "Feature: 427, Score: 0.67885\n",
      "Feature: 428, Score: -0.28658\n",
      "Feature: 429, Score: -1.18626\n",
      "Feature: 430, Score: 0.61920\n",
      "Feature: 431, Score: -0.48412\n",
      "Feature: 432, Score: -0.05033\n",
      "Feature: 433, Score: 0.15017\n",
      "Feature: 434, Score: 5.26992\n",
      "Feature: 435, Score: 367.64482\n",
      "Feature: 436, Score: -53.96253\n",
      "Feature: 437, Score: -76.63634\n",
      "Feature: 438, Score: 738.35894\n",
      "Feature: 439, Score: 121.01813\n",
      "Feature: 440, Score: -27.66463\n",
      "Feature: 441, Score: 140.42196\n",
      "Feature: 442, Score: 111.87576\n",
      "Feature: 443, Score: 43.77193\n",
      "Feature: 444, Score: -74.75402\n",
      "Feature: 445, Score: -216.95324\n",
      "Feature: 446, Score: -0.07648\n",
      "Feature: 447, Score: 0.02651\n",
      "Feature: 448, Score: -0.21938\n",
      "Feature: 449, Score: -0.20531\n",
      "Feature: 450, Score: 2.05766\n",
      "Feature: 451, Score: -6.93948\n",
      "Feature: 452, Score: 34.39900\n",
      "Feature: 453, Score: -1.64525\n",
      "Feature: 454, Score: 0.89118\n",
      "Feature: 455, Score: -5.97321\n",
      "Feature: 456, Score: 7.85556\n",
      "Feature: 457, Score: 7.21506\n",
      "Feature: 458, Score: 0.74845\n",
      "Feature: 459, Score: 8.66144\n",
      "Feature: 460, Score: -0.61991\n",
      "Feature: 461, Score: -7.26228\n",
      "Feature: 462, Score: 0.12263\n",
      "Feature: 463, Score: 0.85814\n",
      "Feature: 464, Score: -0.56243\n",
      "Feature: 465, Score: 0.92110\n",
      "Feature: 466, Score: -4.50180\n",
      "Feature: 467, Score: -0.46305\n",
      "Feature: 468, Score: 1.36962\n",
      "Feature: 469, Score: 0.45787\n",
      "Feature: 470, Score: 2.27381\n",
      "Feature: 471, Score: 1.32170\n",
      "Feature: 472, Score: 2.98942\n",
      "Feature: 473, Score: -0.05033\n",
      "Feature: 474, Score: -3.36315\n",
      "Feature: 475, Score: -0.00685\n",
      "Feature: 476, Score: -3.24620\n",
      "Feature: 477, Score: -2.76221\n",
      "Feature: 478, Score: 3.37687\n",
      "Feature: 479, Score: 2.17414\n",
      "Feature: 480, Score: 1.49720\n",
      "Feature: 481, Score: -0.86676\n",
      "Feature: 482, Score: 0.48745\n",
      "Feature: 483, Score: 0.29584\n",
      "Feature: 484, Score: -0.09066\n",
      "Feature: 485, Score: 1.93512\n",
      "Feature: 486, Score: 3.29867\n",
      "Feature: 487, Score: -3.95701\n",
      "Feature: 488, Score: -0.12106\n",
      "Feature: 489, Score: 99.09474\n",
      "Feature: 490, Score: 3.05744\n",
      "Feature: 491, Score: 0.09641\n",
      "Feature: 492, Score: 10.32347\n",
      "Feature: 493, Score: -0.03324\n",
      "Feature: 494, Score: 0.03558\n",
      "Feature: 495, Score: 0.06500\n",
      "Feature: 496, Score: -0.08170\n",
      "Feature: 497, Score: 0.01323\n",
      "Feature: 498, Score: 0.04567\n",
      "Feature: 499, Score: 0.10850\n",
      "Feature: 500, Score: -0.13781\n",
      "Feature: 501, Score: -0.00012\n",
      "Feature: 502, Score: -0.00057\n",
      "Feature: 503, Score: 0.00019\n",
      "Feature: 504, Score: -0.00373\n",
      "Feature: 505, Score: 0.05652\n",
      "Feature: 506, Score: 0.10420\n",
      "Feature: 507, Score: -0.04644\n",
      "Feature: 508, Score: 0.01429\n",
      "Feature: 509, Score: 0.00327\n",
      "Feature: 510, Score: 0.00790\n",
      "Feature: 511, Score: -0.04523\n",
      "Feature: 512, Score: 0.02673\n",
      "Feature: 513, Score: 0.02935\n",
      "Feature: 514, Score: -0.02137\n",
      "Feature: 515, Score: 0.00335\n",
      "Feature: 516, Score: -0.00671\n",
      "Feature: 517, Score: 0.00196\n",
      "Feature: 518, Score: -0.00162\n",
      "Feature: 519, Score: -0.01062\n",
      "Feature: 520, Score: 0.01642\n",
      "Feature: 521, Score: 0.01204\n",
      "Feature: 522, Score: 0.01598\n",
      "Feature: 523, Score: 0.00333\n",
      "Feature: 524, Score: -0.02269\n",
      "Feature: 525, Score: -0.02292\n",
      "Feature: 526, Score: 0.00793\n",
      "Feature: 527, Score: -0.00606\n",
      "Feature: 528, Score: -0.00600\n",
      "Feature: 529, Score: -0.00497\n",
      "Feature: 530, Score: -0.01300\n",
      "Feature: 531, Score: -0.01370\n",
      "Feature: 532, Score: 0.00184\n",
      "Feature: 533, Score: -0.00283\n",
      "Feature: 534, Score: -0.00328\n",
      "Feature: 535, Score: 0.00112\n",
      "Feature: 536, Score: 0.00398\n",
      "Feature: 537, Score: 0.00744\n",
      "Feature: 538, Score: -0.00207\n",
      "Feature: 539, Score: 0.00090\n",
      "Feature: 540, Score: -0.00542\n",
      "Feature: 541, Score: 0.00186\n",
      "Feature: 542, Score: 0.02873\n",
      "Feature: 543, Score: -10.42128\n",
      "Feature: 544, Score: -0.61272\n",
      "Feature: 545, Score: -0.03836\n",
      "Feature: 546, Score: -19.04149\n",
      "Feature: 547, Score: 0.00361\n",
      "Feature: 548, Score: -0.03028\n",
      "Feature: 549, Score: -0.08638\n",
      "Feature: 550, Score: -0.03882\n",
      "Feature: 551, Score: 0.07323\n",
      "Feature: 552, Score: -0.05852\n",
      "Feature: 553, Score: 0.04769\n",
      "Feature: 554, Score: -0.02519\n",
      "Feature: 555, Score: 0.00007\n",
      "Feature: 556, Score: 0.00011\n",
      "Feature: 557, Score: 0.00031\n",
      "Feature: 558, Score: 0.05250\n",
      "Feature: 559, Score: 0.06206\n",
      "Feature: 560, Score: -0.07888\n",
      "Feature: 561, Score: 0.00770\n",
      "Feature: 562, Score: -0.02067\n",
      "Feature: 563, Score: 0.02364\n",
      "Feature: 564, Score: 0.02872\n",
      "Feature: 565, Score: -0.01519\n",
      "Feature: 566, Score: -0.01196\n",
      "Feature: 567, Score: 0.03149\n",
      "Feature: 568, Score: 0.02568\n",
      "Feature: 569, Score: 0.02810\n",
      "Feature: 570, Score: -0.00083\n",
      "Feature: 571, Score: -0.01381\n",
      "Feature: 572, Score: 0.01302\n",
      "Feature: 573, Score: 0.01877\n",
      "Feature: 574, Score: -0.02527\n",
      "Feature: 575, Score: -0.00460\n",
      "Feature: 576, Score: -0.02832\n",
      "Feature: 577, Score: 0.00628\n",
      "Feature: 578, Score: 0.01135\n",
      "Feature: 579, Score: 0.01816\n",
      "Feature: 580, Score: -0.00059\n",
      "Feature: 581, Score: -0.00229\n",
      "Feature: 582, Score: -0.00533\n",
      "Feature: 583, Score: 0.00104\n",
      "Feature: 584, Score: 0.00557\n",
      "Feature: 585, Score: -0.00217\n",
      "Feature: 586, Score: -0.00054\n",
      "Feature: 587, Score: 0.00311\n",
      "Feature: 588, Score: 0.01000\n",
      "Feature: 589, Score: -0.00150\n",
      "Feature: 590, Score: -0.00773\n",
      "Feature: 591, Score: -0.00708\n",
      "Feature: 592, Score: -0.00470\n",
      "Feature: 593, Score: -0.00887\n",
      "Feature: 594, Score: -0.03830\n",
      "Feature: 595, Score: 0.01353\n",
      "Feature: 596, Score: -0.90096\n",
      "Feature: 597, Score: 0.69450\n",
      "Feature: 598, Score: 21.37127\n",
      "Feature: 599, Score: 3.01708\n",
      "Feature: 600, Score: 0.21431\n",
      "Feature: 601, Score: 0.49495\n",
      "Feature: 602, Score: 0.76137\n",
      "Feature: 603, Score: 1.58868\n",
      "Feature: 604, Score: 0.45944\n",
      "Feature: 605, Score: 2.39455\n",
      "Feature: 606, Score: 0.54173\n",
      "Feature: 607, Score: -2.00823\n",
      "Feature: 608, Score: -4.61438\n",
      "Feature: 609, Score: -0.00995\n",
      "Feature: 610, Score: -0.02709\n",
      "Feature: 611, Score: 0.00589\n",
      "Feature: 612, Score: 0.35575\n",
      "Feature: 613, Score: 1.46494\n",
      "Feature: 614, Score: 0.54472\n",
      "Feature: 615, Score: -0.87948\n",
      "Feature: 616, Score: 0.03097\n",
      "Feature: 617, Score: -0.22077\n",
      "Feature: 618, Score: -1.11976\n",
      "Feature: 619, Score: -0.21937\n",
      "Feature: 620, Score: 0.32609\n",
      "Feature: 621, Score: -0.19237\n",
      "Feature: 622, Score: -0.68814\n",
      "Feature: 623, Score: -0.46648\n",
      "Feature: 624, Score: -0.04056\n",
      "Feature: 625, Score: -0.01206\n",
      "Feature: 626, Score: 0.05839\n",
      "Feature: 627, Score: -0.93422\n",
      "Feature: 628, Score: 1.17942\n",
      "Feature: 629, Score: 0.59408\n",
      "Feature: 630, Score: 0.23234\n",
      "Feature: 631, Score: 0.04279\n",
      "Feature: 632, Score: -1.45275\n",
      "Feature: 633, Score: -1.32421\n",
      "Feature: 634, Score: 0.71286\n",
      "Feature: 635, Score: -0.32774\n",
      "Feature: 636, Score: 0.22224\n",
      "Feature: 637, Score: 0.13657\n",
      "Feature: 638, Score: -0.71446\n",
      "Feature: 639, Score: -0.96091\n",
      "Feature: 640, Score: -0.19784\n",
      "Feature: 641, Score: 0.08063\n",
      "Feature: 642, Score: -0.49510\n",
      "Feature: 643, Score: -0.35222\n",
      "Feature: 644, Score: 0.27155\n",
      "Feature: 645, Score: 0.72410\n",
      "Feature: 646, Score: -0.22694\n",
      "Feature: 647, Score: 0.34513\n",
      "Feature: 648, Score: 0.14532\n",
      "Feature: 649, Score: -0.10672\n",
      "Feature: 650, Score: -0.00563\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "for i,j in enumerate(importance):\n",
    "    #if j > 10 or j < -10:\n",
    "    print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3031105910.659565\n",
      "101063.16184368526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0056343674659729"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())\n",
    "importance[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 50 components, MSE Model 4 = 0.0381\n",
      "MLR with 50 components, MAE Model 4 = 0.11\n",
      "MLR with 50 components, 1 - MAE Model 4 = 0.89\n",
      "-------------------------------------------\n",
      "MLR with 100 components, MSE Model 4 = 0.0574\n",
      "MLR with 100 components, MAE Model 4 = 0.107\n",
      "MLR with 100 components, 1 - MAE Model 4 = 0.893\n",
      "-------------------------------------------\n",
      "MLR with 200 components, MSE Model 4 = 0.0466\n",
      "MLR with 200 components, MAE Model 4 = 0.105\n",
      "MLR with 200 components, 1 - MAE Model 4 = 0.895\n",
      "-------------------------------------------\n",
      "MLR with 300 components, MSE Model 4 = 0.024\n",
      "MLR with 300 components, MAE Model 4 = 0.102\n",
      "MLR with 300 components, 1 - MAE Model 4 = 0.898\n",
      "-------------------------------------------\n",
      "MLR with 400 components, MSE Model 4 = 0.041\n",
      "MLR with 400 components, MAE Model 4 = 0.104\n",
      "MLR with 400 components, 1 - MAE Model 4 = 0.896\n",
      "-------------------------------------------\n",
      "MLR with 500 components, MSE Model 4 = 0.0312\n",
      "MLR with 500 components, MAE Model 4 = 0.102\n",
      "MLR with 500 components, 1 - MAE Model 4 = 0.898\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_mlr_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    X_4_test_mlr_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_4_train_mlr_PCA, y_4_train)\n",
    "    mlr_y_4_PCA = regressor.predict(X_4_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 4 = {:.3}\".format(mean_squared_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 4 = {:.3}\".format(mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 4 = {:.3}\".format(1-mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0159\n",
      "MAE 0.101\n",
      "1 - MAE 0.899\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_4_train_dummy, y_4_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_4 = rfr.predict(X_4_test_dummy)\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_4_test, rfr_y_4)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_4_test, rfr_y_4)))\n",
    "print(\"1 - MAE {:.3}\".format((1-mean_absolute_error(y_4_test, rfr_y_4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 50 components, MSE Model 4 = 0.0187\n",
      "RFR with 50 components, MAE Model 4 = 0.109\n",
      "RFR with 50 components, 1 - MAE Model 4 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 100 components, MSE Model 4 = 0.0187\n",
      "RFR with 100 components, MAE Model 4 = 0.109\n",
      "RFR with 100 components, 1 - MAE Model 4 = 0.891\n",
      "-------------------------------------------\n",
      "RFR with 200 components, MSE Model 4 = 0.0184\n",
      "RFR with 200 components, MAE Model 4 = 0.108\n",
      "RFR with 200 components, 1 - MAE Model 4 = 0.892\n",
      "-------------------------------------------\n",
      "RFR with 300 components, MSE Model 4 = 0.0186\n",
      "RFR with 300 components, MAE Model 4 = 0.11\n",
      "RFR with 300 components, 1 - MAE Model 4 = 0.89\n",
      "-------------------------------------------\n",
      "RFR with 400 components, MSE Model 4 = 0.0189\n",
      "RFR with 400 components, MAE Model 4 = 0.11\n",
      "RFR with 400 components, 1 - MAE Model 4 = 0.89\n",
      "-------------------------------------------\n",
      "RFR with 500 components, MSE Model 4 = 0.0186\n",
      "RFR with 500 components, MAE Model 4 = 0.108\n",
      "RFR with 500 components, 1 - MAE Model 4 = 0.892\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_4_train_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    rfr_X_4_test_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    rfr.fit(rfr_X_4_train_PCA, y_4_train.ravel())\n",
    "    rfr_y_4_PCA = rfr.predict(rfr_X_4_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 4 = {:.3}\".format(mean_squared_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 4 = {:.3}\".format(mean_absolute_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 4 = {:.3}\".format(1-mean_absolute_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the gender and ethnicity dummies\n",
    "X_4_train_scale = X_4_train_dummy.copy()\n",
    "X_4_test_scale = X_4_test_dummy.copy()\n",
    "\n",
    "X_4_train_scale[:, 2:-1] = X_sc.fit_transform(X_4_train_dummy[:, 2:-1])\n",
    "X_4_test_scale[:, 2:-1] = X_sc.transform(X_4_test_dummy[:, 2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0148\n",
      "MAE 0.097\n",
      "MAE 0.903\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_4_train_scale, y_4_train.ravel())\n",
    "svr_y_4 = svr.predict(X_4_test_scale)\n",
    "\n",
    "print(\"MSE {:.3}\".format(mean_squared_error(y_4_test, svr_y_4)))\n",
    "print(\"MAE {:.3}\".format(mean_absolute_error(y_4_test, svr_y_4)))\n",
    "print(\"MAE {:.3}\".format((1-mean_absolute_error(y_4_test, svr_y_4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 50 components, MSE Model 4 = 0.0158\n",
      "SVR with 50 components, MAE Model 4 = 0.1\n",
      "SVR with 50 components, 1 - MAE Model 4 = 0.9\n",
      "-------------------------------------------\n",
      "SVR with 100 components, MSE Model 4 = 0.0153\n",
      "SVR with 100 components, MAE Model 4 = 0.0984\n",
      "SVR with 100 components, 1 - MAE Model 4 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 200 components, MSE Model 4 = 0.0152\n",
      "SVR with 200 components, MAE Model 4 = 0.0981\n",
      "SVR with 200 components, 1 - MAE Model 4 = 0.902\n",
      "-------------------------------------------\n",
      "SVR with 300 components, MSE Model 4 = 0.015\n",
      "SVR with 300 components, MAE Model 4 = 0.0974\n",
      "SVR with 300 components, 1 - MAE Model 4 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 400 components, MSE Model 4 = 0.0149\n",
      "SVR with 400 components, MAE Model 4 = 0.0972\n",
      "SVR with 400 components, 1 - MAE Model 4 = 0.903\n",
      "-------------------------------------------\n",
      "SVR with 500 components, MSE Model 4 = 0.0148\n",
      "SVR with 500 components, MAE Model 4 = 0.097\n",
      "SVR with 500 components, 1 - MAE Model 4 = 0.903\n",
      "-------------------------------------------\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [50, 100, 200, 300, 400, 500]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_PCA = pca.fit_transform(X_4_train_scale)\n",
    "    X_4_test_PCA = pca.transform(X_4_test_scale)\n",
    "    \n",
    "    svr.fit(X_4_train_PCA, y_4_train.ravel())\n",
    "    svr_y_4_PCA = svr.predict(X_4_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 4 = {:.3}\".format(mean_squared_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 4 = {:.3}\".format(mean_absolute_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 4 = {:.3}\".format(1-mean_absolute_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-tests between the models that have slight differences in MAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99710887]\n",
      "[0.89070229]\n"
     ]
    }
   ],
   "source": [
    "# Related t-test MLR\n",
    "print(scipy.stats.ttest_rel(y_1_pred, y_3_pred).pvalue)       # models 1 and 3\n",
    "print(scipy.stats.ttest_rel(y_1_pred, y_4_pred).pvalue)       # models 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42503305881643527\n",
      "0.3783730077594538\n"
     ]
    }
   ],
   "source": [
    "# Related t-test SVR\n",
    "print(scipy.stats.ttest_rel(svr_y_1, svr_y_3).pvalue)         # models 1 and 3\n",
    "print(scipy.stats.ttest_rel(svr_y_1, svr_y_4).pvalue)         # models 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20509937797166497\n",
      "0.11305714088544984\n",
      "0.44509266952623605\n"
     ]
    }
   ],
   "source": [
    "# Related t-test RFR\n",
    "print(scipy.stats.ttest_rel(rfr_y_1, rfr_y_2).pvalue)         # models 1 and 2\n",
    "print(scipy.stats.ttest_rel(rfr_y_1, rfr_y_3).pvalue)         # models 1 and 3\n",
    "print(scipy.stats.ttest_rel(rfr_y_1, rfr_y_4).pvalue)         # models 1 and 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
