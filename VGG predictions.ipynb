{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG predictions\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries to load the data\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = # Please provide own path\n",
    "# os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile_annot_training = open(path+\"\\\\\"+\"annotation_training.pkl\", \"rb\")\n",
    "annotation_training = pickle.load(infile_annot_training, encoding = \"latin1\")\n",
    "\n",
    "infile_annot_validation = open(path+\"\\\\\"+\"annotation_validation.pkl\", \"rb\")\n",
    "annotation_validation = pickle.load(infile_annot_validation, encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the ethnicity and gender data\n",
    "# and preparing it to merge with other data frames\n",
    "excel_data = pd.read_excel(r\"C:\\Users\\Marie-Claire\\Downloads\\eth_gender_anno_all.xlsx\")\n",
    "excel_to_merge = excel_data.drop(['YouTubeID'], axis = 1)\n",
    "excel_to_merge.columns = excel_to_merge.columns.str.lower()\n",
    "excel_to_merge.rename(columns = {\"videoname\" : \"filenames\", \n",
    "                         }, inplace = True)\n",
    "#excel_to_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG predictions\n",
    "#### Loading the VGG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 20480)\n",
      "(8000, 1)\n"
     ]
    }
   ],
   "source": [
    "data_dir = # Please provide own path\n",
    "\n",
    "lgbptop_file_path = os.path.join(data_dir, 'vggfer33fun.mat')\n",
    "feats = loadmat(lgbptop_file_path)\n",
    "\n",
    "mdtype = feats['vggfer33fun'].dtype\n",
    "\n",
    "vgg_ndata = {n: feats['vggfer33fun'][n][0, 0] for n in mdtype.names}\n",
    "\n",
    "filenames = [filename[0] for filename in vgg_ndata['filename'].squeeze()]\n",
    "\n",
    "data_vgg = {'filenames': filenames, 'features': vgg_ndata['data']}\n",
    "\n",
    "print(vgg_ndata['data'].shape)\n",
    "print(vgg_ndata['filename'].shape)\n",
    "\n",
    "#pickle.dump(data, open(os.path.join(data_dir, 'lgbptop.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training data by converting the dictionaries to dataframes\n",
    "# This way the data can be merged later on\n",
    "\n",
    "# Creating a copy just in case\n",
    "data_3 = data_vgg.copy()\n",
    "\n",
    "# Creating a data frame of the features data\n",
    "                         #test_array = np.array(data_1['features'])\n",
    "vgg_features_df = pd.DataFrame(data_3['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy just in case\n",
    "data_4 = data_vgg.copy()\n",
    "\n",
    "# Creating a data frame of the filenames data\n",
    "vgg_filenames_df = pd.DataFrame(data_4['filenames'])       # np.array() weggehaald\n",
    "vgg_filenames_df = vgg_filenames_df.rename(columns= {0 : \"filenames\"})\n",
    "#vgg_filenames_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20470</th>\n",
       "      <th>20471</th>\n",
       "      <th>20472</th>\n",
       "      <th>20473</th>\n",
       "      <th>20474</th>\n",
       "      <th>20475</th>\n",
       "      <th>20476</th>\n",
       "      <th>20477</th>\n",
       "      <th>20478</th>\n",
       "      <th>20479</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--Ymqszjv54.001.mp4</td>\n",
       "      <td>-0.055683</td>\n",
       "      <td>-0.236555</td>\n",
       "      <td>-19.559164</td>\n",
       "      <td>-0.367740</td>\n",
       "      <td>-0.235484</td>\n",
       "      <td>-8.412696</td>\n",
       "      <td>-0.449136</td>\n",
       "      <td>3.356551</td>\n",
       "      <td>-5.745764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.268699</td>\n",
       "      <td>-8.778057</td>\n",
       "      <td>-0.267969</td>\n",
       "      <td>-0.335641</td>\n",
       "      <td>-19.149986</td>\n",
       "      <td>0.558402</td>\n",
       "      <td>-7.428394</td>\n",
       "      <td>-3.032744</td>\n",
       "      <td>-18.559444</td>\n",
       "      <td>-0.137366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--Ymqszjv54.003.mp4</td>\n",
       "      <td>-0.166791</td>\n",
       "      <td>-0.243937</td>\n",
       "      <td>-16.418880</td>\n",
       "      <td>-0.372782</td>\n",
       "      <td>-0.237834</td>\n",
       "      <td>-6.789306</td>\n",
       "      <td>-0.431950</td>\n",
       "      <td>4.065738</td>\n",
       "      <td>-2.327925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250965</td>\n",
       "      <td>-3.500271</td>\n",
       "      <td>-0.261685</td>\n",
       "      <td>-0.298480</td>\n",
       "      <td>-18.654526</td>\n",
       "      <td>3.006815</td>\n",
       "      <td>-7.294409</td>\n",
       "      <td>-2.013493</td>\n",
       "      <td>-15.928368</td>\n",
       "      <td>-0.127522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--Ymqszjv54.004.mp4</td>\n",
       "      <td>-0.095448</td>\n",
       "      <td>-0.242685</td>\n",
       "      <td>-16.341707</td>\n",
       "      <td>-0.371635</td>\n",
       "      <td>-0.249516</td>\n",
       "      <td>-12.727262</td>\n",
       "      <td>-0.463436</td>\n",
       "      <td>4.256013</td>\n",
       "      <td>-3.142007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223702</td>\n",
       "      <td>0.982952</td>\n",
       "      <td>-0.277151</td>\n",
       "      <td>-0.332205</td>\n",
       "      <td>-14.339534</td>\n",
       "      <td>9.292833</td>\n",
       "      <td>-9.241794</td>\n",
       "      <td>1.561962</td>\n",
       "      <td>-13.497578</td>\n",
       "      <td>-0.114644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--Ymqszjv54.005.mp4</td>\n",
       "      <td>-0.344704</td>\n",
       "      <td>-0.252611</td>\n",
       "      <td>-16.521524</td>\n",
       "      <td>-0.392809</td>\n",
       "      <td>-0.269870</td>\n",
       "      <td>-14.835567</td>\n",
       "      <td>-0.472394</td>\n",
       "      <td>4.873293</td>\n",
       "      <td>-3.527074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232808</td>\n",
       "      <td>0.702202</td>\n",
       "      <td>-0.284889</td>\n",
       "      <td>-0.318367</td>\n",
       "      <td>-14.908902</td>\n",
       "      <td>3.359067</td>\n",
       "      <td>-7.684659</td>\n",
       "      <td>1.701056</td>\n",
       "      <td>-11.071396</td>\n",
       "      <td>-0.111634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2qsCrkXdWs.001.mp4</td>\n",
       "      <td>0.470167</td>\n",
       "      <td>-0.277294</td>\n",
       "      <td>2.179229</td>\n",
       "      <td>-0.303704</td>\n",
       "      <td>-0.193008</td>\n",
       "      <td>-2.502256</td>\n",
       "      <td>-0.419951</td>\n",
       "      <td>-1.081634</td>\n",
       "      <td>-5.319240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.206349</td>\n",
       "      <td>3.991546</td>\n",
       "      <td>-0.161194</td>\n",
       "      <td>-0.221222</td>\n",
       "      <td>2.620975</td>\n",
       "      <td>3.629155</td>\n",
       "      <td>-7.599644</td>\n",
       "      <td>-17.188986</td>\n",
       "      <td>-0.243650</td>\n",
       "      <td>-0.116243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20481 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             filenames         0         1          2         3         4  \\\n",
       "0  --Ymqszjv54.001.mp4 -0.055683 -0.236555 -19.559164 -0.367740 -0.235484   \n",
       "1  --Ymqszjv54.003.mp4 -0.166791 -0.243937 -16.418880 -0.372782 -0.237834   \n",
       "2  --Ymqszjv54.004.mp4 -0.095448 -0.242685 -16.341707 -0.371635 -0.249516   \n",
       "3  --Ymqszjv54.005.mp4 -0.344704 -0.252611 -16.521524 -0.392809 -0.269870   \n",
       "4  -2qsCrkXdWs.001.mp4  0.470167 -0.277294   2.179229 -0.303704 -0.193008   \n",
       "\n",
       "           5         6         7         8  ...     20470     20471     20472  \\\n",
       "0  -8.412696 -0.449136  3.356551 -5.745764  ... -0.268699 -8.778057 -0.267969   \n",
       "1  -6.789306 -0.431950  4.065738 -2.327925  ... -0.250965 -3.500271 -0.261685   \n",
       "2 -12.727262 -0.463436  4.256013 -3.142007  ... -0.223702  0.982952 -0.277151   \n",
       "3 -14.835567 -0.472394  4.873293 -3.527074  ... -0.232808  0.702202 -0.284889   \n",
       "4  -2.502256 -0.419951 -1.081634 -5.319240  ... -0.206349  3.991546 -0.161194   \n",
       "\n",
       "      20473      20474     20475     20476      20477      20478     20479  \n",
       "0 -0.335641 -19.149986  0.558402 -7.428394  -3.032744 -18.559444 -0.137366  \n",
       "1 -0.298480 -18.654526  3.006815 -7.294409  -2.013493 -15.928368 -0.127522  \n",
       "2 -0.332205 -14.339534  9.292833 -9.241794   1.561962 -13.497578 -0.114644  \n",
       "3 -0.318367 -14.908902  3.359067 -7.684659   1.701056 -11.071396 -0.111634  \n",
       "4 -0.221222   2.620975  3.629155 -7.599644 -17.188986  -0.243650 -0.116243  \n",
       "\n",
       "[5 rows x 20481 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_containing_vgg = pd.concat([vgg_filenames_df, vgg_features_df], axis = 1)\n",
    "df_containing_vgg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Ymqszjv54.001.mp4\n",
      "[ -0.05568334  -0.2365552  -19.559164   ...  -3.0327437  -18.559444\n",
      "  -0.13736597]\n",
      "--Ymqszjv54.003.mp4\n",
      "[ -0.1667905   -0.2439367  -16.41888    ...  -2.0134928  -15.928368\n",
      "  -0.12752174]\n"
     ]
    }
   ],
   "source": [
    "# Checking if the sequence is in correct order\n",
    "print(data_vgg['filenames'][0])\n",
    "print(data_vgg['features'][0])\n",
    "\n",
    "print(data_vgg['filenames'][1])\n",
    "print(data_vgg['features'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "vgg_df_interview = pd.DataFrame(annotation_training[\"interview\"],  index=[0]).T\n",
    "vgg_df_interview.reset_index(inplace = True)\n",
    "vgg_df_interview.rename(columns = {\"index\" : \"filenames\", 0 : \"interview\"}, inplace = True)\n",
    "print(vgg_df_interview.shape)\n",
    "#vgg_df_interview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 20482)\n"
     ]
    }
   ],
   "source": [
    "vgg_1_train = pd.merge(vgg_df_interview, df_containing_vgg, on = [\"filenames\"], how = \"left\")\n",
    "print(vgg_1_train.shape)\n",
    "\n",
    "# Checking for missing data & outliers in the training set\n",
    "for column in vgg_1_train.columns:\n",
    "    if vgg_1_train[column].isnull().any():\n",
    "        print(column)                           # No output means no missing data in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20470</th>\n",
       "      <th>20471</th>\n",
       "      <th>20472</th>\n",
       "      <th>20473</th>\n",
       "      <th>20474</th>\n",
       "      <th>20475</th>\n",
       "      <th>20476</th>\n",
       "      <th>20477</th>\n",
       "      <th>20478</th>\n",
       "      <th>20479</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503780</td>\n",
       "      <td>0.270279</td>\n",
       "      <td>-0.285081</td>\n",
       "      <td>-11.687315</td>\n",
       "      <td>-0.325762</td>\n",
       "      <td>-0.207463</td>\n",
       "      <td>-8.980567</td>\n",
       "      <td>-0.445502</td>\n",
       "      <td>5.119049</td>\n",
       "      <td>-6.217986</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234988</td>\n",
       "      <td>-7.431848</td>\n",
       "      <td>-0.196004</td>\n",
       "      <td>-0.308717</td>\n",
       "      <td>-8.130709</td>\n",
       "      <td>-1.737559</td>\n",
       "      <td>-4.992703</td>\n",
       "      <td>-5.760758</td>\n",
       "      <td>-7.486003</td>\n",
       "      <td>-0.125419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.150148</td>\n",
       "      <td>0.624930</td>\n",
       "      <td>0.065183</td>\n",
       "      <td>7.498257</td>\n",
       "      <td>0.059253</td>\n",
       "      <td>0.042329</td>\n",
       "      <td>7.412726</td>\n",
       "      <td>0.094788</td>\n",
       "      <td>8.331264</td>\n",
       "      <td>7.760859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>7.239368</td>\n",
       "      <td>0.042703</td>\n",
       "      <td>0.071241</td>\n",
       "      <td>7.204863</td>\n",
       "      <td>8.712741</td>\n",
       "      <td>7.813078</td>\n",
       "      <td>6.188989</td>\n",
       "      <td>6.417091</td>\n",
       "      <td>0.033693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.843723</td>\n",
       "      <td>-0.542196</td>\n",
       "      <td>-44.137951</td>\n",
       "      <td>-0.573281</td>\n",
       "      <td>-0.417524</td>\n",
       "      <td>-37.007336</td>\n",
       "      <td>-0.920841</td>\n",
       "      <td>-38.122360</td>\n",
       "      <td>-50.331089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.507402</td>\n",
       "      <td>-39.297752</td>\n",
       "      <td>-0.389587</td>\n",
       "      <td>-0.629192</td>\n",
       "      <td>-34.041065</td>\n",
       "      <td>-33.186592</td>\n",
       "      <td>-33.021378</td>\n",
       "      <td>-30.586414</td>\n",
       "      <td>-32.924835</td>\n",
       "      <td>-0.279788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.401869</td>\n",
       "      <td>-0.157689</td>\n",
       "      <td>-0.326142</td>\n",
       "      <td>-16.231661</td>\n",
       "      <td>-0.364254</td>\n",
       "      <td>-0.234987</td>\n",
       "      <td>-13.602871</td>\n",
       "      <td>-0.504585</td>\n",
       "      <td>-0.199337</td>\n",
       "      <td>-11.053427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.270404</td>\n",
       "      <td>-11.921796</td>\n",
       "      <td>-0.222966</td>\n",
       "      <td>-0.356357</td>\n",
       "      <td>-12.747404</td>\n",
       "      <td>-7.484886</td>\n",
       "      <td>-10.314631</td>\n",
       "      <td>-9.545395</td>\n",
       "      <td>-11.566561</td>\n",
       "      <td>-0.146197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.253475</td>\n",
       "      <td>-0.281383</td>\n",
       "      <td>-11.121806</td>\n",
       "      <td>-0.323276</td>\n",
       "      <td>-0.206054</td>\n",
       "      <td>-8.674579</td>\n",
       "      <td>-0.441152</td>\n",
       "      <td>5.125813</td>\n",
       "      <td>-5.575384</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.230486</td>\n",
       "      <td>-7.218858</td>\n",
       "      <td>-0.193964</td>\n",
       "      <td>-0.307008</td>\n",
       "      <td>-8.057387</td>\n",
       "      <td>-1.736244</td>\n",
       "      <td>-4.799426</td>\n",
       "      <td>-5.530936</td>\n",
       "      <td>-7.419476</td>\n",
       "      <td>-0.123316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.609813</td>\n",
       "      <td>0.689761</td>\n",
       "      <td>-0.239574</td>\n",
       "      <td>-6.617743</td>\n",
       "      <td>-0.286216</td>\n",
       "      <td>-0.177989</td>\n",
       "      <td>-3.944145</td>\n",
       "      <td>-0.379699</td>\n",
       "      <td>10.391348</td>\n",
       "      <td>-0.939446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195276</td>\n",
       "      <td>-2.571614</td>\n",
       "      <td>-0.166588</td>\n",
       "      <td>-0.258341</td>\n",
       "      <td>-3.661532</td>\n",
       "      <td>3.860130</td>\n",
       "      <td>0.475111</td>\n",
       "      <td>-1.623880</td>\n",
       "      <td>-3.304805</td>\n",
       "      <td>-0.101691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.983497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.586001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.620678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.396881</td>\n",
       "      <td>17.629379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.489363</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.269672</td>\n",
       "      <td>33.160877</td>\n",
       "      <td>20.251741</td>\n",
       "      <td>16.215490</td>\n",
       "      <td>17.485193</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 20481 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         interview            0            1            2            3  \\\n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000   \n",
       "mean      0.503780     0.270279    -0.285081   -11.687315    -0.325762   \n",
       "std       0.150148     0.624930     0.065183     7.498257     0.059253   \n",
       "min       0.000000    -1.843723    -0.542196   -44.137951    -0.573281   \n",
       "25%       0.401869    -0.157689    -0.326142   -16.231661    -0.364254   \n",
       "50%       0.514019     0.253475    -0.281383   -11.121806    -0.323276   \n",
       "75%       0.609813     0.689761    -0.239574    -6.617743    -0.286216   \n",
       "max       1.000000     2.983497     0.000000    12.586001     0.000000   \n",
       "\n",
       "                 4            5            6            7            8  ...  \\\n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000  ...   \n",
       "mean     -0.207463    -8.980567    -0.445502     5.119049    -6.217986  ...   \n",
       "std       0.042329     7.412726     0.094788     8.331264     7.760859  ...   \n",
       "min      -0.417524   -37.007336    -0.920841   -38.122360   -50.331089  ...   \n",
       "25%      -0.234987   -13.602871    -0.504585    -0.199337   -11.053427  ...   \n",
       "50%      -0.206054    -8.674579    -0.441152     5.125813    -5.575384  ...   \n",
       "75%      -0.177989    -3.944145    -0.379699    10.391348    -0.939446  ...   \n",
       "max       0.000000    16.620678     0.000000    40.396881    17.629379  ...   \n",
       "\n",
       "             20470        20471        20472        20473        20474  \\\n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000   \n",
       "mean     -0.234988    -7.431848    -0.196004    -0.308717    -8.130709   \n",
       "std       0.056034     7.239368     0.042703     0.071241     7.204863   \n",
       "min      -0.507402   -39.297752    -0.389587    -0.629192   -34.041065   \n",
       "25%      -0.270404   -11.921796    -0.222966    -0.356357   -12.747404   \n",
       "50%      -0.230486    -7.218858    -0.193964    -0.307008    -8.057387   \n",
       "75%      -0.195276    -2.571614    -0.166588    -0.258341    -3.661532   \n",
       "max       0.000000    20.489363     0.000000     0.000000    29.269672   \n",
       "\n",
       "             20475        20476        20477        20478        20479  \n",
       "count  6000.000000  6000.000000  6000.000000  6000.000000  6000.000000  \n",
       "mean     -1.737559    -4.992703    -5.760758    -7.486003    -0.125419  \n",
       "std       8.712741     7.813078     6.188989     6.417091     0.033693  \n",
       "min     -33.186592   -33.021378   -30.586414   -32.924835    -0.279788  \n",
       "25%      -7.484886   -10.314631    -9.545395   -11.566561    -0.146197  \n",
       "50%      -1.736244    -4.799426    -5.530936    -7.419476    -0.123316  \n",
       "75%       3.860130     0.475111    -1.623880    -3.304805    -0.101691  \n",
       "max      33.160877    20.251741    16.215490    17.485193     0.000000  \n",
       "\n",
       "[8 rows x 20481 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of training set\n",
    "vgg_1_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the test set similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "vgg_df_interview_test = pd.DataFrame(annotation_validation[\"interview\"],  index=[0]).T\n",
    "vgg_df_interview_test.reset_index(inplace = True)\n",
    "vgg_df_interview_test.rename(columns = {\"index\" : \"filenames\", 0 : \"interview\"}, inplace = True)\n",
    "print(vgg_df_interview_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 20482)\n"
     ]
    }
   ],
   "source": [
    "vgg_1_test = pd.merge(vgg_df_interview_test, df_containing_vgg, on = [\"filenames\"], how = \"left\")\n",
    "print(vgg_1_test.shape)\n",
    "\n",
    "# Checking for missing data & outliers in the test set\n",
    "for column in vgg_1_test.columns:\n",
    "    if vgg_1_test[column].isnull().any():\n",
    "        print(column)                           # No output means no missing data in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>interview</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>20470</th>\n",
       "      <th>20471</th>\n",
       "      <th>20472</th>\n",
       "      <th>20473</th>\n",
       "      <th>20474</th>\n",
       "      <th>20475</th>\n",
       "      <th>20476</th>\n",
       "      <th>20477</th>\n",
       "      <th>20478</th>\n",
       "      <th>20479</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.504650</td>\n",
       "      <td>0.273111</td>\n",
       "      <td>-0.284625</td>\n",
       "      <td>-11.569660</td>\n",
       "      <td>-0.323263</td>\n",
       "      <td>-0.205865</td>\n",
       "      <td>-9.085543</td>\n",
       "      <td>-0.443995</td>\n",
       "      <td>5.054893</td>\n",
       "      <td>-6.176260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234774</td>\n",
       "      <td>-7.407041</td>\n",
       "      <td>-0.194751</td>\n",
       "      <td>-0.308310</td>\n",
       "      <td>-8.286963</td>\n",
       "      <td>-1.771374</td>\n",
       "      <td>-4.944284</td>\n",
       "      <td>-5.808772</td>\n",
       "      <td>-7.138320</td>\n",
       "      <td>-0.125944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.145160</td>\n",
       "      <td>0.629038</td>\n",
       "      <td>0.065210</td>\n",
       "      <td>7.365011</td>\n",
       "      <td>0.058012</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>7.382323</td>\n",
       "      <td>0.094968</td>\n",
       "      <td>8.368805</td>\n",
       "      <td>7.933972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056063</td>\n",
       "      <td>7.177917</td>\n",
       "      <td>0.041924</td>\n",
       "      <td>0.070534</td>\n",
       "      <td>7.161254</td>\n",
       "      <td>8.784038</td>\n",
       "      <td>7.645644</td>\n",
       "      <td>6.027730</td>\n",
       "      <td>6.541465</td>\n",
       "      <td>0.034460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.675340</td>\n",
       "      <td>-0.575146</td>\n",
       "      <td>-47.315029</td>\n",
       "      <td>-0.531785</td>\n",
       "      <td>-0.376412</td>\n",
       "      <td>-43.383350</td>\n",
       "      <td>-0.885063</td>\n",
       "      <td>-35.396843</td>\n",
       "      <td>-47.439022</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.451487</td>\n",
       "      <td>-32.617569</td>\n",
       "      <td>-0.373997</td>\n",
       "      <td>-0.572558</td>\n",
       "      <td>-37.713650</td>\n",
       "      <td>-34.476860</td>\n",
       "      <td>-30.380680</td>\n",
       "      <td>-30.756332</td>\n",
       "      <td>-34.868942</td>\n",
       "      <td>-0.269901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.411215</td>\n",
       "      <td>-0.167317</td>\n",
       "      <td>-0.324275</td>\n",
       "      <td>-16.235450</td>\n",
       "      <td>-0.360963</td>\n",
       "      <td>-0.234632</td>\n",
       "      <td>-13.711173</td>\n",
       "      <td>-0.503499</td>\n",
       "      <td>-0.009272</td>\n",
       "      <td>-11.177060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269484</td>\n",
       "      <td>-11.950513</td>\n",
       "      <td>-0.219698</td>\n",
       "      <td>-0.352340</td>\n",
       "      <td>-12.904392</td>\n",
       "      <td>-7.654516</td>\n",
       "      <td>-10.270492</td>\n",
       "      <td>-9.617095</td>\n",
       "      <td>-11.398297</td>\n",
       "      <td>-0.146514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.249495</td>\n",
       "      <td>-0.279998</td>\n",
       "      <td>-11.183249</td>\n",
       "      <td>-0.322512</td>\n",
       "      <td>-0.203339</td>\n",
       "      <td>-8.678718</td>\n",
       "      <td>-0.442501</td>\n",
       "      <td>4.887539</td>\n",
       "      <td>-5.539994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231450</td>\n",
       "      <td>-7.304997</td>\n",
       "      <td>-0.193672</td>\n",
       "      <td>-0.305563</td>\n",
       "      <td>-8.325303</td>\n",
       "      <td>-1.733344</td>\n",
       "      <td>-4.808083</td>\n",
       "      <td>-5.514301</td>\n",
       "      <td>-7.028734</td>\n",
       "      <td>-0.123758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.684074</td>\n",
       "      <td>-0.238581</td>\n",
       "      <td>-6.701227</td>\n",
       "      <td>-0.285047</td>\n",
       "      <td>-0.177560</td>\n",
       "      <td>-4.269777</td>\n",
       "      <td>-0.376240</td>\n",
       "      <td>10.044693</td>\n",
       "      <td>-0.860253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195308</td>\n",
       "      <td>-2.860291</td>\n",
       "      <td>-0.166715</td>\n",
       "      <td>-0.261471</td>\n",
       "      <td>-3.550395</td>\n",
       "      <td>3.990892</td>\n",
       "      <td>0.476962</td>\n",
       "      <td>-1.767284</td>\n",
       "      <td>-2.823713</td>\n",
       "      <td>-0.101118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.915888</td>\n",
       "      <td>2.195411</td>\n",
       "      <td>-0.090423</td>\n",
       "      <td>14.258253</td>\n",
       "      <td>-0.156801</td>\n",
       "      <td>-0.074755</td>\n",
       "      <td>15.895493</td>\n",
       "      <td>-0.186607</td>\n",
       "      <td>39.655376</td>\n",
       "      <td>18.683762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090791</td>\n",
       "      <td>18.518583</td>\n",
       "      <td>-0.071463</td>\n",
       "      <td>-0.125923</td>\n",
       "      <td>25.900566</td>\n",
       "      <td>30.128595</td>\n",
       "      <td>17.002779</td>\n",
       "      <td>18.811586</td>\n",
       "      <td>23.617056</td>\n",
       "      <td>-0.045220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 20481 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         interview            0            1            2            3  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean      0.504650     0.273111    -0.284625   -11.569660    -0.323263   \n",
       "std       0.145160     0.629038     0.065210     7.365011     0.058012   \n",
       "min       0.000000    -1.675340    -0.575146   -47.315029    -0.531785   \n",
       "25%       0.411215    -0.167317    -0.324275   -16.235450    -0.360963   \n",
       "50%       0.514019     0.249495    -0.279998   -11.183249    -0.322512   \n",
       "75%       0.607477     0.684074    -0.238581    -6.701227    -0.285047   \n",
       "max       0.915888     2.195411    -0.090423    14.258253    -0.156801   \n",
       "\n",
       "                 4            5            6            7            8  ...  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
       "mean     -0.205865    -9.085543    -0.443995     5.054893    -6.176260  ...   \n",
       "std       0.041905     7.382323     0.094968     8.368805     7.933972  ...   \n",
       "min      -0.376412   -43.383350    -0.885063   -35.396843   -47.439022  ...   \n",
       "25%      -0.234632   -13.711173    -0.503499    -0.009272   -11.177060  ...   \n",
       "50%      -0.203339    -8.678718    -0.442501     4.887539    -5.539994  ...   \n",
       "75%      -0.177560    -4.269777    -0.376240    10.044693    -0.860253  ...   \n",
       "max      -0.074755    15.895493    -0.186607    39.655376    18.683762  ...   \n",
       "\n",
       "             20470        20471        20472        20473        20474  \\\n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
       "mean     -0.234774    -7.407041    -0.194751    -0.308310    -8.286963   \n",
       "std       0.056063     7.177917     0.041924     0.070534     7.161254   \n",
       "min      -0.451487   -32.617569    -0.373997    -0.572558   -37.713650   \n",
       "25%      -0.269484   -11.950513    -0.219698    -0.352340   -12.904392   \n",
       "50%      -0.231450    -7.304997    -0.193672    -0.305563    -8.325303   \n",
       "75%      -0.195308    -2.860291    -0.166715    -0.261471    -3.550395   \n",
       "max      -0.090791    18.518583    -0.071463    -0.125923    25.900566   \n",
       "\n",
       "             20475        20476        20477        20478        20479  \n",
       "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
       "mean     -1.771374    -4.944284    -5.808772    -7.138320    -0.125944  \n",
       "std       8.784038     7.645644     6.027730     6.541465     0.034460  \n",
       "min     -34.476860   -30.380680   -30.756332   -34.868942    -0.269901  \n",
       "25%      -7.654516   -10.270492    -9.617095   -11.398297    -0.146514  \n",
       "50%      -1.733344    -4.808083    -5.514301    -7.028734    -0.123758  \n",
       "75%       3.990892     0.476962    -1.767284    -2.823713    -0.101118  \n",
       "max      30.128595    17.002779    18.811586    23.617056    -0.045220  \n",
       "\n",
       "[8 rows x 20481 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Description of test set\n",
    "vgg_1_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Predictions with VGG facial data</h1> \n",
    "<h3 align=\"center\"> Multiple Linear Regression</h3>\n",
    "\n",
    "#### Models that will be trained:\n",
    "- Model 1 = Job interview ~ Facial\n",
    "- Model 2 = Job interview ~ Facial + Gender\n",
    "- Model 3 = Job interview ~ Facial + Ethnicity\n",
    "- Model 4 = Job interview ~ Facial + Gender + Ethnicity\n",
    "\n",
    "## Model 1 - Interview ~ facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the (in)dependent variables\n",
    "X_1_train = vgg_1_train.drop(['filenames', 'interview'], axis=1).values\n",
    "y_1_train = vgg_1_train.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_1_test = vgg_1_test.drop(['filenames', 'interview'], axis=1).values\n",
    "y_1_test = vgg_1_test.loc[:,['interview']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8830107663551402"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a baseline by taking the MAE of the test set outcome variable and the mean of the training set outcome variable\n",
    "1 - mean_absolute_error(y_1_test, np.full(2000, np.mean(y_1_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.06561\n",
      "MAE 0.1991\n",
      "1 - MAE 0.8009\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_1_train, y_1_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_1_pred = regressor.predict(X_1_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_1_test, y_1_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_1_test, y_1_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_1_test, y_1_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the importance of the added variable(s) (not necessary for Model 1)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.031157857\n",
      "0.041963637\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 1 = 0.01385\n",
      "MLR with 500 components, MAE Model 1 = 0.09424\n",
      "MLR with 500 components, 1 - MAE Model 1 = 0.9058\n",
      "-------------------------------------------\n",
      "MLR with 1000 components, MSE Model 1 = 0.01404\n",
      "MLR with 1000 components, MAE Model 1 = 0.09537\n",
      "MLR with 1000 components, 1 - MAE Model 1 = 0.9046\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 1 = 0.01456\n",
      "MLR with 2000 components, MAE Model 1 = 0.09584\n",
      "MLR with 2000 components, 1 - MAE Model 1 = 0.9042\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_mlr_PCA = pca.fit_transform(X_1_train)\n",
    "    X_1_test_mlr_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    regressor.fit(X_1_train_mlr_PCA, y_1_train)\n",
    "    mlr_y_1_PCA = regressor.predict(X_1_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 1 = 0.01371\n",
      "MLR with 500 components, MAE Model 1 = 0.09371\n",
      "MLR with 500 components, 1 - MAE Model 1 = 0.9063\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 1 for additional testing\n",
    "components = [500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_mlr_PCA = pca.fit_transform(X_1_train)\n",
    "    X_1_test_mlr_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    regressor.fit(X_1_train_mlr_PCA, y_1_train)\n",
    "    mlr_y_1_PCA = regressor.predict(X_1_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, mlr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01456\n",
      "MAE 0.09535\n",
      "1 - MAE 0.9046\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_1_train, y_1_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_1 = rfr.predict(X_1_test)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_1_test, rfr_y_1)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_1_test, rfr_y_1)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_1_test, rfr_y_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 500 components, MSE Model 1 = 0.01544\n",
      "RFR with 500 components, MAE Model 1 = 0.09914\n",
      "RFR with 500 components, 1 - MAE Model 1 = 0.9009\n",
      "-------------------------------------------\n",
      "RFR with 1000 components, MSE Model 1 = 0.01652\n",
      "RFR with 1000 components, MAE Model 1 = 0.1036\n",
      "RFR with 1000 components, 1 - MAE Model 1 = 0.8964\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 1 = 0.01618\n",
      "RFR with 2000 components, MAE Model 1 = 0.1017\n",
      "RFR with 2000 components, 1 - MAE Model 1 = 0.8983\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_1_train_PCA = pca.fit_transform(X_1_train)\n",
    "    rfr_X_1_test_PCA = pca.transform(X_1_test)\n",
    "    \n",
    "    rfr.fit(rfr_X_1_train_PCA, y_1_train.ravel())\n",
    "    rfr_y_1_PCA = rfr.predict(rfr_X_1_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, rfr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "X_1_train_scale = X_sc.fit_transform(X_1_train)\n",
    "X_1_test_scale = X_sc.transform(X_1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01216\n",
      "MAE 0.08813\n",
      "1 - MAE 0.9119\n",
      "Wall time: 8min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_1_train_scale, y_1_train.ravel())\n",
    "svr_y_1 = svr.predict(X_1_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_1_test, svr_y_1)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_1_test, svr_y_1)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_1_test, svr_y_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 500 components, MSE Model 1 = 0.01223\n",
      "SVR with 500 components, MAE Model 1 = 0.08835\n",
      "SVR with 500 components, 1 - MAE Model 1 = 0.9116\n",
      "-------------------------------------------\n",
      "SVR with 1000 components, MSE Model 1 = 0.01217\n",
      "SVR with 1000 components, MAE Model 1 = 0.08807\n",
      "SVR with 1000 components, 1 - MAE Model 1 = 0.9119\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 1 = 0.01214\n",
      "SVR with 2000 components, MAE Model 1 = 0.08788\n",
      "SVR with 2000 components, 1 - MAE Model 1 = 0.9121\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for support vector regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_1_train_PCA = pca.fit_transform(X_1_train_scale)\n",
    "    X_1_test_PCA = pca.transform(X_1_test_scale)\n",
    "    \n",
    "    svr.fit(X_1_train_PCA, y_1_train.ravel())\n",
    "    svr_y_1_PCA = svr.predict(X_1_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 1 = {:.4}\".format(mean_squared_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 1 = {:.4}\".format(mean_absolute_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 1 = {:.4}\".format(1-mean_absolute_error(y_1_test, svr_y_1_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Interview ~ facial + gender\n",
    "#### Creating the training and validation (test) set for Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a training set for Model 2\n",
    "df_gen_int = excel_to_merge[['filenames', 'gender']]\n",
    "training_face_gen = pd.merge(vgg_1_train, df_gen_int, how = \"left\", on = [\"filenames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 2\n",
    "val_face_gen = pd.merge(vgg_1_test, df_gen_int, how = \"left\", on = [\"filenames\"])\n",
    "#val_face_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Splitting the (in)dependent variables\n",
    "X_2_train = training_face_gen.drop(['filenames','interview'], axis=1).values\n",
    "y_2_train = training_face_gen.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_2_test = val_face_gen.drop(['filenames', 'interview'], axis=1).values\n",
    "y_2_test = val_face_gen.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_2_train[:, -1] = le.fit_transform(X_2_train[:, -1])\n",
    "X_2_test[:, -1] = le.fit_transform(X_2_test[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.06529\n",
      "MAE 0.1988\n",
      "1 - MAE 0.8012\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_2_train, y_2_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_2_pred = regressor.predict(X_2_test)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_2_test, y_2_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_2_test, y_2_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_2_test, y_2_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0317044677696671\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "# Position of gender variable\n",
    "print(importance[-1])\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0317044677696671\n",
      "0.04209157119361864\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 2 = 0.01378\n",
      "MLR with 500 components, MAE Model 2 = 0.09386\n",
      "MLR with 500 components, 1 - MAE Model 2 = 0.9061\n",
      "-------------------------------------------\n",
      "MLR with 1000 components, MSE Model 2 = 0.01391\n",
      "MLR with 1000 components, MAE Model 2 = 0.09461\n",
      "MLR with 1000 components, 1 - MAE Model 2 = 0.9054\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 2 = 0.01446\n",
      "MLR with 2000 components, MAE Model 2 = 0.09554\n",
      "MLR with 2000 components, 1 - MAE Model 2 = 0.9045\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_mlr_PCA = pca.fit_transform(X_2_train)\n",
    "    X_2_test_mlr_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    regressor.fit(X_2_train_mlr_PCA, y_2_train)\n",
    "    mlr_y_2_PCA = regressor.predict(X_2_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1 - mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 2 = 0.01375\n",
      "MLR with 500 components, MAE Model 2 = 0.09387\n",
      "MLR with 500 components, 1 - MAE Model 2 = 0.9061\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 2 for additional testing\n",
    "components = [500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_mlr_PCA = pca.fit_transform(X_2_train)\n",
    "    X_2_test_mlr_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    regressor.fit(X_2_train_mlr_PCA, y_2_train)\n",
    "    mlr_y_2_PCA = regressor.predict(X_2_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1 - mean_absolute_error(y_2_test, mlr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01459\n",
      "MAE 0.0954\n",
      "1 - MAE 0.9046\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_2_train, y_2_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_2 = rfr.predict(X_2_test)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_2_test, rfr_y_2)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_2_test, rfr_y_2)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_2_test, rfr_y_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 500 components, MSE Model 2 = 0.01549\n",
      "RFR with 500 components, MAE Model 2 = 0.09976\n",
      "RFR with 500 components, 1 - MAE Model 2 = 0.9002\n",
      "-------------------------------------------\n",
      "RFR with 1000 components, MSE Model 2 = 0.01637\n",
      "RFR with 1000 components, MAE Model 2 = 0.1032\n",
      "RFR with 1000 components, 1 - MAE Model 2 = 0.8968\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 2 = 0.01678\n",
      "RFR with 2000 components, MAE Model 2 = 0.1041\n",
      "RFR with 2000 components, 1 - MAE Model 2 = 0.8959\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_2_train_PCA = pca.fit_transform(X_2_train)\n",
    "    rfr_X_2_test_PCA = pca.transform(X_2_test)\n",
    "    \n",
    "    rfr.fit(rfr_X_2_train_PCA, y_2_train.ravel())\n",
    "    rfr_y_2_PCA = rfr.predict(rfr_X_2_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1-mean_absolute_error(y_2_test, rfr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the gender dummies\n",
    "X_2_train_scale = X_2_train.copy()\n",
    "X_2_test_scale = X_2_test.copy()\n",
    "\n",
    "X_2_train_scale[:, :-1] = X_sc.fit_transform(X_2_train[:, :-1])\n",
    "X_2_test_scale[:, :-1] = X_sc.transform(X_2_test[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01217\n",
      "MAE 0.08813\n",
      "1 - MAE 0.9119\n",
      "Wall time: 7min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_2_train_scale, y_2_train.ravel())\n",
    "svr_y_2 = svr.predict(X_2_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_2_test, svr_y_2)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_2_test, svr_y_2)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_2_test, svr_y_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 500 components, MSE Model 2 = 0.01223\n",
      "SVR with 500 components, MAE Model 2 = 0.08832\n",
      "SVR with 500 components, 1 - MAE Model 2 = 0.9117\n",
      "-------------------------------------------\n",
      "SVR with 1000 components, MSE Model 2 = 0.01218\n",
      "SVR with 1000 components, MAE Model 2 = 0.08811\n",
      "SVR with 1000 components, 1 - MAE Model 2 = 0.9119\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 2 = 0.01213\n",
      "SVR with 2000 components, MAE Model 2 = 0.08783\n",
      "SVR with 2000 components, 1 - MAE Model 2 = 0.9122\n",
      "-------------------------------------------\n",
      "Wall time: 8min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [500, 1000, 2000]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_2_train_PCA = pca.fit_transform(X_2_train_scale)\n",
    "    X_2_test_PCA = pca.transform(X_2_test_scale)\n",
    "    \n",
    "    svr.fit(X_2_train_PCA, y_2_train.ravel())\n",
    "    svr_y_2_PCA = svr.predict(X_2_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 2 = {:.4}\".format(mean_squared_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 2 = {:.4}\".format(mean_absolute_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 2 = {:.4}\".format(1-mean_absolute_error(y_2_test, svr_y_2_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Interview ~ facial + ethnicity\n",
    "#### Creating the training and validation (test) set for Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 3, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_eth_int = excel_to_merge[['filenames', 'ethnicity']]\n",
    "training_face_eth = pd.merge(vgg_1_train, df_eth_int, how = \"left\", on = [\"filenames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 3\n",
    "#df_val_eth_int = merged_val_df[['videoname', 'ethnicity', 'interview']]\n",
    "val_face_eth = pd.merge(vgg_1_test, df_eth_int, how = \"left\", on = [\"filenames\"])\n",
    "#val_face_eth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Splitting the (in)dependent variables\n",
    "X_3_train = training_face_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_3_train = training_face_eth.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_3_test = val_face_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_3_test = val_face_eth.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')\n",
    "X_3_train = np.array(ct.fit_transform(X_3_train))\n",
    "X_3_test = np.array(ct.fit_transform(X_3_test))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_3_train_dummy = X_3_train.copy()\n",
    "X_3_train_dummy = X_3_train_dummy[:,1:]\n",
    "\n",
    "X_3_test_dummy = X_3_test.copy()\n",
    "X_3_test_dummy = X_3_test_dummy[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.06561\n",
      "MAE 0.1991\n",
      "1 - MAE 0.8009\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_3_train_dummy, y_3_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_3_pred = regressor.predict(X_3_test_dummy)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_3_test, y_3_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_3_test, y_3_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_3_test, y_3_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.011388491580105306\n",
      "0.030659034221820904\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "# Postions of ethnicity dummies\n",
    "print(importance[0])\n",
    "print(importance[1])\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03115504514916712\n",
      "0.04160529942339224\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 3 = 0.01377\n",
      "MLR with 500 components, MAE Model 3 = 0.09381\n",
      "MLR with 500 components, 1 - MAE Model 3 = 0.9062\n",
      "-------------------------------------------\n",
      "MLR with 1000 components, MSE Model 3 = 0.01402\n",
      "MLR with 1000 components, MAE Model 3 = 0.09529\n",
      "MLR with 1000 components, 1 - MAE Model 3 = 0.9047\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 3 = 0.01452\n",
      "MLR with 2000 components, MAE Model 3 = 0.09572\n",
      "MLR with 2000 components, 1 - MAE Model 3 = 0.9043\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_mlr_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    X_3_test_mlr_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_3_train_mlr_PCA, y_3_train)\n",
    "    mlr_y_3_PCA = regressor.predict(X_3_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 3 = 0.0138\n",
      "MLR with 500 components, MAE Model 3 = 0.09415\n",
      "MLR with 500 components, 1 - MAE Model 3 = 0.9059\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 3 for additional testing\n",
    "components = [500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_mlr_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    X_3_test_mlr_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_3_train_mlr_PCA, y_3_train)\n",
    "    mlr_y_3_PCA = regressor.predict(X_3_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, mlr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.0146\n",
      "MAE 0.09543\n",
      "1 - MAE 0.9046\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_3_train_dummy, y_3_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_3 = rfr.predict(X_3_test_dummy)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_3_test, rfr_y_3)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_3_test, rfr_y_3)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_3_test, rfr_y_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 500 components, MSE Model 3 = 0.01571\n",
      "RFR with 500 components, MAE Model 3 = 0.09962\n",
      "RFR with 500 components, 1 - MAE Model 3 = 0.9004\n",
      "-------------------------------------------\n",
      "RFR with 1000 components, MSE Model 3 = 0.0164\n",
      "RFR with 1000 components, MAE Model 3 = 0.1021\n",
      "RFR with 1000 components, 1 - MAE Model 3 = 0.8979\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 3 = 0.0164\n",
      "RFR with 2000 components, MAE Model 3 = 0.1021\n",
      "RFR with 2000 components, 1 - MAE Model 3 = 0.8979\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_3_train_PCA = pca.fit_transform(X_3_train_dummy)\n",
    "    rfr_X_3_test_PCA = pca.transform(X_3_test_dummy)\n",
    "    \n",
    "    rfr.fit(rfr_X_3_train_PCA, y_3_train.ravel())\n",
    "    rfr_y_3_PCA = rfr.predict(rfr_X_3_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, rfr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the ethnicity dummies\n",
    "X_3_train_scale = X_3_train_dummy.copy()\n",
    "X_3_test_scale = X_3_test_dummy.copy()\n",
    "\n",
    "X_3_train_scale[:, 2:] = X_sc.fit_transform(X_3_train_dummy[:, 2:])\n",
    "X_3_test_scale[:, 2:] = X_sc.transform(X_3_test_dummy[:, 2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01217\n",
      "MAE 0.08813\n",
      "1 - MAE 0.9119\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vectore regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_3_train_scale, y_3_train.ravel())\n",
    "svr_y_3 = svr.predict(X_3_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_3_test, svr_y_3)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_3_test, svr_y_3)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_3_test, svr_y_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 500 components, MSE Model 3 = 0.01224\n",
      "SVR with 500 components, MAE Model 3 = 0.08832\n",
      "SVR with 500 components, 1 - MAE Model 3 = 0.9117\n",
      "-------------------------------------------\n",
      "SVR with 1000 components, MSE Model 3 = 0.01219\n",
      "SVR with 1000 components, MAE Model 3 = 0.08812\n",
      "SVR with 1000 components, 1 - MAE Model 3 = 0.9119\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 3 = 0.01214\n",
      "SVR with 2000 components, MAE Model 3 = 0.08789\n",
      "SVR with 2000 components, 1 - MAE Model 3 = 0.9121\n",
      "-------------------------------------------\n",
      "Wall time: 8min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [500, 1000, 2000]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_3_train_PCA = pca.fit_transform(X_3_train_scale)\n",
    "    X_3_test_PCA = pca.transform(X_3_test_scale)\n",
    "    \n",
    "    svr.fit(X_3_train_PCA, y_3_train.ravel())\n",
    "    svr_y_3_PCA = svr.predict(X_3_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 3 = {:.4}\".format(mean_squared_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 3 = {:.4}\".format(mean_absolute_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 3 = {:.4}\".format(1-mean_absolute_error(y_3_test, svr_y_3_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 - Interview ~ facial + gender + ethnicity\n",
    "#### Creating the training and validation (test) set for Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training set for Model 4, merging the new training dataset with\n",
    "# a previously created dataset that contains 'interview', 'gender' and 'ethnicity'\n",
    "\n",
    "df_gen_eth_int = excel_to_merge[['filenames', 'ethnicity', 'gender']]\n",
    "training_face_gen_eth = pd.merge(vgg_1_train, df_gen_eth_int, how = \"left\", on = [\"filenames\"])\n",
    "#training_face_gen_eth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the validation/ test set for Model 4\n",
    "val_face_gen_eth = pd.merge(vgg_1_test, df_gen_eth_int, how = \"left\", on = [\"filenames\"])\n",
    "#val_face_gen_eth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression preparations Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Splitting the (in)dependent variables\n",
    "X_4_train = training_face_gen_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_4_train = training_face_gen_eth.loc[:,['interview']].values\n",
    "\n",
    "# Splitting the (in)dependent variables of validation set\n",
    "X_4_test = val_face_gen_eth.drop(['filenames', 'interview'], axis=1).values\n",
    "y_4_test = val_face_gen_eth.loc[:,['interview']].values\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of gender\n",
    "le = LabelEncoder()\n",
    "X_4_train[:, -1] = le.fit_transform(X_4_train[:, -1])\n",
    "X_4_test[:, -1] = le.fit_transform(X_4_test[:, -1])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Make a dummy variable of the categorical values of ethnicity\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-2])], remainder='passthrough')\n",
    "X_4_train = ct.fit_transform(X_4_train)\n",
    "X_4_test = ct.fit_transform(X_4_test)     # np.array() weggehaald om ct... heen\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Avoid dummy trap\n",
    "X_4_train_dummy = X_4_train.copy()\n",
    "X_4_train_dummy = X_4_train_dummy[:,1:]\n",
    "\n",
    "X_4_test_dummy = X_4_test.copy()\n",
    "X_4_test_dummy = X_4_test_dummy[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple linear regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.06531\n",
      "MAE 0.1989\n",
      "1 - MAE 0.8011\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_4_train_dummy, y_4_train)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Predicting the test set labels\n",
    "y_4_pred = regressor.predict(X_4_test_dummy)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Various performance measure scores\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_4_test, y_4_pred)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_4_test, y_4_pred)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_4_test, y_4_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.010816691292975498\n",
      "0.03106467110895475\n",
      "-0.03164995838259116\n"
     ]
    }
   ],
   "source": [
    "# Checking the importance of the added variable(s)\n",
    "importance = regressor.coef_.ravel()\n",
    "\n",
    "# Position of ethnicity and gender dummy variables\n",
    "print(importance[0])\n",
    "print(importance[1])\n",
    "print(importance[-1])\n",
    "\n",
    "#importance = regressor.coef_.ravel()\n",
    "#for i,j in enumerate(importance):\n",
    "    #print('Feature: %0d, Score: %.5f' % (i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03164995838259116\n",
      "0.041891539759025015\n"
     ]
    }
   ],
   "source": [
    "# The minimum and maximum feature importance\n",
    "print(importance.min())\n",
    "print(importance.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 4 = 0.01363\n",
      "MLR with 500 components, MAE Model 4 = 0.09345\n",
      "MLR with 500 components, 1 - MAE Model 4 = 0.9065\n",
      "-------------------------------------------\n",
      "MLR with 1000 components, MSE Model 4 = 0.01394\n",
      "MLR with 1000 components, MAE Model 4 = 0.09482\n",
      "MLR with 1000 components, 1 - MAE Model 4 = 0.9052\n",
      "-------------------------------------------\n",
      "MLR with 2000 components, MSE Model 4 = 0.01461\n",
      "MLR with 2000 components, MAE Model 4 = 0.09608\n",
      "MLR with 2000 components, 1 - MAE Model 4 = 0.9039\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_mlr_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    X_4_test_mlr_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_4_train_mlr_PCA, y_4_train)\n",
    "    mlr_y_4_PCA = regressor.predict(X_4_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLR with 500 components, MSE Model 4 = 0.01379\n",
      "MLR with 500 components, MAE Model 4 = 0.09391\n",
      "MLR with 500 components, 1 - MAE Model 4 = 0.9061\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for linear regression Model 4 for additional testing\n",
    "components = [500]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_mlr_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    X_4_test_mlr_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    regressor.fit(X_4_train_mlr_PCA, y_4_train)\n",
    "    mlr_y_4_PCA = regressor.predict(X_4_test_mlr_PCA)\n",
    "    print(\"MLR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"MLR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, mlr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01462\n",
      "MAE 0.09546\n",
      "1 - MAE 0.9045\n"
     ]
    }
   ],
   "source": [
    "# Random forest regression\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "rfr.fit(X_4_train_dummy, y_4_train.ravel())\n",
    "\n",
    "# Using .ravel() to get the correct format\n",
    "rfr_y_4 = rfr.predict(X_4_test_dummy)\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_4_test, rfr_y_4)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_4_test, rfr_y_4)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_4_test, rfr_y_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR with 500 components, MSE Model 4 = 0.01543\n",
      "RFR with 500 components, MAE Model 4 = 0.09895\n",
      "RFR with 500 components, 1 - MAE Model 4 = 0.901\n",
      "-------------------------------------------\n",
      "RFR with 1000 components, MSE Model 4 = 0.01634\n",
      "RFR with 1000 components, MAE Model 4 = 0.1029\n",
      "RFR with 1000 components, 1 - MAE Model 4 = 0.8971\n",
      "-------------------------------------------\n",
      "RFR with 2000 components, MSE Model 4 = 0.0166\n",
      "RFR with 2000 components, MAE Model 4 = 0.1033\n",
      "RFR with 2000 components, 1 - MAE Model 4 = 0.8967\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# PCA for random forest regression\n",
    "components = [500, 1000, 2000]\n",
    "\n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    rfr_X_4_train_PCA = pca.fit_transform(X_4_train_dummy)\n",
    "    rfr_X_4_test_PCA = pca.transform(X_4_test_dummy)\n",
    "    \n",
    "    rfr.fit(rfr_X_4_train_PCA, y_4_train.ravel())\n",
    "    rfr_y_4_PCA = rfr.predict(rfr_X_4_test_PCA)\n",
    "    print(\"RFR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"RFR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"RFR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, rfr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vector regression Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features \n",
    "# X\n",
    "X_sc = StandardScaler()\n",
    "\n",
    "# Copying the sets to be able to perform feature scaling on every feature but the gender and ethnicity dummies\n",
    "X_4_train_scale = X_4_train_dummy.copy()\n",
    "X_4_test_scale = X_4_test_dummy.copy()\n",
    "\n",
    "X_4_train_scale[:, 2:-1] = X_sc.fit_transform(X_4_train_dummy[:, 2:-1])\n",
    "X_4_test_scale[:, 2:-1] = X_sc.transform(X_4_test_dummy[:, 2:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE 0.01217\n",
      "MAE 0.08813\n",
      "1 - MAE 0.9119\n",
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Support vector regression\n",
    "\n",
    "svr = SVR(kernel = 'rbf')\n",
    "svr.fit(X_4_train_scale, y_4_train.ravel())\n",
    "svr_y_4 = svr.predict(X_4_test_scale)\n",
    "\n",
    "print(\"MSE {:.4}\".format(mean_squared_error(y_4_test, svr_y_4)))\n",
    "print(\"MAE {:.4}\".format(mean_absolute_error(y_4_test, svr_y_4)))\n",
    "print(\"1 - MAE {:.4}\".format(1-mean_absolute_error(y_4_test, svr_y_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR with 500 components, MSE Model 4 = 0.01226\n",
      "SVR with 500 components, MAE Model 4 = 0.08839\n",
      "SVR with 500 components, 1 - MAE Model 4 = 0.9116\n",
      "-------------------------------------------\n",
      "SVR with 1000 components, MSE Model 4 = 0.01217\n",
      "SVR with 1000 components, MAE Model 4 = 0.08804\n",
      "SVR with 1000 components, 1 - MAE Model 4 = 0.912\n",
      "-------------------------------------------\n",
      "SVR with 2000 components, MSE Model 4 = 0.01214\n",
      "SVR with 2000 components, MAE Model 4 = 0.0879\n",
      "SVR with 2000 components, 1 - MAE Model 4 = 0.9121\n",
      "-------------------------------------------\n",
      "Wall time: 8min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# PCA for support vector regression\n",
    "components = [500, 1000, 2000]\n",
    "    \n",
    "for component in components:\n",
    "    pca = PCA(n_components = component)   \n",
    "    X_4_train_PCA = pca.fit_transform(X_4_train_scale)\n",
    "    X_4_test_PCA = pca.transform(X_4_test_scale)\n",
    "    \n",
    "    svr.fit(X_4_train_PCA, y_4_train.ravel())\n",
    "    svr_y_4_PCA = svr.predict(X_4_test_PCA)\n",
    "    print(\"SVR with\", component, \"components, MSE Model 4 = {:.4}\".format(mean_squared_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"SVR with\", component, \"components, MAE Model 4 = {:.4}\".format(mean_absolute_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"SVR with\", component, \"components, 1 - MAE Model 4 = {:.4}\".format(1-mean_absolute_error(y_4_test, svr_y_4_PCA)))\n",
    "    print(\"-------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-tests between the models that have slight differences in MAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=array([-2.01252288]), pvalue=array([0.04429873]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.ttest_rel(y_1_pred, y_3_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80233428]\n",
      "[0.55279233]\n",
      "[0.51086652]\n"
     ]
    }
   ],
   "source": [
    "# Related t-test MLR PCA\n",
    "MLR_1_500 = mlr_y_1_PCA.copy()\n",
    "MLR_2_500 = mlr_y_2_PCA.copy()\n",
    "MLR_3_500 = mlr_y_3_PCA.copy()\n",
    "MLR_4_500 = mlr_y_4_PCA.copy()\n",
    "\n",
    "print(scipy.stats.ttest_rel(MLR_1_500, MLR_2_500).pvalue)         # models 1 and 2\n",
    "print(scipy.stats.ttest_rel(MLR_1_500, MLR_3_500).pvalue)         # models 1 and 3\n",
    "print(scipy.stats.ttest_rel(MLR_1_500, MLR_4_500).pvalue)         # models 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.576340820033024\n"
     ]
    }
   ],
   "source": [
    "# Related t-test RFR\n",
    "print(scipy.stats.ttest_rel(rfr_y_1, rfr_y_4).pvalue)         # models 1 and 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
